{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.382623\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ Initially, the model should give probability of every class:1/10, and the loss then should be -log(0.1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.800241 analytic: -0.800241, relative error: 3.204326e-08\n",
      "numerical: -3.444316 analytic: -3.444316, relative error: 2.728683e-09\n",
      "numerical: 0.628125 analytic: 0.628125, relative error: 9.080841e-08\n",
      "numerical: 1.049605 analytic: 1.049605, relative error: 4.995045e-08\n",
      "numerical: 0.776500 analytic: 0.776499, relative error: 3.431982e-08\n",
      "numerical: -3.085022 analytic: -3.085022, relative error: 8.484812e-09\n",
      "numerical: 0.287294 analytic: 0.287294, relative error: 3.506451e-08\n",
      "numerical: -0.773910 analytic: -0.773910, relative error: 2.704443e-08\n",
      "numerical: -0.762251 analytic: -0.762251, relative error: 3.708095e-08\n",
      "numerical: -0.484336 analytic: -0.484336, relative error: 6.895626e-08\n",
      "numerical: -0.237996 analytic: -0.237996, relative error: 4.699546e-08\n",
      "numerical: -0.988821 analytic: -0.988821, relative error: 5.209239e-08\n",
      "numerical: 0.043120 analytic: 0.043120, relative error: 2.500237e-07\n",
      "numerical: 3.376907 analytic: 3.376907, relative error: 8.770958e-09\n",
      "numerical: 3.412235 analytic: 3.412235, relative error: 9.943429e-09\n",
      "numerical: 0.274138 analytic: 0.274138, relative error: 3.237980e-07\n",
      "numerical: 1.197481 analytic: 1.197480, relative error: 3.115061e-08\n",
      "numerical: -1.950575 analytic: -1.950575, relative error: 1.193118e-08\n",
      "numerical: -1.986372 analytic: -1.986372, relative error: 1.977306e-08\n",
      "numerical: -0.594371 analytic: -0.594371, relative error: 5.237426e-09\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.382623e+00 computed in 0.163252s\n",
      "vectorized loss: 2.382623e+00 computed in 0.003105s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 554.502860\n",
      "iteration 100 / 1500: loss 3.936774\n",
      "iteration 200 / 1500: loss 2.046264\n",
      "iteration 300 / 1500: loss 2.126978\n",
      "iteration 400 / 1500: loss 2.054485\n",
      "iteration 500 / 1500: loss 2.086567\n",
      "iteration 600 / 1500: loss 1.973379\n",
      "iteration 700 / 1500: loss 2.013208\n",
      "iteration 800 / 1500: loss 2.098555\n",
      "iteration 900 / 1500: loss 2.104237\n",
      "iteration 1000 / 1500: loss 2.004473\n",
      "iteration 1100 / 1500: loss 2.121513\n",
      "iteration 1200 / 1500: loss 2.049798\n",
      "iteration 1300 / 1500: loss 1.966783\n",
      "iteration 1400 / 1500: loss 2.059405\n",
      "iteration 0 / 1500: loss 1501.377833\n",
      "iteration 100 / 1500: loss 2.106840\n",
      "iteration 200 / 1500: loss 2.189677\n",
      "iteration 300 / 1500: loss 2.156026\n",
      "iteration 400 / 1500: loss 2.131761\n",
      "iteration 500 / 1500: loss 2.127666\n",
      "iteration 600 / 1500: loss 2.170555\n",
      "iteration 700 / 1500: loss 2.135382\n",
      "iteration 800 / 1500: loss 2.182443\n",
      "iteration 900 / 1500: loss 2.100178\n",
      "iteration 1000 / 1500: loss 2.161338\n",
      "iteration 1100 / 1500: loss 2.140426\n",
      "iteration 1200 / 1500: loss 2.077579\n",
      "iteration 1300 / 1500: loss 2.114439\n",
      "iteration 1400 / 1500: loss 2.146794\n",
      "iteration 0 / 1500: loss 185.021576\n",
      "iteration 100 / 1500: loss 30.744112\n",
      "iteration 200 / 1500: loss 6.606006\n",
      "iteration 300 / 1500: loss 2.699037\n",
      "iteration 400 / 1500: loss 2.088380\n",
      "iteration 500 / 1500: loss 1.977041\n",
      "iteration 600 / 1500: loss 1.885022\n",
      "iteration 700 / 1500: loss 1.984838\n",
      "iteration 800 / 1500: loss 2.031107\n",
      "iteration 900 / 1500: loss 1.967107\n",
      "iteration 1000 / 1500: loss 2.007957\n",
      "iteration 1100 / 1500: loss 1.946602\n",
      "iteration 1200 / 1500: loss 2.011303\n",
      "iteration 1300 / 1500: loss 2.026865\n",
      "iteration 1400 / 1500: loss 2.031066\n",
      "iteration 0 / 1500: loss 1382.661669\n",
      "iteration 100 / 1500: loss 2.182392\n",
      "iteration 200 / 1500: loss 2.123298\n",
      "iteration 300 / 1500: loss 2.186702\n",
      "iteration 400 / 1500: loss 2.136007\n",
      "iteration 500 / 1500: loss 2.188810\n",
      "iteration 600 / 1500: loss 2.172630\n",
      "iteration 700 / 1500: loss 2.204557\n",
      "iteration 800 / 1500: loss 2.104491\n",
      "iteration 900 / 1500: loss 2.115972\n",
      "iteration 1000 / 1500: loss 2.190913\n",
      "iteration 1100 / 1500: loss 2.149326\n",
      "iteration 1200 / 1500: loss 2.133661\n",
      "iteration 1300 / 1500: loss 2.195877\n",
      "iteration 1400 / 1500: loss 2.233639\n",
      "iteration 0 / 1500: loss 194.355470\n",
      "iteration 100 / 1500: loss 29.136755\n",
      "iteration 200 / 1500: loss 5.903358\n",
      "iteration 300 / 1500: loss 2.528813\n",
      "iteration 400 / 1500: loss 2.101841\n",
      "iteration 500 / 1500: loss 2.041820\n",
      "iteration 600 / 1500: loss 1.957434\n",
      "iteration 700 / 1500: loss 2.017660\n",
      "iteration 800 / 1500: loss 1.949311\n",
      "iteration 900 / 1500: loss 1.989270\n",
      "iteration 1000 / 1500: loss 1.944293\n",
      "iteration 1100 / 1500: loss 1.948267\n",
      "iteration 1200 / 1500: loss 1.980614\n",
      "iteration 1300 / 1500: loss 1.925854\n",
      "iteration 1400 / 1500: loss 1.854961\n",
      "iteration 0 / 1500: loss 1012.081765\n",
      "iteration 100 / 1500: loss 2.194789\n",
      "iteration 200 / 1500: loss 2.183377\n",
      "iteration 300 / 1500: loss 2.047723\n",
      "iteration 400 / 1500: loss 2.158528\n",
      "iteration 500 / 1500: loss 2.130926\n",
      "iteration 600 / 1500: loss 2.182012\n",
      "iteration 700 / 1500: loss 2.137281\n",
      "iteration 800 / 1500: loss 2.110629\n",
      "iteration 900 / 1500: loss 2.152829\n",
      "iteration 1000 / 1500: loss 2.063213\n",
      "iteration 1100 / 1500: loss 2.099127\n",
      "iteration 1200 / 1500: loss 2.158166\n",
      "iteration 1300 / 1500: loss 2.119866\n",
      "iteration 1400 / 1500: loss 2.075304\n",
      "iteration 0 / 1500: loss 230.503341\n",
      "iteration 100 / 1500: loss 24.131208\n",
      "iteration 200 / 1500: loss 4.233455\n",
      "iteration 300 / 1500: loss 2.244343\n",
      "iteration 400 / 1500: loss 2.045762\n",
      "iteration 500 / 1500: loss 1.955251\n",
      "iteration 600 / 1500: loss 1.988699\n",
      "iteration 700 / 1500: loss 2.000624\n",
      "iteration 800 / 1500: loss 1.986103\n",
      "iteration 900 / 1500: loss 1.996329\n",
      "iteration 1000 / 1500: loss 2.011501\n",
      "iteration 1100 / 1500: loss 2.007252\n",
      "iteration 1200 / 1500: loss 2.011999\n",
      "iteration 1300 / 1500: loss 1.984861\n",
      "iteration 1400 / 1500: loss 1.980176\n",
      "iteration 0 / 1500: loss 400.909196\n",
      "iteration 100 / 1500: loss 9.395463\n",
      "iteration 200 / 1500: loss 2.170653\n",
      "iteration 300 / 1500: loss 2.044865\n",
      "iteration 400 / 1500: loss 2.025634\n",
      "iteration 500 / 1500: loss 2.067711\n",
      "iteration 600 / 1500: loss 1.998094\n",
      "iteration 700 / 1500: loss 2.040950\n",
      "iteration 800 / 1500: loss 2.010282\n",
      "iteration 900 / 1500: loss 1.968281\n",
      "iteration 1000 / 1500: loss 1.998322\n",
      "iteration 1100 / 1500: loss 2.045402\n",
      "iteration 1200 / 1500: loss 2.063422\n",
      "iteration 1300 / 1500: loss 2.047754\n",
      "iteration 1400 / 1500: loss 2.084951\n",
      "iteration 0 / 1500: loss 803.708833\n",
      "iteration 100 / 1500: loss 2.296003\n",
      "iteration 200 / 1500: loss 2.121972\n",
      "iteration 300 / 1500: loss 2.110578\n",
      "iteration 400 / 1500: loss 2.098275\n",
      "iteration 500 / 1500: loss 2.084643\n",
      "iteration 600 / 1500: loss 2.156094\n",
      "iteration 700 / 1500: loss 2.035186\n",
      "iteration 800 / 1500: loss 2.136902\n",
      "iteration 900 / 1500: loss 2.107050\n",
      "iteration 1000 / 1500: loss 2.055131\n",
      "iteration 1100 / 1500: loss 2.078102\n",
      "iteration 1200 / 1500: loss 2.142094\n",
      "iteration 1300 / 1500: loss 2.166894\n",
      "iteration 1400 / 1500: loss 2.138552\n",
      "iteration 0 / 1500: loss 379.240225\n",
      "iteration 100 / 1500: loss 10.431435\n",
      "iteration 200 / 1500: loss 2.205517\n",
      "iteration 300 / 1500: loss 1.998315\n",
      "iteration 400 / 1500: loss 2.021187\n",
      "iteration 500 / 1500: loss 2.132747\n",
      "iteration 600 / 1500: loss 2.103926\n",
      "iteration 700 / 1500: loss 2.052983\n",
      "iteration 800 / 1500: loss 2.030035\n",
      "iteration 900 / 1500: loss 2.026564\n",
      "iteration 1000 / 1500: loss 2.043608\n",
      "iteration 1100 / 1500: loss 2.029103\n",
      "iteration 1200 / 1500: loss 1.996351\n",
      "iteration 1300 / 1500: loss 2.071007\n",
      "iteration 1400 / 1500: loss 2.087124\n",
      "iteration 0 / 1500: loss 556.337831\n",
      "iteration 100 / 1500: loss 173.947050\n",
      "iteration 200 / 1500: loss 55.662337\n",
      "iteration 300 / 1500: loss 18.726296\n",
      "iteration 400 / 1500: loss 7.286287\n",
      "iteration 500 / 1500: loss 3.729003\n",
      "iteration 600 / 1500: loss 2.525679\n",
      "iteration 700 / 1500: loss 2.221430\n",
      "iteration 800 / 1500: loss 2.102179\n",
      "iteration 900 / 1500: loss 2.070766\n",
      "iteration 1000 / 1500: loss 2.075564\n",
      "iteration 1100 / 1500: loss 1.979791\n",
      "iteration 1200 / 1500: loss 2.242554\n",
      "iteration 1300 / 1500: loss 2.075109\n",
      "iteration 1400 / 1500: loss 2.089796\n",
      "iteration 0 / 1500: loss 1505.555068\n",
      "iteration 100 / 1500: loss 65.002035\n",
      "iteration 200 / 1500: loss 4.805248\n",
      "iteration 300 / 1500: loss 2.236664\n",
      "iteration 400 / 1500: loss 2.152030\n",
      "iteration 500 / 1500: loss 2.116656\n",
      "iteration 600 / 1500: loss 2.139503\n",
      "iteration 700 / 1500: loss 2.162123\n",
      "iteration 800 / 1500: loss 2.160500\n",
      "iteration 900 / 1500: loss 2.077092\n",
      "iteration 1000 / 1500: loss 2.164098\n",
      "iteration 1100 / 1500: loss 2.158407\n",
      "iteration 1200 / 1500: loss 2.171024\n",
      "iteration 1300 / 1500: loss 2.117400\n",
      "iteration 1400 / 1500: loss 2.150072\n",
      "iteration 0 / 1500: loss 181.824218\n",
      "iteration 100 / 1500: loss 124.420700\n",
      "iteration 200 / 1500: loss 85.820051\n",
      "iteration 300 / 1500: loss 59.138132\n",
      "iteration 400 / 1500: loss 41.064872\n",
      "iteration 500 / 1500: loss 28.787178\n",
      "iteration 600 / 1500: loss 20.336211\n",
      "iteration 700 / 1500: loss 14.475911\n",
      "iteration 800 / 1500: loss 10.580653\n",
      "iteration 900 / 1500: loss 7.843425\n",
      "iteration 1000 / 1500: loss 6.033800\n",
      "iteration 1100 / 1500: loss 4.738221\n",
      "iteration 1200 / 1500: loss 3.829559\n",
      "iteration 1300 / 1500: loss 3.236074\n",
      "iteration 1400 / 1500: loss 2.836420\n",
      "iteration 0 / 1500: loss 1388.997035\n",
      "iteration 100 / 1500: loss 75.566106\n",
      "iteration 200 / 1500: loss 6.016936\n",
      "iteration 300 / 1500: loss 2.314716\n",
      "iteration 400 / 1500: loss 2.126812\n",
      "iteration 500 / 1500: loss 2.106657\n",
      "iteration 600 / 1500: loss 2.124588\n",
      "iteration 700 / 1500: loss 2.140381\n",
      "iteration 800 / 1500: loss 2.124571\n",
      "iteration 900 / 1500: loss 2.163003\n",
      "iteration 1000 / 1500: loss 2.158583\n",
      "iteration 1100 / 1500: loss 2.143643\n",
      "iteration 1200 / 1500: loss 2.139082\n",
      "iteration 1300 / 1500: loss 2.113890\n",
      "iteration 1400 / 1500: loss 2.166181\n",
      "iteration 0 / 1500: loss 195.528024\n",
      "iteration 100 / 1500: loss 130.810468\n",
      "iteration 200 / 1500: loss 87.881412\n",
      "iteration 300 / 1500: loss 59.309626\n",
      "iteration 400 / 1500: loss 40.271754\n",
      "iteration 500 / 1500: loss 27.582917\n",
      "iteration 600 / 1500: loss 19.168963\n",
      "iteration 700 / 1500: loss 13.451418\n",
      "iteration 800 / 1500: loss 9.652038\n",
      "iteration 900 / 1500: loss 7.114696\n",
      "iteration 1000 / 1500: loss 5.335431\n",
      "iteration 1100 / 1500: loss 4.296796\n",
      "iteration 1200 / 1500: loss 3.560901\n",
      "iteration 1300 / 1500: loss 2.994690\n",
      "iteration 1400 / 1500: loss 2.681715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 1025.293204\n",
      "iteration 100 / 1500: loss 122.284818\n",
      "iteration 200 / 1500: loss 16.230311\n",
      "iteration 300 / 1500: loss 3.790117\n",
      "iteration 400 / 1500: loss 2.329702\n",
      "iteration 500 / 1500: loss 2.147787\n",
      "iteration 600 / 1500: loss 2.128382\n",
      "iteration 700 / 1500: loss 2.127278\n",
      "iteration 800 / 1500: loss 2.145936\n",
      "iteration 900 / 1500: loss 2.090574\n",
      "iteration 1000 / 1500: loss 2.087247\n",
      "iteration 1100 / 1500: loss 2.145995\n",
      "iteration 1200 / 1500: loss 2.061621\n",
      "iteration 1300 / 1500: loss 2.133938\n",
      "iteration 1400 / 1500: loss 2.099996\n",
      "iteration 0 / 1500: loss 232.700365\n",
      "iteration 100 / 1500: loss 143.552689\n",
      "iteration 200 / 1500: loss 89.225591\n",
      "iteration 300 / 1500: loss 55.931488\n",
      "iteration 400 / 1500: loss 35.375946\n",
      "iteration 500 / 1500: loss 22.605433\n",
      "iteration 600 / 1500: loss 14.791391\n",
      "iteration 700 / 1500: loss 9.934226\n",
      "iteration 800 / 1500: loss 6.830588\n",
      "iteration 900 / 1500: loss 5.013594\n",
      "iteration 1000 / 1500: loss 3.890940\n",
      "iteration 1100 / 1500: loss 3.111506\n",
      "iteration 1200 / 1500: loss 2.725448\n",
      "iteration 1300 / 1500: loss 2.360036\n",
      "iteration 1400 / 1500: loss 2.223794\n",
      "iteration 0 / 1500: loss 382.915787\n",
      "iteration 100 / 1500: loss 168.498851\n",
      "iteration 200 / 1500: loss 75.022015\n",
      "iteration 300 / 1500: loss 34.135210\n",
      "iteration 400 / 1500: loss 16.204994\n",
      "iteration 500 / 1500: loss 8.267902\n",
      "iteration 600 / 1500: loss 4.720320\n",
      "iteration 700 / 1500: loss 3.169856\n",
      "iteration 800 / 1500: loss 2.570379\n",
      "iteration 900 / 1500: loss 2.249075\n",
      "iteration 1000 / 1500: loss 2.072783\n",
      "iteration 1100 / 1500: loss 2.054685\n",
      "iteration 1200 / 1500: loss 2.091501\n",
      "iteration 1300 / 1500: loss 2.049575\n",
      "iteration 1400 / 1500: loss 2.004589\n",
      "iteration 0 / 1500: loss 820.163543\n",
      "iteration 100 / 1500: loss 147.826050\n",
      "iteration 200 / 1500: loss 28.171064\n",
      "iteration 300 / 1500: loss 6.690438\n",
      "iteration 400 / 1500: loss 2.910472\n",
      "iteration 500 / 1500: loss 2.274166\n",
      "iteration 600 / 1500: loss 2.125929\n",
      "iteration 700 / 1500: loss 2.023188\n",
      "iteration 800 / 1500: loss 2.099457\n",
      "iteration 900 / 1500: loss 2.129839\n",
      "iteration 1000 / 1500: loss 2.079874\n",
      "iteration 1100 / 1500: loss 2.116872\n",
      "iteration 1200 / 1500: loss 2.087049\n",
      "iteration 1300 / 1500: loss 2.059669\n",
      "iteration 1400 / 1500: loss 2.088679\n",
      "iteration 0 / 1500: loss 373.081135\n",
      "iteration 100 / 1500: loss 170.402223\n",
      "iteration 200 / 1500: loss 78.956163\n",
      "iteration 300 / 1500: loss 37.104924\n",
      "iteration 400 / 1500: loss 18.044232\n",
      "iteration 500 / 1500: loss 9.383506\n",
      "iteration 600 / 1500: loss 5.419364\n",
      "iteration 700 / 1500: loss 3.580301\n",
      "iteration 800 / 1500: loss 2.794323\n",
      "iteration 900 / 1500: loss 2.395773\n",
      "iteration 1000 / 1500: loss 2.134642\n",
      "iteration 1100 / 1500: loss 2.087033\n",
      "iteration 1200 / 1500: loss 2.054856\n",
      "iteration 1300 / 1500: loss 2.055433\n",
      "iteration 1400 / 1500: loss 2.024220\n",
      "iteration 0 / 1500: loss 554.605277\n",
      "iteration 100 / 1500: loss 136.839474\n",
      "iteration 200 / 1500: loss 34.960584\n",
      "iteration 300 / 1500: loss 10.108302\n",
      "iteration 400 / 1500: loss 4.030684\n",
      "iteration 500 / 1500: loss 2.511812\n",
      "iteration 600 / 1500: loss 2.151556\n",
      "iteration 700 / 1500: loss 2.099683\n",
      "iteration 800 / 1500: loss 2.065812\n",
      "iteration 900 / 1500: loss 2.067017\n",
      "iteration 1000 / 1500: loss 2.100096\n",
      "iteration 1100 / 1500: loss 2.064012\n",
      "iteration 1200 / 1500: loss 2.128787\n",
      "iteration 1300 / 1500: loss 2.088150\n",
      "iteration 1400 / 1500: loss 2.010527\n",
      "iteration 0 / 1500: loss 1495.997431\n",
      "iteration 100 / 1500: loss 34.209460\n",
      "iteration 200 / 1500: loss 2.875764\n",
      "iteration 300 / 1500: loss 2.129767\n",
      "iteration 400 / 1500: loss 2.173339\n",
      "iteration 500 / 1500: loss 2.103596\n",
      "iteration 600 / 1500: loss 2.156903\n",
      "iteration 700 / 1500: loss 2.116216\n",
      "iteration 800 / 1500: loss 2.129333\n",
      "iteration 900 / 1500: loss 2.128665\n",
      "iteration 1000 / 1500: loss 2.158224\n",
      "iteration 1100 / 1500: loss 2.124279\n",
      "iteration 1200 / 1500: loss 2.098691\n",
      "iteration 1300 / 1500: loss 2.120440\n",
      "iteration 1400 / 1500: loss 2.120947\n",
      "iteration 0 / 1500: loss 185.234035\n",
      "iteration 100 / 1500: loss 116.700788\n",
      "iteration 200 / 1500: loss 74.504691\n",
      "iteration 300 / 1500: loss 47.683439\n",
      "iteration 400 / 1500: loss 30.963196\n",
      "iteration 500 / 1500: loss 20.235183\n",
      "iteration 600 / 1500: loss 13.583620\n",
      "iteration 700 / 1500: loss 9.293042\n",
      "iteration 800 / 1500: loss 6.588944\n",
      "iteration 900 / 1500: loss 4.961847\n",
      "iteration 1000 / 1500: loss 3.804148\n",
      "iteration 1100 / 1500: loss 3.146832\n",
      "iteration 1200 / 1500: loss 2.642634\n",
      "iteration 1300 / 1500: loss 2.437357\n",
      "iteration 1400 / 1500: loss 2.205744\n",
      "iteration 0 / 1500: loss 1385.578514\n",
      "iteration 100 / 1500: loss 41.575405\n",
      "iteration 200 / 1500: loss 3.270200\n",
      "iteration 300 / 1500: loss 2.211109\n",
      "iteration 400 / 1500: loss 2.128255\n",
      "iteration 500 / 1500: loss 2.077954\n",
      "iteration 600 / 1500: loss 2.076311\n",
      "iteration 700 / 1500: loss 2.146589\n",
      "iteration 800 / 1500: loss 2.134641\n",
      "iteration 900 / 1500: loss 2.136101\n",
      "iteration 1000 / 1500: loss 2.146827\n",
      "iteration 1100 / 1500: loss 2.097401\n",
      "iteration 1200 / 1500: loss 2.143379\n",
      "iteration 1300 / 1500: loss 2.119854\n",
      "iteration 1400 / 1500: loss 2.178129\n",
      "iteration 0 / 1500: loss 196.864686\n",
      "iteration 100 / 1500: loss 120.694275\n",
      "iteration 200 / 1500: loss 74.791151\n",
      "iteration 300 / 1500: loss 46.768468\n",
      "iteration 400 / 1500: loss 29.364495\n",
      "iteration 500 / 1500: loss 18.868793\n",
      "iteration 600 / 1500: loss 12.280286\n",
      "iteration 700 / 1500: loss 8.333576\n",
      "iteration 800 / 1500: loss 5.931504\n",
      "iteration 900 / 1500: loss 4.278167\n",
      "iteration 1000 / 1500: loss 3.490859\n",
      "iteration 1100 / 1500: loss 2.926419\n",
      "iteration 1200 / 1500: loss 2.570086\n",
      "iteration 1300 / 1500: loss 2.390364\n",
      "iteration 1400 / 1500: loss 2.184476\n",
      "iteration 0 / 1500: loss 1003.416499\n",
      "iteration 100 / 1500: loss 76.936692\n",
      "iteration 200 / 1500: loss 7.713896\n",
      "iteration 300 / 1500: loss 2.490698\n",
      "iteration 400 / 1500: loss 2.139166\n",
      "iteration 500 / 1500: loss 2.136905\n",
      "iteration 600 / 1500: loss 2.124618\n",
      "iteration 700 / 1500: loss 2.112155\n",
      "iteration 800 / 1500: loss 2.110092\n",
      "iteration 900 / 1500: loss 2.089216\n",
      "iteration 1000 / 1500: loss 2.110133\n",
      "iteration 1100 / 1500: loss 2.115488\n",
      "iteration 1200 / 1500: loss 2.056700\n",
      "iteration 1300 / 1500: loss 2.101460\n",
      "iteration 1400 / 1500: loss 2.143842\n",
      "iteration 0 / 1500: loss 232.425959\n",
      "iteration 100 / 1500: loss 129.553572\n",
      "iteration 200 / 1500: loss 73.208448\n",
      "iteration 300 / 1500: loss 41.688695\n",
      "iteration 400 / 1500: loss 24.217839\n",
      "iteration 500 / 1500: loss 14.299485\n",
      "iteration 600 / 1500: loss 8.924128\n",
      "iteration 700 / 1500: loss 5.842768\n",
      "iteration 800 / 1500: loss 4.132559\n",
      "iteration 900 / 1500: loss 3.187241\n",
      "iteration 1000 / 1500: loss 2.611701\n",
      "iteration 1100 / 1500: loss 2.367484\n",
      "iteration 1200 / 1500: loss 2.134038\n",
      "iteration 1300 / 1500: loss 2.076783\n",
      "iteration 1400 / 1500: loss 2.040007\n",
      "iteration 0 / 1500: loss 393.047098\n",
      "iteration 100 / 1500: loss 146.084571\n",
      "iteration 200 / 1500: loss 55.309284\n",
      "iteration 300 / 1500: loss 21.702120\n",
      "iteration 400 / 1500: loss 9.332682\n",
      "iteration 500 / 1500: loss 4.741086\n",
      "iteration 600 / 1500: loss 3.018411\n",
      "iteration 700 / 1500: loss 2.406949\n",
      "iteration 800 / 1500: loss 2.140939\n",
      "iteration 900 / 1500: loss 2.092150\n",
      "iteration 1000 / 1500: loss 2.083456\n",
      "iteration 1100 / 1500: loss 2.088031\n",
      "iteration 1200 / 1500: loss 1.988142\n",
      "iteration 1300 / 1500: loss 1.918761\n",
      "iteration 1400 / 1500: loss 2.014631\n",
      "iteration 0 / 1500: loss 807.018011\n",
      "iteration 100 / 1500: loss 101.792397\n",
      "iteration 200 / 1500: loss 14.508182\n",
      "iteration 300 / 1500: loss 3.640602\n",
      "iteration 400 / 1500: loss 2.258667\n",
      "iteration 500 / 1500: loss 2.146026\n",
      "iteration 600 / 1500: loss 2.119049\n",
      "iteration 700 / 1500: loss 2.076731\n",
      "iteration 800 / 1500: loss 2.051737\n",
      "iteration 900 / 1500: loss 2.075780\n",
      "iteration 1000 / 1500: loss 2.111672\n",
      "iteration 1100 / 1500: loss 2.045246\n",
      "iteration 1200 / 1500: loss 2.121351\n",
      "iteration 1300 / 1500: loss 2.104085\n",
      "iteration 1400 / 1500: loss 2.103247\n",
      "iteration 0 / 1500: loss 372.271819\n",
      "iteration 100 / 1500: loss 145.323113\n",
      "iteration 200 / 1500: loss 57.440908\n",
      "iteration 300 / 1500: loss 23.581691\n",
      "iteration 400 / 1500: loss 10.350650\n",
      "iteration 500 / 1500: loss 5.288969\n",
      "iteration 600 / 1500: loss 3.286018\n",
      "iteration 700 / 1500: loss 2.457531\n",
      "iteration 800 / 1500: loss 2.218900\n",
      "iteration 900 / 1500: loss 2.054814\n",
      "iteration 1000 / 1500: loss 1.998714\n",
      "iteration 1100 / 1500: loss 1.984590\n",
      "iteration 1200 / 1500: loss 1.959785\n",
      "iteration 1300 / 1500: loss 1.986601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.045547\n",
      "iteration 0 / 1500: loss 555.492273\n",
      "iteration 100 / 1500: loss 158.956364\n",
      "iteration 200 / 1500: loss 46.839215\n",
      "iteration 300 / 1500: loss 14.767974\n",
      "iteration 400 / 1500: loss 5.657640\n",
      "iteration 500 / 1500: loss 3.108097\n",
      "iteration 600 / 1500: loss 2.412721\n",
      "iteration 700 / 1500: loss 2.117208\n",
      "iteration 800 / 1500: loss 2.140308\n",
      "iteration 900 / 1500: loss 2.087018\n",
      "iteration 1000 / 1500: loss 2.071184\n",
      "iteration 1100 / 1500: loss 2.052593\n",
      "iteration 1200 / 1500: loss 2.085700\n",
      "iteration 1300 / 1500: loss 2.149969\n",
      "iteration 1400 / 1500: loss 2.049408\n",
      "iteration 0 / 1500: loss 1499.051301\n",
      "iteration 100 / 1500: loss 50.945459\n",
      "iteration 200 / 1500: loss 3.758554\n",
      "iteration 300 / 1500: loss 2.227938\n",
      "iteration 400 / 1500: loss 2.149992\n",
      "iteration 500 / 1500: loss 2.104145\n",
      "iteration 600 / 1500: loss 2.133299\n",
      "iteration 700 / 1500: loss 2.124542\n",
      "iteration 800 / 1500: loss 2.132034\n",
      "iteration 900 / 1500: loss 2.148156\n",
      "iteration 1000 / 1500: loss 2.136328\n",
      "iteration 1100 / 1500: loss 2.161761\n",
      "iteration 1200 / 1500: loss 2.156871\n",
      "iteration 1300 / 1500: loss 2.103116\n",
      "iteration 1400 / 1500: loss 2.168884\n",
      "iteration 0 / 1500: loss 182.617761\n",
      "iteration 100 / 1500: loss 120.865734\n",
      "iteration 200 / 1500: loss 81.036226\n",
      "iteration 300 / 1500: loss 54.392345\n",
      "iteration 400 / 1500: loss 36.749594\n",
      "iteration 500 / 1500: loss 25.170602\n",
      "iteration 600 / 1500: loss 17.412250\n",
      "iteration 700 / 1500: loss 12.134497\n",
      "iteration 800 / 1500: loss 8.701957\n",
      "iteration 900 / 1500: loss 6.549812\n",
      "iteration 1000 / 1500: loss 4.971314\n",
      "iteration 1100 / 1500: loss 3.938467\n",
      "iteration 1200 / 1500: loss 3.240051\n",
      "iteration 1300 / 1500: loss 2.885215\n",
      "iteration 1400 / 1500: loss 2.564732\n",
      "iteration 0 / 1500: loss 1380.451179\n",
      "iteration 100 / 1500: loss 60.150282\n",
      "iteration 200 / 1500: loss 4.601389\n",
      "iteration 300 / 1500: loss 2.268891\n",
      "iteration 400 / 1500: loss 2.124155\n",
      "iteration 500 / 1500: loss 2.107134\n",
      "iteration 600 / 1500: loss 2.102760\n",
      "iteration 700 / 1500: loss 2.059115\n",
      "iteration 800 / 1500: loss 2.108831\n",
      "iteration 900 / 1500: loss 2.135409\n",
      "iteration 1000 / 1500: loss 2.129523\n",
      "iteration 1100 / 1500: loss 2.090006\n",
      "iteration 1200 / 1500: loss 2.108264\n",
      "iteration 1300 / 1500: loss 2.130581\n",
      "iteration 1400 / 1500: loss 2.176775\n",
      "iteration 0 / 1500: loss 197.313082\n",
      "iteration 100 / 1500: loss 126.243544\n",
      "iteration 200 / 1500: loss 82.300422\n",
      "iteration 300 / 1500: loss 54.098417\n",
      "iteration 400 / 1500: loss 35.661286\n",
      "iteration 500 / 1500: loss 23.880990\n",
      "iteration 600 / 1500: loss 16.177342\n",
      "iteration 700 / 1500: loss 11.299968\n",
      "iteration 800 / 1500: loss 7.929023\n",
      "iteration 900 / 1500: loss 5.773648\n",
      "iteration 1000 / 1500: loss 4.483071\n",
      "iteration 1100 / 1500: loss 3.576483\n",
      "iteration 1200 / 1500: loss 2.992989\n",
      "iteration 1300 / 1500: loss 2.604705\n",
      "iteration 1400 / 1500: loss 2.371618\n",
      "iteration 0 / 1500: loss 1012.705508\n",
      "iteration 100 / 1500: loss 102.596276\n",
      "iteration 200 / 1500: loss 12.098235\n",
      "iteration 300 / 1500: loss 3.039750\n",
      "iteration 400 / 1500: loss 2.223463\n",
      "iteration 500 / 1500: loss 2.128170\n",
      "iteration 600 / 1500: loss 2.154433\n",
      "iteration 700 / 1500: loss 2.113590\n",
      "iteration 800 / 1500: loss 2.111381\n",
      "iteration 900 / 1500: loss 2.088477\n",
      "iteration 1000 / 1500: loss 2.135341\n",
      "iteration 1100 / 1500: loss 2.142971\n",
      "iteration 1200 / 1500: loss 2.067095\n",
      "iteration 1300 / 1500: loss 2.144207\n",
      "iteration 1400 / 1500: loss 2.052519\n",
      "iteration 0 / 1500: loss 229.482926\n",
      "iteration 100 / 1500: loss 136.174183\n",
      "iteration 200 / 1500: loss 81.440134\n",
      "iteration 300 / 1500: loss 49.325929\n",
      "iteration 400 / 1500: loss 30.300027\n",
      "iteration 500 / 1500: loss 18.740728\n",
      "iteration 600 / 1500: loss 12.052900\n",
      "iteration 700 / 1500: loss 7.955913\n",
      "iteration 800 / 1500: loss 5.489884\n",
      "iteration 900 / 1500: loss 4.148333\n",
      "iteration 1000 / 1500: loss 3.203238\n",
      "iteration 1100 / 1500: loss 2.747693\n",
      "iteration 1200 / 1500: loss 2.392939\n",
      "iteration 1300 / 1500: loss 2.185528\n",
      "iteration 1400 / 1500: loss 2.143804\n",
      "iteration 0 / 1500: loss 389.837678\n",
      "iteration 100 / 1500: loss 161.307516\n",
      "iteration 200 / 1500: loss 67.600152\n",
      "iteration 300 / 1500: loss 28.946488\n",
      "iteration 400 / 1500: loss 13.128530\n",
      "iteration 500 / 1500: loss 6.647455\n",
      "iteration 600 / 1500: loss 3.921575\n",
      "iteration 700 / 1500: loss 2.807414\n",
      "iteration 800 / 1500: loss 2.317906\n",
      "iteration 900 / 1500: loss 2.147346\n",
      "iteration 1000 / 1500: loss 2.106993\n",
      "iteration 1100 / 1500: loss 2.050395\n",
      "iteration 1200 / 1500: loss 2.072261\n",
      "iteration 1300 / 1500: loss 1.947234\n",
      "iteration 1400 / 1500: loss 2.052620\n",
      "iteration 0 / 1500: loss 816.669635\n",
      "iteration 100 / 1500: loss 129.143282\n",
      "iteration 200 / 1500: loss 21.875547\n",
      "iteration 300 / 1500: loss 5.171780\n",
      "iteration 400 / 1500: loss 2.529691\n",
      "iteration 500 / 1500: loss 2.106672\n",
      "iteration 600 / 1500: loss 2.076413\n",
      "iteration 700 / 1500: loss 2.076522\n",
      "iteration 800 / 1500: loss 2.129611\n",
      "iteration 900 / 1500: loss 2.067742\n",
      "iteration 1000 / 1500: loss 2.050461\n",
      "iteration 1100 / 1500: loss 2.031159\n",
      "iteration 1200 / 1500: loss 2.136809\n",
      "iteration 1300 / 1500: loss 2.119053\n",
      "iteration 1400 / 1500: loss 2.022694\n",
      "iteration 0 / 1500: loss 379.868582\n",
      "iteration 100 / 1500: loss 164.052651\n",
      "iteration 200 / 1500: loss 71.725073\n",
      "iteration 300 / 1500: loss 31.998976\n",
      "iteration 400 / 1500: loss 14.868549\n",
      "iteration 500 / 1500: loss 7.538077\n",
      "iteration 600 / 1500: loss 4.401421\n",
      "iteration 700 / 1500: loss 3.077312\n",
      "iteration 800 / 1500: loss 2.449436\n",
      "iteration 900 / 1500: loss 2.221314\n",
      "iteration 1000 / 1500: loss 2.103873\n",
      "iteration 1100 / 1500: loss 2.079109\n",
      "iteration 1200 / 1500: loss 2.076908\n",
      "iteration 1300 / 1500: loss 2.033958\n",
      "iteration 1400 / 1500: loss 2.060239\n",
      "iteration 0 / 1500: loss 557.708936\n",
      "iteration 100 / 1500: loss 207.298235\n",
      "iteration 200 / 1500: loss 78.030755\n",
      "iteration 300 / 1500: loss 30.241972\n",
      "iteration 400 / 1500: loss 12.428651\n",
      "iteration 500 / 1500: loss 5.880655\n",
      "iteration 600 / 1500: loss 3.448871\n",
      "iteration 700 / 1500: loss 2.612321\n",
      "iteration 800 / 1500: loss 2.219307\n",
      "iteration 900 / 1500: loss 2.115364\n",
      "iteration 1000 / 1500: loss 2.094670\n",
      "iteration 1100 / 1500: loss 2.045561\n",
      "iteration 1200 / 1500: loss 2.019360\n",
      "iteration 1300 / 1500: loss 2.094280\n",
      "iteration 1400 / 1500: loss 2.016019\n",
      "iteration 0 / 1500: loss 1507.118199\n",
      "iteration 100 / 1500: loss 102.751695\n",
      "iteration 200 / 1500: loss 8.919247\n",
      "iteration 300 / 1500: loss 2.591053\n",
      "iteration 400 / 1500: loss 2.203663\n",
      "iteration 500 / 1500: loss 2.113804\n",
      "iteration 600 / 1500: loss 2.165305\n",
      "iteration 700 / 1500: loss 2.159000\n",
      "iteration 800 / 1500: loss 2.119307\n",
      "iteration 900 / 1500: loss 2.164531\n",
      "iteration 1000 / 1500: loss 2.150822\n",
      "iteration 1100 / 1500: loss 2.120604\n",
      "iteration 1200 / 1500: loss 2.079408\n",
      "iteration 1300 / 1500: loss 2.116889\n",
      "iteration 1400 / 1500: loss 2.150252\n",
      "iteration 0 / 1500: loss 183.247435\n",
      "iteration 100 / 1500: loss 131.662025\n",
      "iteration 200 / 1500: loss 95.630308\n",
      "iteration 300 / 1500: loss 69.595723\n",
      "iteration 400 / 1500: loss 50.896185\n",
      "iteration 500 / 1500: loss 37.259202\n",
      "iteration 600 / 1500: loss 27.575510\n",
      "iteration 700 / 1500: loss 20.431410\n",
      "iteration 800 / 1500: loss 15.273999\n",
      "iteration 900 / 1500: loss 11.723333\n",
      "iteration 1000 / 1500: loss 9.082371\n",
      "iteration 1100 / 1500: loss 7.041861\n",
      "iteration 1200 / 1500: loss 5.627058\n",
      "iteration 1300 / 1500: loss 4.644001\n",
      "iteration 1400 / 1500: loss 3.952694\n",
      "iteration 0 / 1500: loss 1363.679893\n",
      "iteration 100 / 1500: loss 113.254759\n",
      "iteration 200 / 1500: loss 11.254143\n",
      "iteration 300 / 1500: loss 2.803817\n",
      "iteration 400 / 1500: loss 2.233800\n",
      "iteration 500 / 1500: loss 2.131357\n",
      "iteration 600 / 1500: loss 2.039296\n",
      "iteration 700 / 1500: loss 2.157362\n",
      "iteration 800 / 1500: loss 2.177563\n",
      "iteration 900 / 1500: loss 2.129519\n",
      "iteration 1000 / 1500: loss 2.141741\n",
      "iteration 1100 / 1500: loss 2.121935\n",
      "iteration 1200 / 1500: loss 2.149083\n",
      "iteration 1300 / 1500: loss 2.148477\n",
      "iteration 1400 / 1500: loss 2.063875\n",
      "iteration 0 / 1500: loss 195.547352\n",
      "iteration 100 / 1500: loss 137.760895\n",
      "iteration 200 / 1500: loss 98.485156\n",
      "iteration 300 / 1500: loss 70.400352\n",
      "iteration 400 / 1500: loss 50.473510\n",
      "iteration 500 / 1500: loss 36.280916\n",
      "iteration 600 / 1500: loss 26.305388\n",
      "iteration 700 / 1500: loss 19.349017\n",
      "iteration 800 / 1500: loss 14.330052\n",
      "iteration 900 / 1500: loss 10.676821\n",
      "iteration 1000 / 1500: loss 8.119912\n",
      "iteration 1100 / 1500: loss 6.426064\n",
      "iteration 1200 / 1500: loss 5.042839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 4.205977\n",
      "iteration 1400 / 1500: loss 3.561574\n",
      "iteration 0 / 1500: loss 1007.505217\n",
      "iteration 100 / 1500: loss 164.178968\n",
      "iteration 200 / 1500: loss 28.158207\n",
      "iteration 300 / 1500: loss 6.315103\n",
      "iteration 400 / 1500: loss 2.777971\n",
      "iteration 500 / 1500: loss 2.183224\n",
      "iteration 600 / 1500: loss 2.124511\n",
      "iteration 700 / 1500: loss 2.128069\n",
      "iteration 800 / 1500: loss 2.072084\n",
      "iteration 900 / 1500: loss 2.103139\n",
      "iteration 1000 / 1500: loss 2.142570\n",
      "iteration 1100 / 1500: loss 2.095101\n",
      "iteration 1200 / 1500: loss 2.064861\n",
      "iteration 1300 / 1500: loss 2.133005\n",
      "iteration 1400 / 1500: loss 2.164780\n",
      "iteration 0 / 1500: loss 230.381348\n",
      "iteration 100 / 1500: loss 152.436655\n",
      "iteration 200 / 1500: loss 101.750493\n",
      "iteration 300 / 1500: loss 68.058148\n",
      "iteration 400 / 1500: loss 45.806052\n",
      "iteration 500 / 1500: loss 31.044947\n",
      "iteration 600 / 1500: loss 21.312110\n",
      "iteration 700 / 1500: loss 14.883878\n",
      "iteration 800 / 1500: loss 10.431954\n",
      "iteration 900 / 1500: loss 7.623838\n",
      "iteration 1000 / 1500: loss 5.742951\n",
      "iteration 1100 / 1500: loss 4.455734\n",
      "iteration 1200 / 1500: loss 3.592527\n",
      "iteration 1300 / 1500: loss 3.130070\n",
      "iteration 1400 / 1500: loss 2.722952\n",
      "iteration 0 / 1500: loss 392.583178\n",
      "iteration 100 / 1500: loss 194.974109\n",
      "iteration 200 / 1500: loss 97.564953\n",
      "iteration 300 / 1500: loss 49.408595\n",
      "iteration 400 / 1500: loss 25.526781\n",
      "iteration 500 / 1500: loss 13.636173\n",
      "iteration 600 / 1500: loss 7.830543\n",
      "iteration 700 / 1500: loss 4.881431\n",
      "iteration 800 / 1500: loss 3.527847\n",
      "iteration 900 / 1500: loss 2.677550\n",
      "iteration 1000 / 1500: loss 2.319850\n",
      "iteration 1100 / 1500: loss 2.228548\n",
      "iteration 1200 / 1500: loss 2.122492\n",
      "iteration 1300 / 1500: loss 2.021912\n",
      "iteration 1400 / 1500: loss 2.058713\n",
      "iteration 0 / 1500: loss 824.507019\n",
      "iteration 100 / 1500: loss 190.996451\n",
      "iteration 200 / 1500: loss 45.502326\n",
      "iteration 300 / 1500: loss 12.108585\n",
      "iteration 400 / 1500: loss 4.392523\n",
      "iteration 500 / 1500: loss 2.635902\n",
      "iteration 600 / 1500: loss 2.226857\n",
      "iteration 700 / 1500: loss 2.142701\n",
      "iteration 800 / 1500: loss 2.082312\n",
      "iteration 900 / 1500: loss 2.046237\n",
      "iteration 1000 / 1500: loss 2.087190\n",
      "iteration 1100 / 1500: loss 2.046528\n",
      "iteration 1200 / 1500: loss 2.079933\n",
      "iteration 1300 / 1500: loss 2.099129\n",
      "iteration 1400 / 1500: loss 2.129860\n",
      "iteration 0 / 1500: loss 370.811566\n",
      "iteration 100 / 1500: loss 190.217100\n",
      "iteration 200 / 1500: loss 98.331205\n",
      "iteration 300 / 1500: loss 51.446005\n",
      "iteration 400 / 1500: loss 27.376844\n",
      "iteration 500 / 1500: loss 15.026449\n",
      "iteration 600 / 1500: loss 8.691616\n",
      "iteration 700 / 1500: loss 5.421544\n",
      "iteration 800 / 1500: loss 3.783414\n",
      "iteration 900 / 1500: loss 2.880074\n",
      "iteration 1000 / 1500: loss 2.491911\n",
      "iteration 1100 / 1500: loss 2.213844\n",
      "iteration 1200 / 1500: loss 2.137032\n",
      "iteration 1300 / 1500: loss 2.075339\n",
      "iteration 1400 / 1500: loss 2.106871\n",
      "iteration 0 / 1500: loss 541.500167\n",
      "iteration 100 / 1500: loss 112.579473\n",
      "iteration 200 / 1500: loss 24.742557\n",
      "iteration 300 / 1500: loss 6.715890\n",
      "iteration 400 / 1500: loss 3.004778\n",
      "iteration 500 / 1500: loss 2.313934\n",
      "iteration 600 / 1500: loss 2.073142\n",
      "iteration 700 / 1500: loss 2.050177\n",
      "iteration 800 / 1500: loss 2.035924\n",
      "iteration 900 / 1500: loss 2.001670\n",
      "iteration 1000 / 1500: loss 2.027115\n",
      "iteration 1100 / 1500: loss 2.012773\n",
      "iteration 1200 / 1500: loss 1.964802\n",
      "iteration 1300 / 1500: loss 2.046142\n",
      "iteration 1400 / 1500: loss 1.964684\n",
      "iteration 0 / 1500: loss 1479.310708\n",
      "iteration 100 / 1500: loss 21.801657\n",
      "iteration 200 / 1500: loss 2.427833\n",
      "iteration 300 / 1500: loss 2.139158\n",
      "iteration 400 / 1500: loss 2.089956\n",
      "iteration 500 / 1500: loss 2.138451\n",
      "iteration 600 / 1500: loss 2.147679\n",
      "iteration 700 / 1500: loss 2.111467\n",
      "iteration 800 / 1500: loss 2.087833\n",
      "iteration 900 / 1500: loss 2.157175\n",
      "iteration 1000 / 1500: loss 2.156870\n",
      "iteration 1100 / 1500: loss 2.165765\n",
      "iteration 1200 / 1500: loss 2.123266\n",
      "iteration 1300 / 1500: loss 2.106871\n",
      "iteration 1400 / 1500: loss 2.163059\n",
      "iteration 0 / 1500: loss 182.445028\n",
      "iteration 100 / 1500: loss 108.629057\n",
      "iteration 200 / 1500: loss 65.670227\n",
      "iteration 300 / 1500: loss 40.040309\n",
      "iteration 400 / 1500: loss 24.668795\n",
      "iteration 500 / 1500: loss 15.468559\n",
      "iteration 600 / 1500: loss 10.053413\n",
      "iteration 700 / 1500: loss 6.807241\n",
      "iteration 800 / 1500: loss 4.880501\n",
      "iteration 900 / 1500: loss 3.745900\n",
      "iteration 1000 / 1500: loss 2.949548\n",
      "iteration 1100 / 1500: loss 2.608981\n",
      "iteration 1200 / 1500: loss 2.369980\n",
      "iteration 1300 / 1500: loss 2.186080\n",
      "iteration 1400 / 1500: loss 2.225032\n",
      "iteration 0 / 1500: loss 1376.167855\n",
      "iteration 100 / 1500: loss 27.432065\n",
      "iteration 200 / 1500: loss 2.570787\n",
      "iteration 300 / 1500: loss 2.137538\n",
      "iteration 400 / 1500: loss 2.156047\n",
      "iteration 500 / 1500: loss 2.137497\n",
      "iteration 600 / 1500: loss 2.125007\n",
      "iteration 700 / 1500: loss 2.110354\n",
      "iteration 800 / 1500: loss 2.162048\n",
      "iteration 900 / 1500: loss 2.219300\n",
      "iteration 1000 / 1500: loss 2.164958\n",
      "iteration 1100 / 1500: loss 2.131519\n",
      "iteration 1200 / 1500: loss 2.102931\n",
      "iteration 1300 / 1500: loss 2.166269\n",
      "iteration 1400 / 1500: loss 2.166380\n",
      "iteration 0 / 1500: loss 195.364847\n",
      "iteration 100 / 1500: loss 112.912941\n",
      "iteration 200 / 1500: loss 66.058179\n",
      "iteration 300 / 1500: loss 39.021532\n",
      "iteration 400 / 1500: loss 23.503728\n",
      "iteration 500 / 1500: loss 14.440087\n",
      "iteration 600 / 1500: loss 9.157003\n",
      "iteration 700 / 1500: loss 6.114774\n",
      "iteration 800 / 1500: loss 4.433670\n",
      "iteration 900 / 1500: loss 3.350430\n",
      "iteration 1000 / 1500: loss 2.783688\n",
      "iteration 1100 / 1500: loss 2.384773\n",
      "iteration 1200 / 1500: loss 2.265009\n",
      "iteration 1300 / 1500: loss 2.127102\n",
      "iteration 1400 / 1500: loss 2.062365\n",
      "iteration 0 / 1500: loss 1015.954275\n",
      "iteration 100 / 1500: loss 57.395320\n",
      "iteration 200 / 1500: loss 5.142202\n",
      "iteration 300 / 1500: loss 2.275097\n",
      "iteration 400 / 1500: loss 2.093755\n",
      "iteration 500 / 1500: loss 2.065467\n",
      "iteration 600 / 1500: loss 2.140732\n",
      "iteration 700 / 1500: loss 2.138410\n",
      "iteration 800 / 1500: loss 2.124354\n",
      "iteration 900 / 1500: loss 2.126245\n",
      "iteration 1000 / 1500: loss 2.112728\n",
      "iteration 1100 / 1500: loss 2.085694\n",
      "iteration 1200 / 1500: loss 2.061120\n",
      "iteration 1300 / 1500: loss 2.034871\n",
      "iteration 1400 / 1500: loss 2.118688\n",
      "iteration 0 / 1500: loss 228.045029\n",
      "iteration 100 / 1500: loss 118.804630\n",
      "iteration 200 / 1500: loss 62.514908\n",
      "iteration 300 / 1500: loss 33.467768\n",
      "iteration 400 / 1500: loss 18.484604\n",
      "iteration 500 / 1500: loss 10.485415\n",
      "iteration 600 / 1500: loss 6.457172\n",
      "iteration 700 / 1500: loss 4.274029\n",
      "iteration 800 / 1500: loss 3.130789\n",
      "iteration 900 / 1500: loss 2.577101\n",
      "iteration 1000 / 1500: loss 2.319648\n",
      "iteration 1100 / 1500: loss 2.200165\n",
      "iteration 1200 / 1500: loss 2.020799\n",
      "iteration 1300 / 1500: loss 2.057992\n",
      "iteration 1400 / 1500: loss 2.015638\n",
      "iteration 0 / 1500: loss 393.195740\n",
      "iteration 100 / 1500: loss 129.414125\n",
      "iteration 200 / 1500: loss 43.792155\n",
      "iteration 300 / 1500: loss 15.798996\n",
      "iteration 400 / 1500: loss 6.507992\n",
      "iteration 500 / 1500: loss 3.469671\n",
      "iteration 600 / 1500: loss 2.476917\n",
      "iteration 700 / 1500: loss 2.198484\n",
      "iteration 800 / 1500: loss 2.116933\n",
      "iteration 900 / 1500: loss 2.086408\n",
      "iteration 1000 / 1500: loss 2.012496\n",
      "iteration 1100 / 1500: loss 2.031437\n",
      "iteration 1200 / 1500: loss 1.979135\n",
      "iteration 1300 / 1500: loss 2.020456\n",
      "iteration 1400 / 1500: loss 1.999078\n",
      "iteration 0 / 1500: loss 804.679140\n",
      "iteration 100 / 1500: loss 79.361700\n",
      "iteration 200 / 1500: loss 9.520530\n",
      "iteration 300 / 1500: loss 2.831293\n",
      "iteration 400 / 1500: loss 2.169678\n",
      "iteration 500 / 1500: loss 2.064559\n",
      "iteration 600 / 1500: loss 2.068760\n",
      "iteration 700 / 1500: loss 2.076039\n",
      "iteration 800 / 1500: loss 2.093335\n",
      "iteration 900 / 1500: loss 2.065706\n",
      "iteration 1000 / 1500: loss 2.096762\n",
      "iteration 1100 / 1500: loss 2.039307\n",
      "iteration 1200 / 1500: loss 2.112557\n",
      "iteration 1300 / 1500: loss 2.067557\n",
      "iteration 1400 / 1500: loss 2.112566\n",
      "iteration 0 / 1500: loss 371.774960\n",
      "iteration 100 / 1500: loss 128.921765\n",
      "iteration 200 / 1500: loss 45.781770\n",
      "iteration 300 / 1500: loss 17.251281\n",
      "iteration 400 / 1500: loss 7.252884\n",
      "iteration 500 / 1500: loss 3.894592\n",
      "iteration 600 / 1500: loss 2.591924\n",
      "iteration 700 / 1500: loss 2.229291\n",
      "iteration 800 / 1500: loss 2.034783\n",
      "iteration 900 / 1500: loss 2.117335\n",
      "iteration 1000 / 1500: loss 2.110635\n",
      "iteration 1100 / 1500: loss 2.073956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.048640\n",
      "iteration 1300 / 1500: loss 2.081340\n",
      "iteration 1400 / 1500: loss 1.913158\n",
      "iteration 0 / 1500: loss 553.660669\n",
      "iteration 100 / 1500: loss 53.609219\n",
      "iteration 200 / 1500: loss 6.889049\n",
      "iteration 300 / 1500: loss 2.533838\n",
      "iteration 400 / 1500: loss 2.122044\n",
      "iteration 500 / 1500: loss 2.065787\n",
      "iteration 600 / 1500: loss 2.069138\n",
      "iteration 700 / 1500: loss 2.075237\n",
      "iteration 800 / 1500: loss 2.046982\n",
      "iteration 900 / 1500: loss 2.064350\n",
      "iteration 1000 / 1500: loss 2.095801\n",
      "iteration 1100 / 1500: loss 2.125613\n",
      "iteration 1200 / 1500: loss 2.071891\n",
      "iteration 1300 / 1500: loss 2.050793\n",
      "iteration 1400 / 1500: loss 2.045862\n",
      "iteration 0 / 1500: loss 1512.618032\n",
      "iteration 100 / 1500: loss 4.426686\n",
      "iteration 200 / 1500: loss 2.124932\n",
      "iteration 300 / 1500: loss 2.128677\n",
      "iteration 400 / 1500: loss 2.120220\n",
      "iteration 500 / 1500: loss 2.180040\n",
      "iteration 600 / 1500: loss 2.120591\n",
      "iteration 700 / 1500: loss 2.158870\n",
      "iteration 800 / 1500: loss 2.088483\n",
      "iteration 900 / 1500: loss 2.164194\n",
      "iteration 1000 / 1500: loss 2.160984\n",
      "iteration 1100 / 1500: loss 2.160880\n",
      "iteration 1200 / 1500: loss 2.156582\n",
      "iteration 1300 / 1500: loss 2.156252\n",
      "iteration 1400 / 1500: loss 2.109335\n",
      "iteration 0 / 1500: loss 181.536272\n",
      "iteration 100 / 1500: loss 83.824705\n",
      "iteration 200 / 1500: loss 39.666791\n",
      "iteration 300 / 1500: loss 19.416282\n",
      "iteration 400 / 1500: loss 10.029291\n",
      "iteration 500 / 1500: loss 5.690349\n",
      "iteration 600 / 1500: loss 3.757613\n",
      "iteration 700 / 1500: loss 2.800255\n",
      "iteration 800 / 1500: loss 2.293593\n",
      "iteration 900 / 1500: loss 2.135345\n",
      "iteration 1000 / 1500: loss 1.954883\n",
      "iteration 1100 / 1500: loss 1.951510\n",
      "iteration 1200 / 1500: loss 1.964580\n",
      "iteration 1300 / 1500: loss 2.019899\n",
      "iteration 1400 / 1500: loss 1.941722\n",
      "iteration 0 / 1500: loss 1399.288259\n",
      "iteration 100 / 1500: loss 5.609464\n",
      "iteration 200 / 1500: loss 2.125973\n",
      "iteration 300 / 1500: loss 2.083191\n",
      "iteration 400 / 1500: loss 2.073383\n",
      "iteration 500 / 1500: loss 2.183115\n",
      "iteration 600 / 1500: loss 2.146573\n",
      "iteration 700 / 1500: loss 2.136983\n",
      "iteration 800 / 1500: loss 2.133920\n",
      "iteration 900 / 1500: loss 2.103323\n",
      "iteration 1000 / 1500: loss 2.163910\n",
      "iteration 1100 / 1500: loss 2.142825\n",
      "iteration 1200 / 1500: loss 2.148932\n",
      "iteration 1300 / 1500: loss 2.112273\n",
      "iteration 1400 / 1500: loss 2.079501\n",
      "iteration 0 / 1500: loss 195.233365\n",
      "iteration 100 / 1500: loss 86.736661\n",
      "iteration 200 / 1500: loss 39.295883\n",
      "iteration 300 / 1500: loss 18.580329\n",
      "iteration 400 / 1500: loss 9.337480\n",
      "iteration 500 / 1500: loss 5.216460\n",
      "iteration 600 / 1500: loss 3.414741\n",
      "iteration 700 / 1500: loss 2.591951\n",
      "iteration 800 / 1500: loss 2.321964\n",
      "iteration 900 / 1500: loss 2.170269\n",
      "iteration 1000 / 1500: loss 1.986299\n",
      "iteration 1100 / 1500: loss 1.986254\n",
      "iteration 1200 / 1500: loss 1.947007\n",
      "iteration 1300 / 1500: loss 1.914246\n",
      "iteration 1400 / 1500: loss 2.086629\n",
      "iteration 0 / 1500: loss 1003.705278\n",
      "iteration 100 / 1500: loss 14.909265\n",
      "iteration 200 / 1500: loss 2.261544\n",
      "iteration 300 / 1500: loss 2.121087\n",
      "iteration 400 / 1500: loss 2.108343\n",
      "iteration 500 / 1500: loss 2.088366\n",
      "iteration 600 / 1500: loss 2.102069\n",
      "iteration 700 / 1500: loss 2.061515\n",
      "iteration 800 / 1500: loss 2.120275\n",
      "iteration 900 / 1500: loss 2.083467\n",
      "iteration 1000 / 1500: loss 2.095751\n",
      "iteration 1100 / 1500: loss 2.102304\n",
      "iteration 1200 / 1500: loss 2.103073\n",
      "iteration 1300 / 1500: loss 2.139626\n",
      "iteration 1400 / 1500: loss 2.131553\n",
      "iteration 0 / 1500: loss 233.838574\n",
      "iteration 100 / 1500: loss 88.624774\n",
      "iteration 200 / 1500: loss 34.457034\n",
      "iteration 300 / 1500: loss 14.301682\n",
      "iteration 400 / 1500: loss 6.642938\n",
      "iteration 500 / 1500: loss 3.772158\n",
      "iteration 600 / 1500: loss 2.676159\n",
      "iteration 700 / 1500: loss 2.230060\n",
      "iteration 800 / 1500: loss 1.978893\n",
      "iteration 900 / 1500: loss 1.982217\n",
      "iteration 1000 / 1500: loss 2.057344\n",
      "iteration 1100 / 1500: loss 1.950437\n",
      "iteration 1200 / 1500: loss 2.062131\n",
      "iteration 1300 / 1500: loss 2.019261\n",
      "iteration 1400 / 1500: loss 1.893304\n",
      "iteration 0 / 1500: loss 397.893477\n",
      "iteration 100 / 1500: loss 76.054134\n",
      "iteration 200 / 1500: loss 15.971430\n",
      "iteration 300 / 1500: loss 4.628210\n",
      "iteration 400 / 1500: loss 2.651452\n",
      "iteration 500 / 1500: loss 2.149490\n",
      "iteration 600 / 1500: loss 2.097344\n",
      "iteration 700 / 1500: loss 2.061286\n",
      "iteration 800 / 1500: loss 1.952343\n",
      "iteration 900 / 1500: loss 2.006154\n",
      "iteration 1000 / 1500: loss 2.059523\n",
      "iteration 1100 / 1500: loss 2.013951\n",
      "iteration 1200 / 1500: loss 1.988385\n",
      "iteration 1300 / 1500: loss 2.044096\n",
      "iteration 1400 / 1500: loss 2.046936\n",
      "iteration 0 / 1500: loss 812.253072\n",
      "iteration 100 / 1500: loss 26.411704\n",
      "iteration 200 / 1500: loss 2.774371\n",
      "iteration 300 / 1500: loss 2.127209\n",
      "iteration 400 / 1500: loss 2.069337\n",
      "iteration 500 / 1500: loss 2.168073\n",
      "iteration 600 / 1500: loss 2.095669\n",
      "iteration 700 / 1500: loss 2.077087\n",
      "iteration 800 / 1500: loss 2.110703\n",
      "iteration 900 / 1500: loss 2.119765\n",
      "iteration 1000 / 1500: loss 2.095063\n",
      "iteration 1100 / 1500: loss 2.076692\n",
      "iteration 1200 / 1500: loss 2.080122\n",
      "iteration 1300 / 1500: loss 2.153892\n",
      "iteration 1400 / 1500: loss 2.107350\n",
      "iteration 0 / 1500: loss 375.672343\n",
      "iteration 100 / 1500: loss 77.692270\n",
      "iteration 200 / 1500: loss 17.486802\n",
      "iteration 300 / 1500: loss 5.221588\n",
      "iteration 400 / 1500: loss 2.660450\n",
      "iteration 500 / 1500: loss 2.105972\n",
      "iteration 600 / 1500: loss 2.002996\n",
      "iteration 700 / 1500: loss 2.045265\n",
      "iteration 800 / 1500: loss 2.005447\n",
      "iteration 900 / 1500: loss 1.964733\n",
      "iteration 1000 / 1500: loss 2.062131\n",
      "iteration 1100 / 1500: loss 2.026363\n",
      "iteration 1200 / 1500: loss 2.031188\n",
      "iteration 1300 / 1500: loss 1.982979\n",
      "iteration 1400 / 1500: loss 2.011727\n",
      "iteration 0 / 1500: loss 552.275003\n",
      "iteration 100 / 1500: loss 218.758655\n",
      "iteration 200 / 1500: loss 87.542768\n",
      "iteration 300 / 1500: loss 35.765634\n",
      "iteration 400 / 1500: loss 15.376091\n",
      "iteration 500 / 1500: loss 7.408485\n",
      "iteration 600 / 1500: loss 4.096599\n",
      "iteration 700 / 1500: loss 2.873230\n",
      "iteration 800 / 1500: loss 2.387397\n",
      "iteration 900 / 1500: loss 2.176594\n",
      "iteration 1000 / 1500: loss 2.133013\n",
      "iteration 1100 / 1500: loss 2.025094\n",
      "iteration 1200 / 1500: loss 2.070956\n",
      "iteration 1300 / 1500: loss 2.168078\n",
      "iteration 1400 / 1500: loss 2.100036\n",
      "iteration 0 / 1500: loss 1476.771084\n",
      "iteration 100 / 1500: loss 119.405587\n",
      "iteration 200 / 1500: loss 11.512454\n",
      "iteration 300 / 1500: loss 2.826572\n",
      "iteration 400 / 1500: loss 2.241353\n",
      "iteration 500 / 1500: loss 2.119918\n",
      "iteration 600 / 1500: loss 2.151597\n",
      "iteration 700 / 1500: loss 2.107921\n",
      "iteration 800 / 1500: loss 2.157204\n",
      "iteration 900 / 1500: loss 2.160279\n",
      "iteration 1000 / 1500: loss 2.177385\n",
      "iteration 1100 / 1500: loss 2.190349\n",
      "iteration 1200 / 1500: loss 2.144168\n",
      "iteration 1300 / 1500: loss 2.159914\n",
      "iteration 1400 / 1500: loss 2.108738\n",
      "iteration 0 / 1500: loss 183.706145\n",
      "iteration 100 / 1500: loss 134.785812\n",
      "iteration 200 / 1500: loss 99.742938\n",
      "iteration 300 / 1500: loss 74.289666\n",
      "iteration 400 / 1500: loss 55.274261\n",
      "iteration 500 / 1500: loss 41.313654\n",
      "iteration 600 / 1500: loss 31.059117\n",
      "iteration 700 / 1500: loss 23.474164\n",
      "iteration 800 / 1500: loss 17.882273\n",
      "iteration 900 / 1500: loss 13.839289\n",
      "iteration 1000 / 1500: loss 10.553815\n",
      "iteration 1100 / 1500: loss 8.372953\n",
      "iteration 1200 / 1500: loss 6.652651\n",
      "iteration 1300 / 1500: loss 5.448071\n",
      "iteration 1400 / 1500: loss 4.568786\n",
      "iteration 0 / 1500: loss 1390.376131\n",
      "iteration 100 / 1500: loss 135.210943\n",
      "iteration 200 / 1500: loss 14.931312\n",
      "iteration 300 / 1500: loss 3.329690\n",
      "iteration 400 / 1500: loss 2.243071\n",
      "iteration 500 / 1500: loss 2.143399\n",
      "iteration 600 / 1500: loss 2.206912\n",
      "iteration 700 / 1500: loss 2.143206\n",
      "iteration 800 / 1500: loss 2.118243\n",
      "iteration 900 / 1500: loss 2.111341\n",
      "iteration 1000 / 1500: loss 2.134582\n",
      "iteration 1100 / 1500: loss 2.110377\n",
      "iteration 1200 / 1500: loss 2.150505\n",
      "iteration 1300 / 1500: loss 2.173662\n",
      "iteration 1400 / 1500: loss 2.168096\n",
      "iteration 0 / 1500: loss 194.704497\n",
      "iteration 100 / 1500: loss 140.347897\n",
      "iteration 200 / 1500: loss 102.151349\n",
      "iteration 300 / 1500: loss 74.882720\n",
      "iteration 400 / 1500: loss 54.503630\n",
      "iteration 500 / 1500: loss 40.136409\n",
      "iteration 600 / 1500: loss 29.630847\n",
      "iteration 700 / 1500: loss 22.008837\n",
      "iteration 800 / 1500: loss 16.541702\n",
      "iteration 900 / 1500: loss 12.510174\n",
      "iteration 1000 / 1500: loss 9.664858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss 7.538877\n",
      "iteration 1200 / 1500: loss 5.945879\n",
      "iteration 1300 / 1500: loss 4.824656\n",
      "iteration 1400 / 1500: loss 4.039694\n",
      "iteration 0 / 1500: loss 1011.285587\n",
      "iteration 100 / 1500: loss 184.876557\n",
      "iteration 200 / 1500: loss 35.273696\n",
      "iteration 300 / 1500: loss 8.109782\n",
      "iteration 400 / 1500: loss 3.160752\n",
      "iteration 500 / 1500: loss 2.279803\n",
      "iteration 600 / 1500: loss 2.162677\n",
      "iteration 700 / 1500: loss 2.145790\n",
      "iteration 800 / 1500: loss 2.079983\n",
      "iteration 900 / 1500: loss 2.122629\n",
      "iteration 1000 / 1500: loss 2.095565\n",
      "iteration 1100 / 1500: loss 2.111645\n",
      "iteration 1200 / 1500: loss 2.113471\n",
      "iteration 1300 / 1500: loss 2.123054\n",
      "iteration 1400 / 1500: loss 2.089129\n",
      "iteration 0 / 1500: loss 230.879752\n",
      "iteration 100 / 1500: loss 156.487188\n",
      "iteration 200 / 1500: loss 107.111858\n",
      "iteration 300 / 1500: loss 73.417665\n",
      "iteration 400 / 1500: loss 50.575652\n",
      "iteration 500 / 1500: loss 35.112175\n",
      "iteration 600 / 1500: loss 24.540807\n",
      "iteration 700 / 1500: loss 17.335836\n",
      "iteration 800 / 1500: loss 12.383173\n",
      "iteration 900 / 1500: loss 9.157755\n",
      "iteration 1000 / 1500: loss 6.782727\n",
      "iteration 1100 / 1500: loss 5.318847\n",
      "iteration 1200 / 1500: loss 4.237715\n",
      "iteration 1300 / 1500: loss 3.539040\n",
      "iteration 1400 / 1500: loss 2.983270\n",
      "iteration 0 / 1500: loss 385.339565\n",
      "iteration 100 / 1500: loss 200.128657\n",
      "iteration 200 / 1500: loss 104.668919\n",
      "iteration 300 / 1500: loss 55.212881\n",
      "iteration 400 / 1500: loss 29.525672\n",
      "iteration 500 / 1500: loss 16.438726\n",
      "iteration 600 / 1500: loss 9.410333\n",
      "iteration 700 / 1500: loss 5.883201\n",
      "iteration 800 / 1500: loss 4.074456\n",
      "iteration 900 / 1500: loss 3.093958\n",
      "iteration 1000 / 1500: loss 2.518168\n",
      "iteration 1100 / 1500: loss 2.271723\n",
      "iteration 1200 / 1500: loss 2.113054\n",
      "iteration 1300 / 1500: loss 2.110066\n",
      "iteration 1400 / 1500: loss 2.059811\n",
      "iteration 0 / 1500: loss 831.105339\n",
      "iteration 100 / 1500: loss 211.245720\n",
      "iteration 200 / 1500: loss 55.001422\n",
      "iteration 300 / 1500: loss 15.506004\n",
      "iteration 400 / 1500: loss 5.486723\n",
      "iteration 500 / 1500: loss 2.870580\n",
      "iteration 600 / 1500: loss 2.310435\n",
      "iteration 700 / 1500: loss 2.110469\n",
      "iteration 800 / 1500: loss 2.121602\n",
      "iteration 900 / 1500: loss 2.129123\n",
      "iteration 1000 / 1500: loss 2.080646\n",
      "iteration 1100 / 1500: loss 2.092249\n",
      "iteration 1200 / 1500: loss 2.187208\n",
      "iteration 1300 / 1500: loss 2.055545\n",
      "iteration 1400 / 1500: loss 2.113695\n",
      "iteration 0 / 1500: loss 375.748135\n",
      "iteration 100 / 1500: loss 201.338830\n",
      "iteration 200 / 1500: loss 108.504731\n",
      "iteration 300 / 1500: loss 58.950624\n",
      "iteration 400 / 1500: loss 32.491066\n",
      "iteration 500 / 1500: loss 18.367169\n",
      "iteration 600 / 1500: loss 10.789149\n",
      "iteration 700 / 1500: loss 6.690410\n",
      "iteration 800 / 1500: loss 4.497081\n",
      "iteration 900 / 1500: loss 3.337545\n",
      "iteration 1000 / 1500: loss 2.664134\n",
      "iteration 1100 / 1500: loss 2.377110\n",
      "iteration 1200 / 1500: loss 2.283839\n",
      "iteration 1300 / 1500: loss 2.084944\n",
      "iteration 1400 / 1500: loss 2.151984\n",
      "iteration 0 / 1500: loss 558.611229\n",
      "iteration 100 / 1500: loss 262.060362\n",
      "iteration 200 / 1500: loss 123.633127\n",
      "iteration 300 / 1500: loss 59.112488\n",
      "iteration 400 / 1500: loss 28.741748\n",
      "iteration 500 / 1500: loss 14.507769\n",
      "iteration 600 / 1500: loss 7.820036\n",
      "iteration 700 / 1500: loss 4.781857\n",
      "iteration 800 / 1500: loss 3.399625\n",
      "iteration 900 / 1500: loss 2.689628\n",
      "iteration 1000 / 1500: loss 2.357463\n",
      "iteration 1100 / 1500: loss 2.160825\n",
      "iteration 1200 / 1500: loss 2.183329\n",
      "iteration 1300 / 1500: loss 2.069274\n",
      "iteration 1400 / 1500: loss 2.126437\n",
      "iteration 0 / 1500: loss 1505.153725\n",
      "iteration 100 / 1500: loss 192.865750\n",
      "iteration 200 / 1500: loss 26.358972\n",
      "iteration 300 / 1500: loss 5.202067\n",
      "iteration 400 / 1500: loss 2.469010\n",
      "iteration 500 / 1500: loss 2.173178\n",
      "iteration 600 / 1500: loss 2.192796\n",
      "iteration 700 / 1500: loss 2.106383\n",
      "iteration 800 / 1500: loss 2.168555\n",
      "iteration 900 / 1500: loss 2.123352\n",
      "iteration 1000 / 1500: loss 2.152518\n",
      "iteration 1100 / 1500: loss 2.219284\n",
      "iteration 1200 / 1500: loss 2.139295\n",
      "iteration 1300 / 1500: loss 2.147232\n",
      "iteration 1400 / 1500: loss 2.124701\n",
      "iteration 0 / 1500: loss 182.769936\n",
      "iteration 100 / 1500: loss 141.846626\n",
      "iteration 200 / 1500: loss 111.133899\n",
      "iteration 300 / 1500: loss 87.021909\n",
      "iteration 400 / 1500: loss 68.411426\n",
      "iteration 500 / 1500: loss 53.947492\n",
      "iteration 600 / 1500: loss 42.378534\n",
      "iteration 700 / 1500: loss 33.541628\n",
      "iteration 800 / 1500: loss 26.662357\n",
      "iteration 900 / 1500: loss 21.208179\n",
      "iteration 1000 / 1500: loss 17.018011\n",
      "iteration 1100 / 1500: loss 13.700628\n",
      "iteration 1200 / 1500: loss 11.225727\n",
      "iteration 1300 / 1500: loss 9.173602\n",
      "iteration 1400 / 1500: loss 7.560984\n",
      "iteration 0 / 1500: loss 1367.581001\n",
      "iteration 100 / 1500: loss 204.077449\n",
      "iteration 200 / 1500: loss 32.016697\n",
      "iteration 300 / 1500: loss 6.646513\n",
      "iteration 400 / 1500: loss 2.804226\n",
      "iteration 500 / 1500: loss 2.222121\n",
      "iteration 600 / 1500: loss 2.128405\n",
      "iteration 700 / 1500: loss 2.131734\n",
      "iteration 800 / 1500: loss 2.122666\n",
      "iteration 900 / 1500: loss 2.147325\n",
      "iteration 1000 / 1500: loss 2.066247\n",
      "iteration 1100 / 1500: loss 2.085640\n",
      "iteration 1200 / 1500: loss 2.098910\n",
      "iteration 1300 / 1500: loss 2.114015\n",
      "iteration 1400 / 1500: loss 2.158193\n",
      "iteration 0 / 1500: loss 195.065803\n",
      "iteration 100 / 1500: loss 148.834328\n",
      "iteration 200 / 1500: loss 114.611438\n",
      "iteration 300 / 1500: loss 88.609616\n",
      "iteration 400 / 1500: loss 68.529865\n",
      "iteration 500 / 1500: loss 53.285139\n",
      "iteration 600 / 1500: loss 41.398679\n",
      "iteration 700 / 1500: loss 32.201944\n",
      "iteration 800 / 1500: loss 25.271499\n",
      "iteration 900 / 1500: loss 19.989882\n",
      "iteration 1000 / 1500: loss 15.875270\n",
      "iteration 1100 / 1500: loss 12.580886\n",
      "iteration 1200 / 1500: loss 10.184668\n",
      "iteration 1300 / 1500: loss 8.254993\n",
      "iteration 1400 / 1500: loss 6.731774\n",
      "iteration 0 / 1500: loss 1011.456262\n",
      "iteration 100 / 1500: loss 252.313034\n",
      "iteration 200 / 1500: loss 64.252876\n",
      "iteration 300 / 1500: loss 17.531249\n",
      "iteration 400 / 1500: loss 5.975879\n",
      "iteration 500 / 1500: loss 3.070098\n",
      "iteration 600 / 1500: loss 2.339269\n",
      "iteration 700 / 1500: loss 2.129400\n",
      "iteration 800 / 1500: loss 2.154273\n",
      "iteration 900 / 1500: loss 2.132953\n",
      "iteration 1000 / 1500: loss 2.102024\n",
      "iteration 1100 / 1500: loss 2.126532\n",
      "iteration 1200 / 1500: loss 2.144273\n",
      "iteration 1300 / 1500: loss 2.112686\n",
      "iteration 1400 / 1500: loss 2.137679\n",
      "iteration 0 / 1500: loss 231.676371\n",
      "iteration 100 / 1500: loss 168.486494\n",
      "iteration 200 / 1500: loss 123.536986\n",
      "iteration 300 / 1500: loss 90.645153\n",
      "iteration 400 / 1500: loss 66.826808\n",
      "iteration 500 / 1500: loss 49.431050\n",
      "iteration 600 / 1500: loss 36.597829\n",
      "iteration 700 / 1500: loss 27.245732\n",
      "iteration 800 / 1500: loss 20.531613\n",
      "iteration 900 / 1500: loss 15.548340\n",
      "iteration 1000 / 1500: loss 11.908351\n",
      "iteration 1100 / 1500: loss 9.242447\n",
      "iteration 1200 / 1500: loss 7.297562\n",
      "iteration 1300 / 1500: loss 5.828595\n",
      "iteration 1400 / 1500: loss 4.764178\n",
      "iteration 0 / 1500: loss 391.940031\n",
      "iteration 100 / 1500: loss 229.116318\n",
      "iteration 200 / 1500: loss 134.880418\n",
      "iteration 300 / 1500: loss 79.569641\n",
      "iteration 400 / 1500: loss 47.469035\n",
      "iteration 500 / 1500: loss 28.662028\n",
      "iteration 600 / 1500: loss 17.565237\n",
      "iteration 700 / 1500: loss 11.145735\n",
      "iteration 800 / 1500: loss 7.424033\n",
      "iteration 900 / 1500: loss 5.138705\n",
      "iteration 1000 / 1500: loss 3.818664\n",
      "iteration 1100 / 1500: loss 3.136856\n",
      "iteration 1200 / 1500: loss 2.707634\n",
      "iteration 1300 / 1500: loss 2.415183\n",
      "iteration 1400 / 1500: loss 2.192490\n",
      "iteration 0 / 1500: loss 825.560847\n",
      "iteration 100 / 1500: loss 269.419703\n",
      "iteration 200 / 1500: loss 89.245000\n",
      "iteration 300 / 1500: loss 30.474412\n",
      "iteration 400 / 1500: loss 11.310524\n",
      "iteration 500 / 1500: loss 5.100018\n",
      "iteration 600 / 1500: loss 3.079012\n",
      "iteration 700 / 1500: loss 2.428768\n",
      "iteration 800 / 1500: loss 2.180931\n",
      "iteration 900 / 1500: loss 2.124023\n",
      "iteration 1000 / 1500: loss 2.089649\n",
      "iteration 1100 / 1500: loss 2.042467\n",
      "iteration 1200 / 1500: loss 2.060671\n",
      "iteration 1300 / 1500: loss 2.068131\n",
      "iteration 1400 / 1500: loss 2.201140\n",
      "iteration 0 / 1500: loss 373.726162\n",
      "iteration 100 / 1500: loss 224.433790\n",
      "iteration 200 / 1500: loss 135.476309\n",
      "iteration 300 / 1500: loss 82.113651\n",
      "iteration 400 / 1500: loss 50.195575\n",
      "iteration 500 / 1500: loss 30.862460\n",
      "iteration 600 / 1500: loss 19.310133\n",
      "iteration 700 / 1500: loss 12.416816\n",
      "iteration 800 / 1500: loss 8.231287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1500: loss 5.811017\n",
      "iteration 1000 / 1500: loss 4.316980\n",
      "iteration 1100 / 1500: loss 3.384606\n",
      "iteration 1200 / 1500: loss 2.813992\n",
      "iteration 1300 / 1500: loss 2.515863\n",
      "iteration 1400 / 1500: loss 2.321453\n",
      "iteration 0 / 1500: loss 554.058465\n",
      "iteration 100 / 1500: loss 9.143054\n",
      "iteration 200 / 1500: loss 2.111841\n",
      "iteration 300 / 1500: loss 2.014030\n",
      "iteration 400 / 1500: loss 2.041840\n",
      "iteration 500 / 1500: loss 2.079215\n",
      "iteration 600 / 1500: loss 2.037501\n",
      "iteration 700 / 1500: loss 2.055130\n",
      "iteration 800 / 1500: loss 2.097940\n",
      "iteration 900 / 1500: loss 2.131725\n",
      "iteration 1000 / 1500: loss 2.009609\n",
      "iteration 1100 / 1500: loss 2.050866\n",
      "iteration 1200 / 1500: loss 2.035469\n",
      "iteration 1300 / 1500: loss 2.027712\n",
      "iteration 1400 / 1500: loss 2.099015\n",
      "iteration 0 / 1500: loss 1497.899569\n",
      "iteration 100 / 1500: loss 2.161136\n",
      "iteration 200 / 1500: loss 2.183655\n",
      "iteration 300 / 1500: loss 2.189580\n",
      "iteration 400 / 1500: loss 2.118906\n",
      "iteration 500 / 1500: loss 2.167166\n",
      "iteration 600 / 1500: loss 2.162016\n",
      "iteration 700 / 1500: loss 2.098730\n",
      "iteration 800 / 1500: loss 2.111206\n",
      "iteration 900 / 1500: loss 2.146199\n",
      "iteration 1000 / 1500: loss 2.167099\n",
      "iteration 1100 / 1500: loss 2.129097\n",
      "iteration 1200 / 1500: loss 2.143265\n",
      "iteration 1300 / 1500: loss 2.138850\n",
      "iteration 1400 / 1500: loss 2.153993\n",
      "iteration 0 / 1500: loss 183.150714\n",
      "iteration 100 / 1500: loss 45.793631\n",
      "iteration 200 / 1500: loss 12.768230\n",
      "iteration 300 / 1500: loss 4.566934\n",
      "iteration 400 / 1500: loss 2.545344\n",
      "iteration 500 / 1500: loss 2.212500\n",
      "iteration 600 / 1500: loss 2.052511\n",
      "iteration 700 / 1500: loss 1.961441\n",
      "iteration 800 / 1500: loss 1.940591\n",
      "iteration 900 / 1500: loss 1.905556\n",
      "iteration 1000 / 1500: loss 1.946755\n",
      "iteration 1100 / 1500: loss 1.969545\n",
      "iteration 1200 / 1500: loss 2.005947\n",
      "iteration 1300 / 1500: loss 1.953548\n",
      "iteration 1400 / 1500: loss 1.914063\n",
      "iteration 0 / 1500: loss 1385.348483\n",
      "iteration 100 / 1500: loss 2.198681\n",
      "iteration 200 / 1500: loss 2.101960\n",
      "iteration 300 / 1500: loss 2.155848\n",
      "iteration 400 / 1500: loss 2.103821\n",
      "iteration 500 / 1500: loss 2.177273\n",
      "iteration 600 / 1500: loss 2.145321\n",
      "iteration 700 / 1500: loss 2.118140\n",
      "iteration 800 / 1500: loss 2.094626\n",
      "iteration 900 / 1500: loss 2.225997\n",
      "iteration 1000 / 1500: loss 2.127742\n",
      "iteration 1100 / 1500: loss 2.108278\n",
      "iteration 1200 / 1500: loss 2.146359\n",
      "iteration 1300 / 1500: loss 2.134005\n",
      "iteration 1400 / 1500: loss 2.137295\n",
      "iteration 0 / 1500: loss 195.552606\n",
      "iteration 100 / 1500: loss 44.925610\n",
      "iteration 200 / 1500: loss 11.669328\n",
      "iteration 300 / 1500: loss 4.071925\n",
      "iteration 400 / 1500: loss 2.467439\n",
      "iteration 500 / 1500: loss 2.062434\n",
      "iteration 600 / 1500: loss 1.963816\n",
      "iteration 700 / 1500: loss 1.908569\n",
      "iteration 800 / 1500: loss 1.909731\n",
      "iteration 900 / 1500: loss 1.882024\n",
      "iteration 1000 / 1500: loss 1.957416\n",
      "iteration 1100 / 1500: loss 1.973937\n",
      "iteration 1200 / 1500: loss 1.951620\n",
      "iteration 1300 / 1500: loss 1.920954\n",
      "iteration 1400 / 1500: loss 1.983884\n",
      "iteration 0 / 1500: loss 1017.925668\n",
      "iteration 100 / 1500: loss 2.484736\n",
      "iteration 200 / 1500: loss 2.108717\n",
      "iteration 300 / 1500: loss 2.134136\n",
      "iteration 400 / 1500: loss 2.137169\n",
      "iteration 500 / 1500: loss 2.127541\n",
      "iteration 600 / 1500: loss 2.110851\n",
      "iteration 700 / 1500: loss 2.163164\n",
      "iteration 800 / 1500: loss 2.084016\n",
      "iteration 900 / 1500: loss 2.219323\n",
      "iteration 1000 / 1500: loss 2.128844\n",
      "iteration 1100 / 1500: loss 2.049103\n",
      "iteration 1200 / 1500: loss 2.066972\n",
      "iteration 1300 / 1500: loss 2.107042\n",
      "iteration 1400 / 1500: loss 2.110034\n",
      "iteration 0 / 1500: loss 230.870677\n",
      "iteration 100 / 1500: loss 39.644949\n",
      "iteration 200 / 1500: loss 8.284408\n",
      "iteration 300 / 1500: loss 3.101464\n",
      "iteration 400 / 1500: loss 2.220491\n",
      "iteration 500 / 1500: loss 1.982006\n",
      "iteration 600 / 1500: loss 2.011624\n",
      "iteration 700 / 1500: loss 1.950514\n",
      "iteration 800 / 1500: loss 1.904348\n",
      "iteration 900 / 1500: loss 1.950810\n",
      "iteration 1000 / 1500: loss 1.958510\n",
      "iteration 1100 / 1500: loss 2.007569\n",
      "iteration 1200 / 1500: loss 1.993938\n",
      "iteration 1300 / 1500: loss 1.987460\n",
      "iteration 1400 / 1500: loss 1.975774\n",
      "iteration 0 / 1500: loss 396.383271\n",
      "iteration 100 / 1500: loss 20.181856\n",
      "iteration 200 / 1500: loss 2.938598\n",
      "iteration 300 / 1500: loss 2.085086\n",
      "iteration 400 / 1500: loss 2.033060\n",
      "iteration 500 / 1500: loss 2.061104\n",
      "iteration 600 / 1500: loss 2.064309\n",
      "iteration 700 / 1500: loss 1.999097\n",
      "iteration 800 / 1500: loss 2.090570\n",
      "iteration 900 / 1500: loss 2.071017\n",
      "iteration 1000 / 1500: loss 2.085206\n",
      "iteration 1100 / 1500: loss 2.034126\n",
      "iteration 1200 / 1500: loss 2.096961\n",
      "iteration 1300 / 1500: loss 2.066976\n",
      "iteration 1400 / 1500: loss 1.981944\n",
      "iteration 0 / 1500: loss 818.576957\n",
      "iteration 100 / 1500: loss 3.349204\n",
      "iteration 200 / 1500: loss 2.120885\n",
      "iteration 300 / 1500: loss 2.133856\n",
      "iteration 400 / 1500: loss 2.081956\n",
      "iteration 500 / 1500: loss 2.158176\n",
      "iteration 600 / 1500: loss 2.072599\n",
      "iteration 700 / 1500: loss 2.129316\n",
      "iteration 800 / 1500: loss 2.075018\n",
      "iteration 900 / 1500: loss 2.054272\n",
      "iteration 1000 / 1500: loss 2.068643\n",
      "iteration 1100 / 1500: loss 2.145008\n",
      "iteration 1200 / 1500: loss 2.121802\n",
      "iteration 1300 / 1500: loss 2.119098\n",
      "iteration 1400 / 1500: loss 2.090367\n",
      "iteration 0 / 1500: loss 369.324421\n",
      "iteration 100 / 1500: loss 21.700681\n",
      "iteration 200 / 1500: loss 3.077318\n",
      "iteration 300 / 1500: loss 2.080836\n",
      "iteration 400 / 1500: loss 2.031988\n",
      "iteration 500 / 1500: loss 1.968021\n",
      "iteration 600 / 1500: loss 2.106101\n",
      "iteration 700 / 1500: loss 2.010918\n",
      "iteration 800 / 1500: loss 2.031661\n",
      "iteration 900 / 1500: loss 2.022432\n",
      "iteration 1000 / 1500: loss 1.992599\n",
      "iteration 1100 / 1500: loss 2.030643\n",
      "iteration 1200 / 1500: loss 2.039595\n",
      "iteration 1300 / 1500: loss 2.051611\n",
      "iteration 1400 / 1500: loss 2.083659\n",
      "lr 1.057975e-07 reg 1.158495e+04 train accuracy: 0.348224 val accuracy: 0.348000\n",
      "lr 1.057975e-07 reg 1.228728e+04 train accuracy: 0.350469 val accuracy: 0.352000\n",
      "lr 1.057975e-07 reg 1.471586e+04 train accuracy: 0.353551 val accuracy: 0.366000\n",
      "lr 1.057975e-07 reg 2.397229e+04 train accuracy: 0.349592 val accuracy: 0.366000\n",
      "lr 1.057975e-07 reg 2.517498e+04 train accuracy: 0.351959 val accuracy: 0.368000\n",
      "lr 1.057975e-07 reg 3.568364e+04 train accuracy: 0.340224 val accuracy: 0.357000\n",
      "lr 1.057975e-07 reg 5.271420e+04 train accuracy: 0.327286 val accuracy: 0.340000\n",
      "lr 1.057975e-07 reg 6.548155e+04 train accuracy: 0.318469 val accuracy: 0.335000\n",
      "lr 1.057975e-07 reg 8.972204e+04 train accuracy: 0.307082 val accuracy: 0.327000\n",
      "lr 1.057975e-07 reg 9.685758e+04 train accuracy: 0.307102 val accuracy: 0.324000\n",
      "lr 1.296016e-07 reg 1.158495e+04 train accuracy: 0.358694 val accuracy: 0.357000\n",
      "lr 1.296016e-07 reg 1.228728e+04 train accuracy: 0.359816 val accuracy: 0.379000\n",
      "lr 1.296016e-07 reg 1.471586e+04 train accuracy: 0.360265 val accuracy: 0.376000\n",
      "lr 1.296016e-07 reg 2.397229e+04 train accuracy: 0.351163 val accuracy: 0.366000\n",
      "lr 1.296016e-07 reg 2.517498e+04 train accuracy: 0.342980 val accuracy: 0.353000\n",
      "lr 1.296016e-07 reg 3.568364e+04 train accuracy: 0.335959 val accuracy: 0.349000\n",
      "lr 1.296016e-07 reg 5.271420e+04 train accuracy: 0.327245 val accuracy: 0.346000\n",
      "lr 1.296016e-07 reg 6.548155e+04 train accuracy: 0.322837 val accuracy: 0.337000\n",
      "lr 1.296016e-07 reg 8.972204e+04 train accuracy: 0.315327 val accuracy: 0.331000\n",
      "lr 1.296016e-07 reg 9.685758e+04 train accuracy: 0.311898 val accuracy: 0.329000\n",
      "lr 1.384452e-07 reg 1.158495e+04 train accuracy: 0.362163 val accuracy: 0.377000\n",
      "lr 1.384452e-07 reg 1.228728e+04 train accuracy: 0.365612 val accuracy: 0.380000\n",
      "lr 1.384452e-07 reg 1.471586e+04 train accuracy: 0.362469 val accuracy: 0.368000\n",
      "lr 1.384452e-07 reg 2.397229e+04 train accuracy: 0.352816 val accuracy: 0.370000\n",
      "lr 1.384452e-07 reg 2.517498e+04 train accuracy: 0.354653 val accuracy: 0.362000\n",
      "lr 1.384452e-07 reg 3.568364e+04 train accuracy: 0.340020 val accuracy: 0.349000\n",
      "lr 1.384452e-07 reg 5.271420e+04 train accuracy: 0.325000 val accuracy: 0.336000\n",
      "lr 1.384452e-07 reg 6.548155e+04 train accuracy: 0.322776 val accuracy: 0.334000\n",
      "lr 1.384452e-07 reg 8.972204e+04 train accuracy: 0.308633 val accuracy: 0.322000\n",
      "lr 1.384452e-07 reg 9.685758e+04 train accuracy: 0.310184 val accuracy: 0.315000\n",
      "lr 1.622097e-07 reg 1.158495e+04 train accuracy: 0.367245 val accuracy: 0.380000\n",
      "lr 1.622097e-07 reg 1.228728e+04 train accuracy: 0.364184 val accuracy: 0.371000\n",
      "lr 1.622097e-07 reg 1.471586e+04 train accuracy: 0.365939 val accuracy: 0.377000\n",
      "lr 1.622097e-07 reg 2.397229e+04 train accuracy: 0.352184 val accuracy: 0.368000\n",
      "lr 1.622097e-07 reg 2.517498e+04 train accuracy: 0.346347 val accuracy: 0.361000\n",
      "lr 1.622097e-07 reg 3.568364e+04 train accuracy: 0.342469 val accuracy: 0.354000\n",
      "lr 1.622097e-07 reg 5.271420e+04 train accuracy: 0.327776 val accuracy: 0.340000\n",
      "lr 1.622097e-07 reg 6.548155e+04 train accuracy: 0.321898 val accuracy: 0.336000\n",
      "lr 1.622097e-07 reg 8.972204e+04 train accuracy: 0.317204 val accuracy: 0.328000\n",
      "lr 1.622097e-07 reg 9.685758e+04 train accuracy: 0.311633 val accuracy: 0.320000\n",
      "lr 1.748680e-07 reg 1.158495e+04 train accuracy: 0.366122 val accuracy: 0.366000\n",
      "lr 1.748680e-07 reg 1.228728e+04 train accuracy: 0.367939 val accuracy: 0.379000\n",
      "lr 1.748680e-07 reg 1.471586e+04 train accuracy: 0.364510 val accuracy: 0.377000\n",
      "lr 1.748680e-07 reg 2.397229e+04 train accuracy: 0.354837 val accuracy: 0.375000\n",
      "lr 1.748680e-07 reg 2.517498e+04 train accuracy: 0.350163 val accuracy: 0.364000\n",
      "lr 1.748680e-07 reg 3.568364e+04 train accuracy: 0.335061 val accuracy: 0.349000\n",
      "lr 1.748680e-07 reg 5.271420e+04 train accuracy: 0.322245 val accuracy: 0.332000\n",
      "lr 1.748680e-07 reg 6.548155e+04 train accuracy: 0.313918 val accuracy: 0.330000\n",
      "lr 1.748680e-07 reg 8.972204e+04 train accuracy: 0.311245 val accuracy: 0.332000\n",
      "lr 1.748680e-07 reg 9.685758e+04 train accuracy: 0.309735 val accuracy: 0.326000\n",
      "lr 1.961245e-07 reg 1.158495e+04 train accuracy: 0.369306 val accuracy: 0.382000\n",
      "lr 1.961245e-07 reg 1.228728e+04 train accuracy: 0.368510 val accuracy: 0.385000\n",
      "lr 1.961245e-07 reg 1.471586e+04 train accuracy: 0.366408 val accuracy: 0.385000\n",
      "lr 1.961245e-07 reg 2.397229e+04 train accuracy: 0.354735 val accuracy: 0.361000\n",
      "lr 1.961245e-07 reg 2.517498e+04 train accuracy: 0.348735 val accuracy: 0.355000\n",
      "lr 1.961245e-07 reg 3.568364e+04 train accuracy: 0.342490 val accuracy: 0.346000\n",
      "lr 1.961245e-07 reg 5.271420e+04 train accuracy: 0.328633 val accuracy: 0.349000\n",
      "lr 1.961245e-07 reg 6.548155e+04 train accuracy: 0.320224 val accuracy: 0.336000\n",
      "lr 1.961245e-07 reg 8.972204e+04 train accuracy: 0.314082 val accuracy: 0.322000\n",
      "lr 1.961245e-07 reg 9.685758e+04 train accuracy: 0.310694 val accuracy: 0.321000\n",
      "lr 2.199444e-07 reg 1.158495e+04 train accuracy: 0.371612 val accuracy: 0.378000\n",
      "lr 2.199444e-07 reg 1.228728e+04 train accuracy: 0.370408 val accuracy: 0.378000\n",
      "lr 2.199444e-07 reg 1.471586e+04 train accuracy: 0.364714 val accuracy: 0.378000\n",
      "lr 2.199444e-07 reg 2.397229e+04 train accuracy: 0.355531 val accuracy: 0.372000\n",
      "lr 2.199444e-07 reg 2.517498e+04 train accuracy: 0.352347 val accuracy: 0.364000\n",
      "lr 2.199444e-07 reg 3.568364e+04 train accuracy: 0.339918 val accuracy: 0.354000\n",
      "lr 2.199444e-07 reg 5.271420e+04 train accuracy: 0.334714 val accuracy: 0.339000\n",
      "lr 2.199444e-07 reg 6.548155e+04 train accuracy: 0.316694 val accuracy: 0.337000\n",
      "lr 2.199444e-07 reg 8.972204e+04 train accuracy: 0.303347 val accuracy: 0.317000\n",
      "lr 2.199444e-07 reg 9.685758e+04 train accuracy: 0.304224 val accuracy: 0.324000\n",
      "lr 3.285829e-07 reg 1.158495e+04 train accuracy: 0.372490 val accuracy: 0.383000\n",
      "lr 3.285829e-07 reg 1.228728e+04 train accuracy: 0.368469 val accuracy: 0.388000\n",
      "lr 3.285829e-07 reg 1.471586e+04 train accuracy: 0.359306 val accuracy: 0.371000\n",
      "lr 3.285829e-07 reg 2.397229e+04 train accuracy: 0.351898 val accuracy: 0.363000\n",
      "lr 3.285829e-07 reg 2.517498e+04 train accuracy: 0.352592 val accuracy: 0.370000\n",
      "lr 3.285829e-07 reg 3.568364e+04 train accuracy: 0.338714 val accuracy: 0.355000\n",
      "lr 3.285829e-07 reg 5.271420e+04 train accuracy: 0.324755 val accuracy: 0.331000\n",
      "lr 3.285829e-07 reg 6.548155e+04 train accuracy: 0.318510 val accuracy: 0.321000\n",
      "lr 3.285829e-07 reg 8.972204e+04 train accuracy: 0.297714 val accuracy: 0.315000\n",
      "lr 3.285829e-07 reg 9.685758e+04 train accuracy: 0.313796 val accuracy: 0.341000\n",
      "lr 6.016408e-07 reg 1.158495e+04 train accuracy: 0.367735 val accuracy: 0.385000\n",
      "lr 6.016408e-07 reg 1.228728e+04 train accuracy: 0.366265 val accuracy: 0.377000\n",
      "lr 6.016408e-07 reg 1.471586e+04 train accuracy: 0.361592 val accuracy: 0.366000\n",
      "lr 6.016408e-07 reg 2.397229e+04 train accuracy: 0.348612 val accuracy: 0.358000\n",
      "lr 6.016408e-07 reg 2.517498e+04 train accuracy: 0.348959 val accuracy: 0.354000\n",
      "lr 6.016408e-07 reg 3.568364e+04 train accuracy: 0.337959 val accuracy: 0.349000\n",
      "lr 6.016408e-07 reg 5.271420e+04 train accuracy: 0.324286 val accuracy: 0.339000\n",
      "lr 6.016408e-07 reg 6.548155e+04 train accuracy: 0.311735 val accuracy: 0.319000\n",
      "lr 6.016408e-07 reg 8.972204e+04 train accuracy: 0.316122 val accuracy: 0.324000\n",
      "lr 6.016408e-07 reg 9.685758e+04 train accuracy: 0.293490 val accuracy: 0.306000\n",
      "lr 7.808879e-07 reg 1.158495e+04 train accuracy: 0.365000 val accuracy: 0.379000\n",
      "lr 7.808879e-07 reg 1.228728e+04 train accuracy: 0.358122 val accuracy: 0.367000\n",
      "lr 7.808879e-07 reg 1.471586e+04 train accuracy: 0.360531 val accuracy: 0.373000\n",
      "lr 7.808879e-07 reg 2.397229e+04 train accuracy: 0.346898 val accuracy: 0.356000\n",
      "lr 7.808879e-07 reg 2.517498e+04 train accuracy: 0.334898 val accuracy: 0.348000\n",
      "lr 7.808879e-07 reg 3.568364e+04 train accuracy: 0.335163 val accuracy: 0.351000\n",
      "lr 7.808879e-07 reg 5.271420e+04 train accuracy: 0.331000 val accuracy: 0.331000\n",
      "lr 7.808879e-07 reg 6.548155e+04 train accuracy: 0.327633 val accuracy: 0.338000\n",
      "lr 7.808879e-07 reg 8.972204e+04 train accuracy: 0.301490 val accuracy: 0.310000\n",
      "lr 7.808879e-07 reg 9.685758e+04 train accuracy: 0.304612 val accuracy: 0.299000\n",
      "best validation accuracy achieved during cross-validation: 0.388000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "#learning_rates = [1e-7, 5e-7]\n",
    "#regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "rng = np.random.default_rng()\n",
    "learning_rates = 10**(-1 * rng.random(10,) - 6) #[1e-7, 5e-5]\n",
    "regularization_strengths = 10**(1 * rng.random(10,) + 4.0) #[2.5e4, 5e4]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for regularization_strength in regularization_strengths:\n",
    "        #print('lr:',learning_rate,'; reg:', regularization_strength)\n",
    "        softmax = Softmax()\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=learning_rate, reg=regularization_strength, num_iters=1500, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        y_train_acc = np.mean(y_train_pred==y_train)\n",
    "        y_val_acc = np.mean(y_val_pred==y_val)\n",
    "        results[(learning_rate, regularization_strength)] = [y_train_acc, y_val_acc]\n",
    "        if y_val_acc > best_val:\n",
    "            best_val = y_val_acc\n",
    "            best_softmax = softmax\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.379000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ True.\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$ When the new datapoint is delicately chosen so that every new segment of SVM loss is 0, the SVM loss is unchanged. However, the Softmax classifier loss always increase with new datapoints according to the loss function formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAFrCAYAAADVbFNIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOy9e7RtaVrW977zui577X3OqepGuqUxQjSKEtAgalQQHRIxxA7eYlTSGkyMIkFHFGF0QhsxGAOakE40omiComBLVCJxOAyaiJcYFSWKoyNI0w00TVWdy97rMu9f/ti79/tbh1P7VFWvU6fOquc3Ro1aZ++55ppzfpf17ff5nvf1lJIJIYQQQhwz2dO+ACGEEEKIJ40WPEIIIYQ4erTgEUIIIcTRowWPEEIIIY4eLXiEEEIIcfRowSOEEEKIo+eZXfC4+2e7+w8+7esQQgTu/gF3/8WP+PnPd/f3v8pz/Sl3/6rDXZ0Q4s08rp7ZBY8Q4tkhpfS3Uko/+Wlfh3j9eLnFrxBPCy14xNHg7sXTvgbx6lG7CfFs86yM4Tf8gufqr4Qvd/fvcfd77v4n3X32iON+j7t/n7tfXB377+J373L373T3r7k6x/e7+y/F78/c/U+4+4fd/Yfc/avcPX+97lFc4u6f4O7f6u4vuPtL7v5ed/8kd/+Oq3+/6O5/xt1v4T0fcPcvc/fvNrPNszLwjpzPeHi8PixBP6rd3P3T3f0fXY3hbzazHzPOxdPh1Y5Nd/9GM3uHmX2bu6/d/Xc/3Tt483LTuHL3f9vd/7G733f3v+Pun4rfvc3d/8JVm3+/u38Jfvced3+fu/9pdz83s3e9rjf1GnnDL3iu+HVm9rlm9klm9pPM7N2POOb7zOznm9mZmf1eM/vT7v7x+P1nmtn7zex5M/uDZvYn3N2vfvc/m9lgZp9sZp9uZr/EzL7o8LchXo6rBeb/ZmY/YGY/wczebmZ/zszczL7azN5mZj/FzD7BzN7z0Nt/rZn9MjO7lVIaXp8rFjfwSsarGdrNLueiv2hm32hmd8zsz5vZr3jiVyoey2sZmyml32BmHzSzz08pnaSU/uDrfuHC3L2ylxlX7v4zzOwbzOw/NrPnzOx/MrO/7O61u2dm9m1m9k/ssr1/kZl9qbt/Lk7/y83sfXY5fv/M63JDHysppTf0f2b2ATP7Lfj359nl4uazzewHb3jfPzazX371+l1m9r343cLMkpn9ODP7ODNrzWyO3/9aM/sbT/ve30z/mdnPMbMXzKx4zHHvNLPveqh//Kanff36b689HjteH243M/sFZvbDZub42d8xs6962vf0Zv/vYxybv/hpX/+b+b+bxpWZ/REz+30PHf9+M/ssuwwQfPCh3325mf3Jq9fvMbP/62nf36v971kJ/38Ir3/ALv+i2MPdv9DMfqdd/gViZnZil9Gcj/IjH32RUtpeBXdO7HLVW5rZhyPgY9lDnymePJ9gZj+QHorQuPtbzezr7DJ6t7LLtrn30HvVVm8sHjteH3Hc28zsh9LVbIr3iqfPxzI2xdPlpnH1iWb2H7j7b8fvqqv3jGb2Nne/j9/lZva38O9nbt59ViStT8Drd9jlivUad/9EM/t6M/tiM3supXTLzP6pXYZcH8eH7DLC83xK6dbVf6cppU85zKWLV8iHzOwdj9iD89V2GY371JTSqZn9evux7ZpMvJG4cbwCttuHzeztkJk/+l7x9HmtY1Pj8ulz07j6kJn9fnzv3UopLVJKf/bqd9//0O9WKaXPw3meufZ9VhY8v83df7y73zGzrzCzb37o90u7fPgvmJm5+280s5/2Sk6cUvqwmf01M/tadz919+xqM95nHe7yxSvg79vl4PwD7r682uj6b9rlX45rM7vv7m83s9/1NC9SvCIeN14fxd+1y310X3K1gfkLzOxnPcmLFK+Y1zo2P2JmP/H1vVTxEDeNq683s9/i7p/plyzd/Ze5+8ou2/z8ylgwd/fc3X+au3/GU7qPg/CsLHi+yS4XJf/y6r+9pEkppe8xs6+1y8b9iJn9dDP726/i/F9ol6G877HLkOz7zOzjb3yHOCgppdHMPt8uN45/0Mx+0Mx+jV1uQP8ZZvbAzP6KmX3r07pG8Yq5cbw+ipRSZ2ZfYJf77e7ZZdurrd8AfAxj86vN7N1XDqD/7PW7YvFRbhpXKaV/YGa/2czee/W77706jm3+aWb2/Wb2opn9cbs0BT2z+L6098bD3T9gZl+UUvrrT/tahBBCCPFs8qxEeIQQQgghXjNa8AghhBDi6HnDS1pCCCGEEB8rivAIIYQQ4ui5MfHgf/Jl33Ed/mm77fXPmz6iQkusmYYq3jvWcUzaxM+zfn39up8t4/i2uX49ZXHOAmkdJp+uX5dFnH9o4rWnOGay/XJYaRW/yxv8osc9zOM9+RCviyKuux3juuspcnF1w3j9er6IczKKluF5tUXc26KPskGp7K9ff8N/885XkkvoFfFV73rX9YWUZdzbbFZev3aLRqxWiP518fNs6uL4eTzTfsf1c339qsQxA9qzRKmk3tr4qCaOyUo8x5GNZpbGOG6LnGhlHuddlXEPZRb33OO9UxHXWuEj2hT3WSCNRYfPaspo8/wc94B2HrO4hq9473sP0p6/4yt/3vVJF3Vcfxdd0zBMLbVxL1VCW5bx86mJftD3aGPMEpsh7r1CubnC4/h+Hm8oMcVUdhLXOcR8cnkh8eyGho8oXu9yXOsQP58554L4vCyP68vQd/ocfRBjsKyjzcouXvdov6KP8/+h//I7DzY2f9dn/9zrD5mtov/OprgOH+bXr4cs7meG+xzxLMoUfbMpo4/wuZTx2K2u4nMd425I6NcT5tp1tMd4Gn3HzGzeY0wVuzgOc63lj/7uyD3em4/xefM85pEM73X0i/MW3yld/Pxi++D6de/x3mkX5/+q7/irB2nPz/3Vn3Z90lvzuJ5igdJwuIaqjJ+Pfcz92QzHwxtVNnHOPEMbYDw2FuexHt+tffSJtDyNa+tiXC/n+6pPqqLP59GU1mTx86nBfFHFdTi+Hzl/T5h/W4uTpk30o8bjO3cc0Te76I/jNuaRqYzz/+/f+D2PbEtFeIQQQghx9GjBI4QQQoij50ZJq+sRIkMsyxNCxVWsmTKDBNBQioqPaRG+yyABWY3QH8LdQ46QZhHnGbcRsitPIgSadhH2tHw/qpW3ca4JobaxQph2i7B2FfeTLMLJK4Qjd3iC+YgQbRbXV+GgBhKCr+PnPotwXFXGZx2Sktpifef6ZX4S8eQSMmDGsP4S7Vx8XLxGuLRe4t4mhNkhoXiNtp3ivVkfxxR5PLupgrzV7HfXweP9RRm/myfEXafob3keIdxpEWHRAv1igOxZNdGvxiI+q+ziObbnuDejVBT9a7M9fAH3JaQYO4t7P5ni9WZAeHwMOSnhvX0bYzCDhEsdK4eEd7qKexzHOL5GzL3E2HKE2dsprqeooWGYWcIYyfCrTR/tNE+ruO48ft52EQZvsyjlVEO6KXLI0BzvCf0mj2cxg7y1gzTUOaSCA/LcIsZ8hWudOIet4j7vNNGvG0wXOfpahnvg3DRWcUzOZ1RAioAUnLfo4xPGxwLzbr2/fSAr47qrKd4/zHAdjnHaR/9coL+N82jn2Yh5tIhnQQnsBF9pfRVt1ZW345hNnPPibF+KOwSredxvh++sDBJQTmUPbTAU8d61R18uds/F8R7PZ8JgcWzzcGwXMEh7w0l88Azz7AhNcaj259kRMlgJiTGha44jJOk23l+gHw0ltksUcc6xiXsb7fz69VkeP38AiX2XYi7mfD3fPf57UxEeIYQQQhw9WvAIIYQQ4ui5UdKaIDnkG+yqzi+uX6/HCGVXNcKDUJaGBWQJuF2GdRzk2MnPHeaLPMKVW+zwLxEemxCWH7BrfZrv3968j89LDAXyYmexM3xjEUI/gUTRIKy7gCp3MYPjY4rn0s0i3Od45HRa9Aj75q+oyPur53wb5z1bIgTdIAw+jxtqx7jP8TxCpNMiQq0LuBDaXZx/jfcuC1hBILns4EhYj9Fu2Rg/T/fjvVW5/1wKhFdzj+fXoE0qOAbKKqQoh8xmkN9ahGYLxmzx7HrIqSPcL3QkjDle95ASD8WdaI8KrsEW7kiDRDHAoVZh/LZt9HGnbD3EOcsZXHaQFU4pvcKJ2EBGLHhOPM59QctsM4/nNUe7FjXuE5LADk7BHC6dvIn7yeFem0HyzDzmqWyMz91N6B9ncZ4KfQJmsoOS0lvjdajN5l08tBpS+hb3vIQzbz6LeWfdRTv7BNmvjfPcyUNiHuHMmo/RQhe7GO8jpKQJ0k310N/OE8YwpnBLGP8ZuyocuLhNm9vi+jVderMMjlvMlx2k0hHOo3Ki+zTGwinmqUNB12ufxesC2xw6PN95jmeyhLutj4b1KSa1HpKf0Vk4xANdwGm1LeIZFhtsKcBnzamA9fsSfI55sD+By4vHYbzM2PZ5zBFzGDMzOqYhE3dwaE/4flhACp2muJ9TzCR99vh5VhEeIYQQQhw9WvAIIYQQ4ui5UdJK2PU9Inpdp4h/DSniVMnghICcNI0RBvM8QnwJie3GpsExCMEhFNtgV7nB/TFAMkoWu/HzFPKUmZkv47wVbr1pIimVe7z/zCCZzOLnvokQ76aOB5N3scOcoViYnayD/FZDHnDIgdwxf0g6uOhGhLvbLBwSW0iCCS66qUQIchv3nJBwajiN664RNt+MIWv0TRyzRfK4zuKZTjskSRsQsj3ZX5/PszhuUUbf2yGR4LBn2otwfzL0W/TtZi8MDhkXks2uiradwVEIM4PNINHttofXQUqH4wzJMov42L0EexNC9xu0QYYw+4hEoBUSzPWQqGjNGPH3UoJzs1zCRTI9Wkoa4XwyMytbSKxwdoxwLOUFEwNGnxqgM5V1hNazBV1KSDoJV08F+RzKkE24/xxOrvEJSCBmZjanTA4nK+ZLryE9r+G628XcucNjLBZxTF3HfdbYGtAi8d5qomMJ7QP5dxowP2RIBtfvu50mOIk253DkFNzrwASzcN3OML+wj0Hq7CGJ1Jhr6f7bLuL42UsYy5ChrTh8e+6QwLGAvj7tJeqEowrzWvRes4vy1vXrBcZ4t6bLMI7P67jHBuMrRzbVsog5cOLYHzDGq/1nkgbM31norSX61Ax9p4Ks3mDe5Dh1tPECbYypyWr0G4MrcwkJM8f8YNXjHXeK8AghhBDi6NGCRwghhBBHz2NcWqgBhRo6PUKC4wQHwzpkks0KwTk4RLoZEgnG4bbLIsTVOmQfhMeqKY7p6witnvRvifPDdWEFA4Rmjl3iCY6feYHd8AjZF5DcqjrkKsOud0e4L+EeRtQWmhtCbQiPFzWSvmGnfsciSAfEd3Gt3Uncc4Zd8hlcIRPdS0yeh5o2PZLYVR2kJNxyg6Ry4yyu4QFCs5ZF+LbPIIGeIrHUcj+x1ITkWhM+MF+hxhYcaFNJF1204QnkGDqzeriuJiTKqtE+CfLAbooOvSfXMcHagRhYVw5SUcopM8AhBYdaj762xFhrt5CPkZwOZkLLkZhyu0If7yEXwmZTNHGdzcAkZPuS1oAkpxXaP8tYDynabGyjz/oGCd3OIL9AiumgbSZI7AMcaHuJLeFiHPGM+KwPya1bMR4T5IsK19SjfSq4cIaaEiWkxZH9PdqfrrYeksP9PqT9AvrsDu2ZjRhn9yFVzfYdMm2HLQpZ3NsAl1ABx5ctscUAfXWOtk2QfuYYvw634Bb9c4bvrw6JRnM4Ltvx8I7YszykxOx59v+4/tsz1L+bRfslzGmUWxPcpjmueYRjuIeMhZKSNnVIuIvbrRcYH3Ah32r2k0hu8X1fwunMhL0FxkuCHFoMqKtHhx/GYIvv9RW+c7ankJXP4RRdIKkvEgJnxeO/NxXhEUIIIcTRowWPEEIIIY6eGyWtvZIaQ4S11gg7zehtWEYoMo0R3i+KCNOdbiPcd17HB9R5hEDnSBKVJezSX0ZIrMau8gLhyjShvku1v57zM9R6Qon5ASHOAnKVQR7r4CIbNiEV5EjulhCKZy2iDeTAGgmzikQ3EtxBiycjaQ3MALaN65iQiK5jSBzunPsIkZYInWaodVMiFFp2COXCsTfgmAJSYovw7TbD89og5I4wtplZQp2l2TzaenEOdwa6wOZutH+JpGkja0vBFVMgKVuD+jsFklamPpyAPeSwHOHVWdoPER+CHcLadYk+iHpV9Rlqu52jZhQdGXDfVRUtH3C9wbXBEl7ZDs8QtZQQcd5rV46VkyLmATOzEdnz+m3MIxNq492nywMJzarbkPQguSxKJtHEc0GC0GGEPH8ax8/hGnPW8KufjEurHCDRwFE2QFYtZ2hnXJPtJdrE9c3Rr8/j2TUYNyOe1wYSc4n5uLuI86QSMosh6Wa2Lw2dn8cceYKdBTncqHvzEWTyU2xFyNAmJVyZHWu0oQZaiWeX0M9buMZgILV2OPxc2yX0R8yDGWVIJNHs4JQr0GdrfG/m+H4wfK9t4NBLkPVnqCPWog6iw3nskON71BO8qPddpTWSJ3b4vB4u3qKDlFzF2GSu2A7fG0u6NLEOsDbesEDCzwssRkqPLSwGSXrWqpaWEEIIIYQWPEIIIYQ4fm52aSFxWYYETXkb4bUlYtwjQnCN0WkFJwvcJWmDLGlIHjXDVVHeyBHWSqh/1RWRhM1Qw8jLh9ZzPaSyis4chOYQmp2wHuwuEILbS1YV51mh7s84wEXCBHZQNxLqAblRSjq8BGJmtkJippQiNJlZyAtLyDVrJOGr6MxDOJkJrupN3PMFHlGCTDZBiugQKt1lEcbepfvxGrv8zx/su50Yaq2QEC6HtFjBrnCK9lnB3dBCZjS8d1zHZy8WkEr7kLrys+i3C8qekCuZJPNQ5JDJGjjI0pxhajg4IPUWp3ATwmW5bSkfwI0D11CPemnOJI2ogbNCLaGsiddbPB97bl/SypHEDqqidShWd3sd99AM4Zr0ChIjpMR+C7kCTp4cIfc6IUEk3Eg5ZJIVpL6mPbyrx8wsVXgeGJuUW/M87rNFUtWsjWe3m0PWaEMabOG6uoDLcEgxZgvIod0mju/QRwxJGyfWGBv23WtMJHufdc8gX8yZWA/JZnvIWx2cRFVGtyuTamI8Qt6d4KjrLyBLwjVbl/t1ow4Bmsl6zH3DOq5zjXaaM+GnMykixjhr/OGccxTBGvF1N8EN3DxAnTrsQGnhmC7vwZXXY5ya2ew5puREX9hClkO9zNTCfl3EWHM4JbdwphWQmydIsv0DunjjmmokJvXx0W7jl0MRHiGEEEIcPVrwCCGEEOLouVHSKiATNCimVdUIlUK6KeBeys6YzA+70LFr/QRbuJsSrikkquKu/gQZYgOZaIbzOG4p6/Z34E9MCNWxVn28HHPs8k9xPyUSCWaQxiaLUNuaiZV2cT81an/MENYb4LRxyj7Vk3FptUjEdXaKRGdVvJ6QAG5RxfXt4JYzyF4lQqct7rPB67GJcPV9JD0bcM4t6iSN25C3tmhzJo40M3PUcll69M+TAokUPT77RSRf6xu0D2rD5QjzZnAuPEC49OPn0Y92qC1UIvFkCdlzOz98srp8gvNiCknXe9w7miynDAvNuGjj+UyLCH0PkInsAaQLyBMF5ANKRgPauFzABYZxmrr9RHW5wREJJ0l/AXcNpLsCtXh69LUlwt3ncKoUcDVtiuhINeozLVFUbYakoJxG+iejaFkGKX1YRJvk0IZzh4wzYPzCpVpBfhpoGoUzbQHnV0LdubsXMd+NqP9GU19xD+4tSJcP50p1JH00yN4TnFkTkhV2TIoK9+a8hXsP1z2izYtZJL1sIPV6DumD0iBULG8gIR2IRRntd4EtGS0ktuUJ3YR4wDndSJgTIQ2VtD7BbdshQWbhIReu3hoS0w7yVoUtDvlebbKol2VmlkFaW2HbygXGoyMhYQE33QRp2NF+OaRxlI6zBolGYXS2oot+cA55do7kwOUkl5YQQgghhBY8QgghhDh+bpS0ZgvsAJ/g6kFcd40d1rM55YcIVzZL1DxCzY1iQNI6hAEHhLRrxOUnyE1exqXv1nFts9twWqT99VyOJGhrRAUTjstnES7L7yPJFuJr4xLhO+xU9104JxLcHy0S5lWUBCYkdMLnrtKTkbTmkHdSHa9vMbHaHOFhj1Cow7E04XltIPvcbeDygavrPu5th6SSFxskxkPCsG4W19ZcRD/aDftJ307gLmuQlO2li7im51DH6nQW/fAFxLVv0d4A10rZQDJDyLaFjYj13SokjExDXPfKD+8EGSok3kRfmyCNMMTteD4tEm1OZ5CZULfIaXBawKUBySCHg25udDKhVhXCzyX6TbPbr6W1RlLBCfXzcjzrbAs5dBV9pICrZ5ZjvoAbq0eCxXwNByWSxI1I0LaG3LzAHDT2j3eCvCYgk/oOyVbnUd/qokei1hLSUhN9IbuN+biO+9/AEZvB7cQtCTNIaesGkhTGkD//aHl2Vi/3/j22cJExER/qfhVwREIRtQnydo+EexskTCwxR3DrQo/xe4ptAhu4ujL0r116vAzyqllhSwbcVWd79a0gAWN+nHD9I9q7OoUcBtdnt8KYzeFi3UV7JzigU3kvjq+iD+U9HJAZ3dP79bMc20Iqizkuwxx3gfEy7yCb8TsUbqwcQ6qCszLDFpEttk5UJa4PiSPbV2CGVYRHCCGEEEePFjxCCCGEOHpulLTOu0juxQB0gWRuzmRHCC1uEROv1o92RfgSbh8kf0sF7QXhQBkTasDgGrIiwmnNOsJg64ckkPoMibt2IdfslYFZo8bUEGHHosJn7+hyiONXqCcytEi2NURYukEItUzxfEvcZ3vr8M4BM7MdnEancDCUcABMFRKUUfpAkrkOSex6hE4zi9D6po1jHmyjk7Q1EgwipL2FZLZuIzTbbuDG6PdjlhU+7+JufF6NkO/9eYRtGzhebhdx3ffhnpgjsVyOul/tGetVxbN4DnLggH40QE7J8/1+eAgcjrsJNc9GOHmqCc8UbpEBj3HCc88WSJzYQALA6M/xzAe44ZhozlELaYGaTD96wWR5+/eTIGNMkD1tFtedlXHero/xm7Lb16/vsdYX5ogW0uiI8PsC806ZIvTfYVLYIQnfun/IKnggMkh/BkdVhyR5czz7voMTzpnANOSUDknicjh7zmGQGzF39pCCR9RTGpFUsIIzi7UG9xLS2X7dtCxDX51j7qTEjlONkC961AOr0M4TZLZhFn3sZAx3EgxM5tiSMcLhe5Lf+BX4msiR8HaOGnMTXJw95GPD9UyYWzKMr6mDyxDyLL+Ls45ScvSJkSosnnP2AIOlxnPe7lsRczihhhSdp4CcNOH7/gR5B8clZH58r+cT+zguEPpWC0meCYvndLXhedUPJUx8FIrwCCGEEOLo0YJHCCGEEEfPjfG80iO0nBuSCqL+CKWbFiGuGg6s4g6SR6EeFmtV9aynAZdHt8AudLgLzjvKXvGyQrn4Yty/vTG/e/16NkNtqB6JFFFPZsWiKNglPytRP4o1Xfp4RjVkjGFADa8JO+m5+x0h4Wl8MpJWPUU71HCXlXDYjAg/n6Ie1ItI6NbfR6I+JBKrWEutjbatlqiBhppJPVw3qYxnNN6PZzHH8b5AzTQzmyFpWof+ZlWEypkQa4RMs8X9vB31fXLoUiucZ9nBsYVEgo6wcwUHTwfnlPl+kr1D4DMkGBzhNIJLaYDsVRvbAKHiIfrjBEkjQ/07uiUS5I0crsQ1NIm0g4xocJBdxM+3+yYtm59grCHZ5PkOSUshH5Y1peG4jhZJS0u4cTrIQTXruaFtNjta0+LlDMn8pmo/3H8oRtiUKsypxSnkHUTsR9Q57CEfJUhj3ALQom+uMd7pcLqAC2qL81R41iO+Eyo8x1TtN+gFEktWSFBZLcIFWo/xnnYe97nKmDwwXEEl8uFV41uuXw/QaHcYj0zcV+6i/bcZ3EKo1XUoakhaCZ/Vwg3qc0hySLo4wMnUG+ZWXGZB+Q9yJnL82dji+xRbR1YbbE2ADDWN2OKRQzIysw59JId7cYItalhR3kLtrh1kS1zHFpI0a0f6abR3t45xXazimtif0oDvhPl+rcVHoQiPEEIIIY4eLXiEEEIIcfTcKGkxlF00EV5qWFwGNX0qxFzXCJufweHjTDCGXdgJclVDNxHCgPc7hAoRsstuRZjUkVCuKPcTvpW7CME2ObaSw4XiO9TfWaA2yV54Ma6jQzi96BCaRHKzCU6QDGvMZuQO9gjZndqTCZvXNRwZSOi26yEzIcy5yCIceRuOjAxyYG/himiQWG5CQqzkEaZc9PG5JWKzTP+1RPLIBFfIrXxlpIP8WEHinGXRD+s8JI7bzyMcv0ENpSracHYCSQ8h4rMp3DnFEK97SAU5QugZ+qqPD+k3B2C2i/sa+VjwUQ3GIK9gRF285Tr67L1s76DrlxX6foaEn+M2znMBJ+a0exEXAbkY8la925cSLjDXjOhrW7ggy1m4Hesy+h3rM3U1HEsN5i848RISp85Qg22CI6ikSwnjoF4/3gnyWhh2cJTdjvE4h2S+K+CQYd06zK+bCVIBihT1DfomJKMCUtIIiWrWIjEeOg9r3s2RmLab9t1rDjsQ6xM2LRy10GCKDZ73W0KmKCGxF120cwvXYYfzLJAIdYt+O6D22gAnbrE7/N/8AxKwVgskCaQ1Ec+kq+L7q0LCXsO9exfvbdDfl5CeHZ+bUJvS74Yb2CB/5nB7dTEFWvL9xIMVrpUOsTkSFGaQuubYIjCh3iH7xAzffQ1+PtFlCulq2KK9cc9nt/DdvVUtLSGEEEIILXiEEEIIcfzcKGnRadTs1cCKEFlCPY0JLq0Sof4NEw7NUJcFNT7ckKAJ9YnanjVBkExpQBh7E9fDxIbraj80t8zvx++woXueIkw9R72lAaHSDJJenhCKRRhx20OigaxSlEieCFdMh/Dw2W3WHInnckh2kCkWHcPaaM8U1zpBNjxZhIQw6yMEm6MmV8e6UnDanSIxXovkj89t43VfoH4WkgJmPV0h+3V8eiRPzJH4bkL73JnH6yUcTEURodPnYf9YWMgmJ6j7c5rFM9pAEhpQ72ZzEf0rQXKohsMnN7BfyswAACAASURBVGsWcPKgv+SQmGcYv7sdwvsOt9Qy7mWBJGz9iBpmkFU6jC/W/Rkgae3WTASK+SGHo6Tbd1TkDVydSFyYziDFIJnjA7g8dnA1nuK9lJKhaFkHFxhdhjMkF81R62iF5HGb8cnU0uqQQK1CwssJMvTUMLErknnCvdadw005izbB1Gw9Ewbi2U14LhkSvQ3zuJ4lPmuFemabft/Z0+B+OmwByPEsDfeT6migiwljdoALmN9HkJWHJtp8QLJYg0utgPtpopu0PHwiSdatm6p4vo77zat4PYM030J8hpprDWpyzQfUwGrivu4O8R01/EiMj8Yxv2M7Qo4En46aZal4qCgVtpvcgvN5g+0fOeQkx9aBHN85A7TRfsX5EfIs6kjmqMGZ5ahHieSfCYli+1IuLSGEEEIILXiEEEIIcfzc7NKCNDAbISdhl3S+jfDXuESiK4SaEpK29ahnNI0R+jrZk8zgcJojKV6DmkEIX+XYVb5BYsPFMqQXM7MRIdvVhLo+OGbXhqSRX6AkPZwwHUL8bYvd8HCXLe4gOdIaz85wz8gviDx4lhdPJmy+gmsjITTfDAxzwuU2hLuqruJ1hcR7MyQl23g87xJJpu7iuTfI6JafQbqDlJYhXN+gppFn+901g2TBrGx5Hn1jCafOAhJYBXnlFkLftgj30+o8zoOovFUYF1tHPS/UN7qAa2W3OXziwQxJ8mAismYW1zBArnAky8whXZ0iueY55Lmpimd7H0WJ5utI3jmgxlQHF8mQ7l2/3kHa9m0c02/R4c0sLSIEP0DGKNp4vnfmMQgd93CRoabXyDGO5KJoM8PrDLL1sI7PosOzwXMsyugfh6RDV97iuS4Q4p+QfQ5mFkuUc1EXr4d8PsB11qyx9WAOeWCIe2sKykFIZjmPa3iANsj3m9OKt6Cm1xaJZyFrZJgLKkhoWR/tvIXbyO5HOyckTm0WMZeXrJ+1gLzJuopt9E/WoTsULZ7XEjW8GmyxGLGlwFHHitJNi3kpR7LIc2zB2N6Lc/ZtzDM1xtAI5/L5AybNjbGcYNF8OBljmiFxLq61x1gbMAeNm7i3GRrEcZ7lGjLWDLI1i6rB9Tuv4t7SLUhd/D7dPH7rgCI8QgghhDh6tOARQgghxNFzcwwI5ewHR3jfEe5ESJjlrZ5fRAjuAULlS+wS3010QSHBFOSQAmGzHKHrehlh0nSBddsiwn0Vt7mb2Txnor8Ip/aQPUo4ajKG0RLkoHWE3/OONbCQAOwenBNThA6rIu4twZnWbuAOq0NWOyQXeYR+F9jd34e5yOpZhBpPTuL5LRlmR12TMWOSsLi3twxwoBW3r19vIK10kMBWkLcGuFQYlh67famvyqJP9n30q1Sg/s4cIfGLOH41UTaJY7oHcMIskQAPxZ8yJNzaweQxpxsNQ2tXowjQgdjBdTTHYymQDI5Pa8RYnuCcWFMDQF/u4Cycw4nZwlGx20G6Og8niCMxaddEn5iQpPNuv++g9LuQa3BDJ+hTdy9i3BWosVatoo+0cE0OcLKdYKwNOZOcQs6lqxNz2Q4SadY+pN0cCCbtLCEVJdQwLBKuG1Lh2MYzajBfTs2jHW4Z6tbVRYzBoaLsidp2qP+XKKtBSitv7ffxto3rSAskiYWUMeAhN9D3e7g0T7aQ0+B4anIkHcW4a/DdscDXwgAXUokEpnVx+PasICvvMApzPLsCNdnaNZ8VxyMcqqx3iOSXL2DuKtH3N3BrjminYsK8j2frI/r4dj9hb4FnNMfrDPNs2mBLwZy1xOIe5g1d30jyWUKiQrLUilIqEkfOcM4moVbiK2hLRXiEEEIIcfRowSOEEEKIo+dml1YfIei0Qz2sGd6GkPBEqWsRx5w8iBBfh136RRlhqnsWusopdphPKcJme7Iakvx1SC43wCmT3UNNHzNryljfneIWVriHYYVrRb2fhJpAL6EWT9nHPS+Q3WxgeC1FOL0vEEKHS2uHcOQ8PZmwuSOUe7GLtr2NJHzDGNLSBNlra3GftcezX53Ez1dd3P89hFTnCGk7dvAPs3jvCappjWW0AcOalDrNzFB+x05P6MKI8GqPJJkV3Hwjavrkuzg+we3V4QNmDNOO0SezHmF5uE6KPu7z+fkTCJujHtIC17ZBODpHwrg5nBr3p/h5ibB0AWfOCZxMDdxxA5yVW+T58i6OOWeNODhNWrhv8hrFe8wsQZaZobZOgk7eNXCgncSzzuCC7M+iH82RYG6G2m6O8ZXh/CeoXZQgK2Wob9Q/PrfZayLDfJbPIEameGbnOVw7kLTaCXLHGP23LSFlQG4fVtH+O7RtCbvfJkOi1Sok6SlRYoKD8qHtAxkkVL8Xn13fYQbIeLlEYsA1tkCMhvGI4d9u4vNqXFPdxHM5x9dbPnBrBC7h4vBjM4MrtYL8MlXx3Ct8hyTMXUzmV2HLhzVw3OGYGvJsBpcwpl8rB9TPgrTHxKEzxD5K3088WA3xGY5+WkMCTdhSMF/Gee/jvJyva2xVyErUr5zwXbSAGxj3mZC8ctnF8ecX+zL5o1CERwghhBBHjxY8QgghhDh6bpS0GiRM67J4XbZw0SB8xzCwo1hVhaSCULSsg8PruSxcVyOSzRUnEaZawRIzILQ2ooYIw7uT7e829w3DruFCSXB2DXgkZcfkfNip3oSLqkc9rBHSQpHiPHM8o3mi8w3hzlmc3+3xobnXwq23RFhwPURIcfcgwpEndXz2dg25asV6NXHOHRJfFadwdSGE3D8IWWkF2Y/Jyvwj0b+aMyT2Qyg3K/ZrjJXZy0gfrOl2L86bwdW1g6S3eSmubwYZYIMEbQ06LoW17YsPcAwce4u41voWOv2BaOBqrKHPzpBsj6HpAa9Xp0jsiLHc0cmFxIPtOepqIdrdIyT+APVtMkgS8zrG9WwBWQW1vczMihXGEWom+a14jiOkV3QLmz0HGSuHjId6PYY6bwXdnnPMNXBQ7gbKsHAA5vtS3KEYkEmw7OJZ/Cjk4+cgh68tpMiuQr+Gi67dUvaMdrgzj/Ns0YYb1L+zOVxwVby3rPk3MpLOPqQM1ehv9Z3oDy1k7zmcVhlcXY57biGzOT5jiS0DCXNKt4HbFy6wHvPDMMaY9enwf/OnKq6nRu0x7Gwwt7jfEhJrDVnV8Hw2cGkVqL21OEH/TfGsdrvYIlKV2FKByTsfKO3iY2/vLwsG1NVr4bQ6gURnSECbJtTegvQ+nkJy43yNuSbhPGybNguHpvfRH7MeyQ/7x7ubFeERQgghxNGjBY8QQgghjh4teIQQQghx9Ny4h6eDzY0JWQfsN5kvQnM7hfbcGjMKw4qOTIrbGrbBDPa6MXTJEdpujuKDCUUuHQXH5vj5dtrPzJuQFXeaRfZj36u+CC3Z4udI8mnbW7BBtriHKe45n6Hwqoets8MjRz0/69rQ4c+7fYvnoci7Of4Rz2aHey4maqIojNr/6PXrGlbx01vYL4OssGkdx3Qt+gWs5RX2W6Us2mPzANZH7AV6UO3v4Vnl2KOBvVQlskKvLbTfqXkhrmOHrLKwNd9H5tK2xj6OF2Nvz0UWr1lgd/DY3zGDbj6UsOIeiIT9LAYL/HQGWyf2pFQpOnALwX4BO3mL/Rl3X4z2OL/AmN2g6C6y8eZzpI9AuoEBWZ0L+IGXyORsZlZg71iJPSYN2tzR75B5wBawNBce+n7Vx3NfIsP3iPYo0E4lLNrZFPsW/BZSVbx4+LY0M8uwP6er457nSKXx0ib2ZeQonrnDXoeVowjvSYyjCq+ZXblBVuM55jXazOewHHuBfTGwXDM9h5lZjv0daRNZmNMKe+Tux/jvsb+ugB07YU9Wi71hZ9iTZLQyd5GKpN1ivyDuZ4f5aK8K64HYobjnAiksEjYtbpl6ASkjCs5pyIrs6I8T+scS6UKGGfbXoCD0gPZbYD/OgD11qUdVAccXpZk5+lSJ+dhhV189hwzc2NozoDZrOUabFbfivQ8Sxj5XJEirMaHwdTEg7Qjm3+IVNKUiPEIIIYQ4erTgEUIIIcTRc6OklWADTzWyLSITshWPLmiXEJaukMm4maLgYAnLniNci3qDNl5ECG2NAo5jFVLF+ZohwXjdbBFPM7MSRQDzIcJ0F0P8/IShf4RT61Xc52mH8DPkuglyUI6QYr5CZl5Ibn0Dm20VYbqpezLpXOcoIHiOkGqOUP6LkAH787i3c4QyV3cQQkYW7QWywjZbnAcF3oox5D2vP3L9ertDUTsUsdwxw6ZH2NXM7EEb1zFHAdgLWMtL9NWLu2E/L3FNPSznW1jLX/yRkHVGZB1vC/jyUYgR7l27A3ng4+p9+eYQ5BiCE7KLMx1Cxiy6yF5tLaRBFOpME0LZJTKyWvx8t4WVtYw+PkOh1eEcmWaRHbiyOL5Y7Mu2K1jid85pCbLMIsZRVUOSRBoHyiE1GmSgxFxDSoUTO0db5ugHI+aR7MYZ87UzXKBvQ/pLyMg7wbI8YrvBKeZmbiWoMA5OIGMlzK+0DTcxPGzx8dFHsjls35AoV/BZ+wnkcjO7h5Qec6RNoP18nqPgZPXc9ethBzkRD3w+xufdnsf9XEASufdCtFUNibZAQdsJRVindPjtA3tu7YrFQyHPvRj3VRWQzufIal7Coo6UAQ0t/EjV0eF7w7MYa6enkCHHmKO9je/i6pTfafvaEHeGZLCQO5p8RF+YcN2oEWoJkpvhuc/Qd4oenbPG9ooXYw4q0A/8Nr6vm8e3pSI8QgghhDh6tOARQgghxNFzY4DWkWm5KiM83MFF1SNUWCFL4jRFaHFEcc8cO8/vn4eMlWURohwQ7iqnCC13CL/u1gjFoqLftKGT58LIDrc7wVF2iiys3Tw++wy6wcVFHH/7VpzztHz++vVsRskMO9IRoh7WIY1UdYTrdyxU6k8mbr48jWe83EYbbpCptdvF8z5HZuoBct3zFxH7dofkeB+OKDgkRriaCsgj2x+JMHaBjK9bLMOZ8bd6Yd91t0E/nJicGk6Se5voh5RTM8ivc0irDVwed/EsBjjKBlxrBgnlBCH0rAjp7k4dLpVDkaEo55ijrz1AQcBb8borI5S9xHVuEAauMSZujzEmNnW068kS7+14v8gUjsy3cxSYXEJKGnf7bTnCLeULFDvEmC3QrlbDeXInJMMZKvJWyORe4FrLGlmN91IE010WP82Q5XZ6OKXwgdjBaWRrSK8oSDs/i3vrFyiMmqEdElw4WUiAM2TkZeFKw9YDVlROmB/qJSQpFCetEuXm/TlrxczR0A0zOHWm+mXkG7iKJhTndfSfHbYGTG301ZM7yEC9xrOAzZYuTo7rQzFjBmoWD+3ieuYncDVBzdyhD2bIJG90H0KeLuAwrYp4DqsVHLMosNnOUIAX7uTdC3At39ov0nxS47sJeuiCWbCRmbxBFuUM152jTxSQzRaQ6NoF5FxsYWnwvGZwJeZ4LwtNvxyK8AghhBDi6NGCRwghhBBHz43ayQgHUkIIvZ5FiJ5yjaHAqBco+ogQVELIag65YaA7qKN8EMfPmCwP12YIS45DJJdL4/6u7Vke8lMJmS2DG21q45r2NozPGdYOTWuJ8PiIUGbnETrMsa50hJZHFKesDcUsIZMcEkdBQMqA50VIi5t1PLPzDgn2EIJ8CXLiOouCbdVF3PMMofJ5HlJaMYY8Ui/intfY5Z+xPdEXumE/uVnh8e9tEaHsAn2mR5HZBlKUoTDqgxJy2nn0hQb33yJpWFHDbQHn0e1FhG/nz6MI3nP7hTIPwRZh6rRFIq7n4to4XgqEh+sV5CPIPi8gsVuDsPnJDMnT7iCp2ksoSojCgBdTHDM+QFFYuLpOin1XTw9ZsUQI/TakK0f1yLO3ooAgpJF5BXmnRz+AUW5sILGjUHGOvuI7OCVRDLFLT6awL5Pq1XDIZcji1sP5OFtijkQuxBnm5gZjthqRPBHSEG7fSo4bZJptkPTuNMXc53CuFrY/11IqylI8fPeYa0YMR4cs5/iMsgqpZfsASV4XkGaQ9DBvo79QEurgMh0hS+76w0tazgKgFZzFcJX6GnIrJObaIEvx+lcotomCv3TYzlC0NL2EQtFzJC3EPJBxDkHGv6HdXxb0SNq5gO3qAm2+wPzdJbrLsC2mxjwCCXyWIH/CsdaiEPIpxv5IqRrHFKNcWkIIIYQQWvAIIYQQ4vi52Q4EWapgrkHUGGL9qBEyxgqhuXsMiTVwApyiThIcMRmcWQXCXW5MBBjhsTMk3nppAdlqeCiB3wjnEEKK0x2Ek5HUaIIs1SI8fBt1R1qsGSckLZzD1VSdIMw+i/DuNr10/XrzAKF11AY7JMUpQsVIOFeMIUsNkJwyONYSkiTehXR5Cz9fI3HbvEHds1m4o3aosTXHe1uE7ifcfw9HCev7mJllkLvKWdT6mqH+UJOFjNetoz9sWsgXqJ/VoU7cfcisA5I2zpAYbXECqRMJBossJJflItxoBwNh/AbDeIl+PSDRWYmkX45xMaGmGP/6YS2woqXjJtxeuxP0FSTynFj/bjjDazzPej8p6Alq69SoFTSbQ/qCjFOhtk6O9l5gDhoRBp83kEzgHKsh7W7glHKHu2YT80DfIKvcARkt+lQ/4f4X0bYd5lGWXHK4mijzLpHQLYOEuMRz6ZuQletTPFO4sQa07YBnl0HSyLt92fa0iuu4B2m4QDvkkNB2O4xlR6K8FjUWkYSyXaO/QMZM2GIxQQLbdCFVD9DxqmzfkXQIWFcs26CmIOa1xYwZ+eIlU/6hVJ2lMf6xgsTa53BZYg5tkGizeBBts0HbtyPm6BO4mx9yInZIYjigzU+RLHSzYAJTfB62s7SQFW+jXmSOmlkOOWyaom8mjLsBMnczxjH2UD23R6EIjxBCCCGOHi14hBBCCHH03ChpZU2EIncnEdZ8S4l6JZAMctQBofyQreM8yP9ljlD0DGHcrkVNmzxCkVken3XaYaf6Mt77cUh4lyzC72ZmbYrwNcO6aUDoF5HZrKfLAQnXVnGfZ3DpOEKlsxzSHeSBETVmOiRf6+Go6RDKOySnaMNiFe6HdIYEg3dRTwlJIneoE1XBtfFginPutuGQMzy7lzzaYY6g7QbFWLITPqM4TdsieSQTppnZCZxZE3boNzimhwr2oAtpbdjEfY4Dwq4F5NQZXChIcjggyRpUUnM49vIFEp1l+46kQ9DDaTSfonHKKsZIdYE6VJAbDRLe0MW93IZtZgt51uD+2CAjX4XZY4JTrF5En9igtlePennIWXd5LiYrXCFJIpwq1TLuE9F7W+TxfEfUOatqymSYeJBQdWpiTljAQWrncd1thfG4Zu86HH2LxIj4O7RDQr4ac0o5IPS/Qq06tGcHSXbq8MBRb5B1u0a4wIpl9JcSI6pGIrkZ3FRThkJcZjYM0Q/PBsjeNPXiMqYGtdggLVaYpwfMwWmMNhyRDK9EbSm6g0tMBANtbd3h2xO3YkUW81eBJK0j68qxXhgkw4kyH+X1BbdgoJ4d5MIG39HDMp7VDLKS4Twt5MVyeGjrALS1fIEkhKgXWFSUQyFRwX2XQ1avqzhPA4fYKdxbA/rNgO0VhWGrjUf7bYbHOygV4RFCCCHE0aMFjxBCCCGOnhslrZ4hYchbNkbMLp8hpAYZq+3g2MFO8ryELIHQX8ojTDXiF46aVNmIRIg9awmhnlUdYS3P9p0DJ32EvhN2epdISJcjZJ/NkGwRstkc8fgebqwatUWYuwm5Bq1BAsccMsm8QyKt5smEzR1Oo3wZ7pmmi5+vkWCvzJA0C3VQ1tiFn0F+G9A+rUGu26A+l0Xo8wSS5nwd/eUczzRDYrCi2nfI9ClkKTqAph2SR6aQ2do+njGdYBtjn4ln0W7j53eQZG2LPlzWCPHPon7WokByxurwTpAezpwl6oJZG9JCOn3L9evR41kl1CcqIR+MTLSJ5JcDQs4Fxt0cjq0Bdee2kLnPThBmhlttgtPRbF9+mqEtqxIJHBOccjNIMSikVm/hQllHG29vxesCGUWzHOF3SDrnkOhGuGIo8x4SzgUZkq/NMEU7a1fhklYD3Dk5niPGL3ViznE95K0F3E6+DbkqOwnpuObYZCJQ359rK5y3RxLLGs/yYgi5h74glEy0XYutB6iHlUHuyHo4PCmZ4ZpWp9H3Uh9z1pNII1l1qDUItzLUQFuW0b+aPsZLNsY43eBLJMd36KLDlhLMoVsk463OMC9hftguos1KtCU+1rJb+/NsBsfivI7POC0ppSPxICyEO9TFPKH7ElLtALk5oQ5XCakvx7aDiyHab2ISxtnj3c2K8AghhBDi6NGCRwghhBBHz821tDZwV50hwSBqTw1wDtxtEKbCWmqOHdYdbFqnkBXuDnGeClkOxwG791Gvo4CrK4dU4UhONqb90NzqNMKpA8LgAyS0hGSD8wKJBCnvIfw8dggRIs/hBXbVkxoupXwXrhXWdPFp3410KM5OQ/o7nUVY8Oz2j7t+/aMvhHurreOGBtQvaZCYajbRXYYQOm6hzxCKH3HO8+gLa4R7DW6DNVw3s/GhmlSwQ0ysV5Yh1I7ElS1cSOUM9dMGuBBQQ2g+D8mmg8xAx0TCe+s52vY0bujO2eElrWFA8kuEuKdNjIs7RbTxYopjHkBu7HqGteHUQEg838UxZ6dxLwVcMw00lm6iayg+N6FeVtZFzSczswmJ6op5fMZJDccIHJso12WdMTld4EjuNiHJ5QBJ7zZcMbsd2gntna3jc501/A7IBeTZHq5Rh6OI8l6NeXSALLfAdTucrBlk2BY1lCbIKYnJ4E7ieio4vLIKCSxP4DR6sO/sSZB0G3zeHHPt2rldARJPimuaQWK/hbHcG+XUmFO6DZ1QwQzfEQXm9c3wBOZaXPMGiSBXaFeb4jmc3o5ntTuP8VvXcByuKKsyYSu+7yBvlUj8ukI7lZC0mjpkS4MknZ3sS0PodnYyC5k8O8Wci+/vU8iZMPuZY67pIIHNspinMvS1Et/l0xbttI2+2fYYm+Pjk4IqwiOEEEKIo0cLHiGEEEIcPTdKWi3kh2qMZGJrlHwviwhB5dh5Pc0Q0kQywBOWl2cOJCThywq4gxB+7pGorkCyrTsnEQJtIEnUvh+upHRRIuEY8kFZ8nAk2IzyU4T8GrhFciRWGpG0roRzIqEGzgD3h1PeQhSxt8fXBHktVAskh4Ljoa/hWkByqAnPvpvifm4tIG/1EYLte7hi1vG80BUMSqJNU7gZtjtIhgivdnA49f2+e22CRFJBmujw2QPkx6qKn2/wc5Q6s9zxD8iVRgcihs1qHuPC0N/mSIaX1w9l2TsAI+qIdWinAn3nPvrgZhHPOkc4eVGivhwdLqgfNUG2nljbbAPRAHJISWclHDp30F73bb/OXQX3TgnpoiogXUAyHXO4xSCxeo36QD2kVNQWyiG5Qa2yvok5aMR456UO2b677FDMYC9yNESWoX4YpNQSP98gN+eIJKcGJ9sSzpkKLstyr3oT5DCnrMEEn/F8HTJ8Vu9LfRn+ln5+FvPOGm6xBRw5I5J8ZmvMi5BpJtS9yjBpD/djDhoLzK+QWUbUg8s6tGE6vOuO9aZmSOjXGccUpMoW7ipk8+yZdBOyoKMMHY+ZQdpkVscJbVNfMKkvHLDYRpLm+3Xuakjaji+qCeNrBgExw71lyxi/eYN6cRX6F2qnZQusGyBDdk1stdjlSGQLR/P58Hi5WREeIYQQQhw9WvAIIYQQ4ui5MZ6XIC24UYpBOBUOnLqM0OeI7FHITWj9FjIZam91SHqVpUiKN8EtsITTpEKtonyBOjy8trRf5j6Diyyr4lo7ShdIADiGicrSGZIWdi9dv24omeD6ZkXcZ7NF+PU8wm47j2PWTZyzW+87Hg5F30Q489Zzca2LBZIQprj/vo2wYwZdakACww61T+opzt+OqG+0jL6T7qDNPwJnDs7TIKSf4/XDT2Wi44PGLDgD5l0cc2sRroQcoeMR4dwcySNzOCycrkMk1vu4t0Y/rG+Fg8GW8fOxf8hddgB6OBxzJNvzveRbcf3Llg4GOB8RBc4gczKz3Yh6QxVqJO0g9SSE7gskG8vXqPtzGtdwu4JDxMzWU4SsB4Syhx3GC5TBE8oV87jP7l5caz6LEHq1gTQKma3tITnsQvZbXyDEz7mve7T78mOlYnJKFArLILF2BZ06nLPQbpAHSsieEyQw7yHnY4uBIQFcCQl7xup0GDeLCyS984eciEiUl2GunmEsVJDxWjgKeZvnKcZjAXdwDclmxGcvxpBT+xTPYo17GFBnalE8sEPTo20WkIMH1H1aICnkAJl4i++fE49nsl7FPS6xRWREfckMtSKzHPWzshhrDRIe5jvMFaiRVdj+92Y3owsQTskqnJY9tkJ4FmOkbj98/XqLOYVG2hF1AYcTJBRm8k/UHss92ixBlpv548emIjxCCCGEOHq04BFCCCHE0XOjpFVbhLm48zztRc2R0GuIkHBWQwKZYZd/BskI7pKWRVAgQ+TzOE/JUvMjEsQh/NohrJc3+6E5z1GjBiFeg4vqnDvsL5AM7EUkzINGN/B+Wrid4OqZWibni+NLhGhHuFya6ck4QSrc/xq6z2oV9/Pxs5C3PnI7ntGebIIQ9S0k8TqHvHkbodMdkqeNSIzWnsR9JjhtKjic0sQd/Pvr86GjewhhVCScPHkOodoz1Nap6MaDYwJuoXoV5+mRAPMMMkiBGj2nS9R1gVxbPCStHgLk8jSUnNmTlQskDOuRlGuZQyZBkkfW4WLezMR6U6ijVxVwXF6gXg8SPC4hc267kCeWFexRZlbgM0bU0sLQsTlcQdUKtdrg9nG4kYwq3i762oQEi2kJd+iEdmrh0kKyvaF5fHKz18JdOMSehyS4baI/ruCiSmdxn6dviYbOUBewH1DDCn22hfSeo63qJtonW6DWnMecMMfk/wCOq/oEnc3MssT2hFvwASRzJN8bcH0D6xPuYh5ZY57OUNMsR/3ElslvMe6anvvmWgAAIABJREFUBnPFhLm8Z8bTw1AjjjAioWTaxTVskeD2pMT3ErrgroCjqsH3SRXPhHX0dpB3xoHuVNSRiya2Af2jwjUzCaSZWepRzw0uwDM4JQtsf2khwzaQpIuKdRrjnKmO17v7MU4LOALXOH9CAsOBSXpfJtkvUYRHCCGEEEePFjxCCCGEOHpulLR6bKUuEVpt5qyngx3TXYQ1mWqtH+BGQlh2DlmlRLiaEsM0slx8HJJhd/qwDinJEGY/2e0nImKpnDFDcivISadDXEePz96gZokPCKlhh70hLN1m9+L8CN065Lp763jdbO7j+Cfj0roPd8YJkuQt65BB3vqOqKu1fQH1lOC0mtdxzLSMEGSNxJAXu+gXOcLS/S4+K7FB4WpjbaQZHHisn2ZmVj+PsPk2sq/NTyIEfxsGllu33xrvhTzGmmnFIhIJVqxvhiScqYpjVneiH95+Plxapys4DbPHJ8R6tbTzOOdz6OfTaTyHootnncE50W0RWockXUFWpPsBJjbrEscm6hPtYgyeovhOgfGYSjhrmofkyTHktAFOoBJzhJchD2zOIVHg+eYe17drUWcHIfRtF581g9Q3NgjxI1HhfahveX/4tjQzO4EUt4TcmjBFU+op4TpzuJFy1lyCU4qS8e0aDkIkY01wFhaQj0a4FccW0gprCj70WNLIZIPRN7YbJMBk4kLMqc0acz7G5gzJaRundBXjcWpR/xEJbE9QS+w+ptf+SZjumugwF7iGGSTvNMRc1uKrr4B8lM9jzikQm+ggKzlzDbaoVYdkjyWkpDkcti22F7yELQv1Q+XF6jnaCTpxmke7zirUHYTLcontH3TD9qj11UP+9DK+T+5BWuPcNMJxWiDZcbfZrwH2KBThEUIIIcTRowWPEEIIIY6emyWtLRJxwUVxinf1CMFlSKzUeYS4qsQEbhHi2yaGkLEjneuwbfy8QVKiDHFAROsth9NiLB6qYYSd7qxB0jMShn+wZtJ8gKMKDoEZCu202DGODey2Q4jTEJprUa+nQPkST09G0tqykNU8wuaLRYQ5n79z+/p1ifDnkDHJWJxm2iEMDneZow6ZnyAhIY5ZNnHTLeqs0B3odRxTVPux1gLP0t7+/PXLOaS1W3M4g+BGK4q4/3qJ8C8cH2NCHZg+rm8FR0qxiGNOT+FmKeD+8MeHWl8tS8gYXQoZq34Q13OB8HCBZIlzxKxzOBt61Cc6gXS1g5NngvvDEAafMNZG1AbyxNo78fNFve9c20KWSy9BfnsOjqoKtZ6QALDEBHAXQ+cME9UF7gHlluwCY23C3DTArXcHIfchg3x+QOioumjhRi1jIrldxNic4Oy5C+nuDLXHMkgZHcYvbtMKyFs1pOQta6Zt4xq2cPgNeO0POWJnSL7Xwr263qBeGZyPCUkyG8y7E2pIpS2dv5C0hujnA+rE9eiHPTJsDphrdnTjHYgRyRyLDvIL5ac6vjdKzKENaknl+C7KWBvLQsLLUHfOMiT8HOAYhqM5x/d4giO5hryVd/vPxOdxfRkkZhqd16hpVUM+7Ba8T3z3bePzhjyudcKWEkxxtuU4hds4YWzisbwsivAIIYQQ4ujRgkcIIYQQR8/NtbQQUtrBpXKGsNMGu829x27rBNcFEijZDEmM4FiyAg6ODcL1MObU9+BGWSHJIXf7w+E1lPtSwmwTYbH750g4hjpOhnoci45hWoTRziPkt0FypBx1bMYctbRgBaBcNUO4P41x3f30ZNahNdw5OySjKquQYqDKWI7nN6EzFEhutl3C1XYraqs0HmF2KBF2H+HVHWTCHKHrBpaPBBfR3PfD5iNq/zDh3glraTERH8L3JSSVHM6DAo61GuHfGrVcyhx1XXDODPd2gaSS8+yhOkMHYHsPYw0uinVHS1E8x9N5tGWHceAzhLLhnNrApbPd4fhFyGc9ZI/FmhbIuLYHaPwFkoJ+pN9/JtM2rruDO7R6ED8vOCzgBNthHlniPi/OIekgWRldIQyPt8hymCHBYgcrzPAKkpu9Fkb0kekEEiJk8vu4vhX0hHwec0faoa5YCUkfc3aFeWqiBAQJpetQlwn9esA4cEi1/iBcqWZmO3y13L8XElKX6NSDawd9tcB8PubxXDZUtDEvlJibS3yjtagJNTkdQnFQOx1e0log8Z7jNdWzNEHGgpwJ9c/OCrjPIA3NULdugnTYQCJkIs8SUvUajuEtaiUmfEf1+f7YrPuQw7sixv9sh0SVSLRq/P7GtpVdBVmxgvMP88WuxfYX1Mub9mR4JC3EeKST6+VQhEcIIYQQR48WPEIIIYQ4em6UtAo4FVjP/RwhNZoWiuzRjo8cbpcZykQlJPfqLiJsVg8MP4Z09QAx7XxHaSiuM0dyo5Tt10l5CfWtKIONCKf3rONCBwvcXx0cCSNqAiUkzMqw0z2VEb6csDPeESoe4TQo0n5dmkNx9wLy2wpOMySxKwzJBudwVEDS2kR01W5DZlqv4bpDVLRGLZ1qL+oY/cIQptzBUZEh/J7N9t1rQ4OwNsL6JQrGFAXkkYL1YpBwDev+GSSeE9QcavJo56xASJV9FX8+9BdwAdY3DrPXRD5D0krWT2rj3lOOZJFo1yGLBvQU97jcQpJr45gLuKvGl3ANcGZ8pEZtNoSiu3PUhYKzqocrx8ysyOAegVRZIeldAdk7zZDYEpJ5uoBEU0LGopMTof8eCQkr1MyiUXLEc7TxoaxsB6LEffZIIJfvUG8Kg2eN8XKKOaudRW231GFO8WjnCUn+eshK0ymS28HhlKMG1A6SZo7khynt/+28yeKzK8hyCa7LRR/z4hp9uIDcTFmjhL0OarhNcGxRPl/gcy94fUwE+gT+5PcRySIh49SYy3b4bp3YB5EodYCsOnXoH5R60FGRc9NqyHnn2zjPDG7FDq7Xijbhab/O3QDZj999HZ7dYhvzxTbnuINk3lJKZLLBeC5VFvPXLov2nqBn5sbxiwSX6fE1KBXhEUIIIcTRowWPEEIIIY4eT+nxO5uFEEIIIZ5lFOERQgghxNGjBY8QQgghjh4teIQQQghx9GjBI4QQQoijRwseIYQQQhw9WvAIIYQQ4ujRgkcIIYQQR48WPEIIIYQ4erTgEUIIIcTRowWPEEIIIY4eLXiEEEIIcfRowSOEEEKIo0cLHiGEEEIcPVrwCCGEEOLo0YJHCCGEEEePFjxCCCGEOHq04BFCCCHE0aMFjxBCCCGOHi14hBBCCHH0aMEjhBBCiKNHCx4hhBBCHD1a8AghhBDi6NGCRwghhBBHjxY8QgghhDh6tOARQgghxNGjBY8QQgghjh4teIQQQghx9GjBI4QQQoijRwseIYQQQhw9WvAIIYQQ4ujRgkcIIYQQR48WPEIIIYQ4erTgEUIIIcTRowWPEEIIIY4eLXiEEEIIcfRowSOEEEKIo0cLHiGEEEIcPVrwCCGEEOLo0YJHCCGEEEePFjxCCCGEOHq04BFCCCHE0aMFjxBCCCGOHi14hBBCCHH0aMEjhBBCiKNHCx4hhBBCHD1a8AghhBDi6NGCRwghhBBHjxY8QgghhDh6tOARQgghxNGjBY8QQgghjh4teIQQQghx9GjBI4QQQoijRwseIYQQQhw9WvAIIYQQ4ujRgkcIIYQQR48WPEIIIYQ4erTgEUIIIcTRowWPEEIIIY4eLXiEEEIIcfRowSOEEEKIo0cLHiGEEEIcPVrwCCGEEOLo0YJHCCGEEEePFjxCCCGEOHq04BFCCCHE0aMFjxBCCCGOHi14hBBCCHH0aMEjhBBCiKNHCx4hhBBCHD1a8AghhBDi6NGCRwghhBBHjxY8QgghhDh6tOARQgghxNGjBY8QQgghjh4teIQQQghx9GjBI4QQQoijRwseIYQQQhw9WvAIIYQQ4ujRgkcIIYQQR48WPEIIIYQ4erTgEUIIIcTRowWPEEIIIY4eLXiEEEIIcfRowSOEEEKIo0cLHiGEEEIcPVrwCCGEEOLo0YJHCCGEEEePFjxCCCGEOHq04BFCCCHE0aMFjxBCCCGOHi14hBBCCHH0aMEjhBBCiKNHCx4hhBBCHD1a8AghhBDi6NGCRwghhBBHjxY8QgghhDh6tOARQgghxNGjBY8QQgghjh4teIQQQghx9GjBI4QQQoijRwseIYQQQhw9WvAIIYQQ4ujRgkcIIYQQR48WPEIIIYQ4erTgEUIIIcTRowWPEEIIIY4eLXiEEEIIcfRowSOEEEKIo0cLHiGEEEIcPVrwCCGEEOLo0YJHCCGEEEePFjxCCCGEOHq04BFCCCHE0aMFjxBCCCGOHi14hBBCCHH0aMEjhBBCiKNHCx4hhBBCHD1a8AghhBDi6NGCRwghhBBHjxY8QgghhDh6tOARQgghxNGjBY8QQgghjh4teIQQQghx9GjBI4QQQoij52gWPO7+p9z9q572dYhXh7v/ZHf/Lne/cPcvedrXI1457v4Bd//FT/s6xOuHu7/H3f/0Db//Z+7+2a/jJYmngLsnd//kp30dr5biaV+AeNPzu83sb6aUPv1pX4gQ4mMjpfQpT/saxCXu/gEz+6KU0l9/2tfyRuFoIjzimeUTzeyfPeoX7p6/ztciXmfcXX90CfE682Ydd8/sgsfdP93d/9GVFPLNZjbD736zu3+vu99197/s7m/D736Ju7/f3R+4+//o7v+nu3/RU7mJNznu/h1m9gvN7L3uvnb3b3L3P+Lu3+7uGzP7he5+5u7/i7u/4O4/4O7vdvfs6v25u3+tu7/o7t/v7l98FWp9Uw7mp8Snuft3X42nb3b3mdljx2By99/m7v/CzP6FX/KH3f1Hr87z3e7+066Ord39a9z9g+7+EXf/o+4+f0r3+qbC3b/M3X/oao59v7v/oqtfVVdj8uJKwvo38J5rmfNK/nrfVb+4uJqv//WncjNvMtz9G83sHWb2bVdz6+++Gnf/obt/0My+w90/291/8KH3sf1yd/8Kd/++q/b7h+7+CY/4rJ/n7h9y91/4utzcx8AzueBx98rM/qKZfaOZ3TGzP29mv+Lqd59jZl9tZr/azD7ezH7AzP7c1e+eN7P3mdmXm9lzZvZ+M/u5r/PliytSSp9jZn/LzL44pXRiZp2Z/ftm9vvNbGVm32lm/72ZnZnZTzSzzzKzLzSz33h1it9sZr/UzD7NzH6Gmb3z9bx+YWaX4+zfMrN/xcw+1czeddMYBO80s880s59qZr/EzH6Bmf0kM7tlZr/GzF66Ou6/vvr5p5nZJ5vZ283sv3hytyPMLvfWmdkXm9lnpJRWZva5ZvaBq1//O3bZnrfM7C+b2XtvONUvt8v5+Y6ZfZOZ/UV3L5/QZYsrUkq/wcw+aGaffzW3fsvVrz7LzH6KXbbn4/idZvZrzezzzOzUzH6TmW15gLt/rpn9WTP7FSmlv3GYq39yPJMLHjP72WZWmtl/m1LqU0rvM7P/5+p3v87MviGl9I9SSq1dLm5+jrv/BLtsuH+WUvrWlNJgZl9nZj/yul+9uIm/lFL62ymlycx6u/zy+/KU0kVK6QNm9rVm9huujv3VZvbfpZR+MKV0z8z+wFO54jc3X5dS+uGU0l0z+za7XJjcNAY/ylenlO6mlHZ22c4rM/vXzMxTSv88pfRhd3e7XNT+jqtjL8zsvzKzf+91u7s3L6OZ1Wb2U929TCl9IKX0fVe/+86U0renlEa7/KPzpqjNP0wpvS+l1JvZH7LLSPzPfqJXLm7iPSmlzdW4exxfZGbvTim9P13yT1JKL+H3v8rM/piZfV5K6e8/kas9MM/qgudtZvZDKaWEn/0AfvfR15ZSWtvlX4tvv/rdh/C7ZGZ7IT3x1PkQXj9vZpWhPa9ev/3q9dseOp6vxesD/2DYmtmJ3TwGPwrH4XfYZZTgfzCzj7j7H3P3UzN7i5ktzOwfuvt9d79vZn/16ufiCZJS+l4z+1Ize4+Z/ai7/znIkg+3+ewGGZntPNnlfPu2lzlWPHlezRz5CWb2fTf8/kvN7FtSSv/vx3ZJrx/P6oLnw2b29qu/AD/KO67+/8N2uRHWzMzcfWmX8tUPXb3vx+N3zn+LNwRcxL5ol3/9fyJ+9g67bEuzh9rTLgeoePrcNAY/CtvZUkpfl1L6mWb2KXYpYf0uu2z/nZl9Skrp1tV/Z1chevGESSl9U0rp59llWya7lBdfLddj8mrv3Y+3y/4hnjzpMT/b2OUfFGZ2bRLhHxMfMrNPuuH8v8rM3unuX/qxXOTrybO64Pm7ZjaY2Ze4e+HuX2BmP+vqd99kZr/R3T/N3Wu7DIH/31dyyF8xs5/u7u+8+ovkt5nZj3v9L1+8Eq5C5t9iZr/f3Vfu/ol2qSt/NA/It5jZf+rub3f3W2b2ZU/pUsU+N43BH4O7f4a7f+bV3o6NmTVmNl5FBL7ezP6wu7/16ti3X+0bEE8Qv8yP9TlX7dfY5cJzfA2n+pnu/gVX8+2XmllrZn/vgJcqXp6P2OXex5fj/7PL6Nwvuxp777ZLGfOj/HEz+33u/q9eGQs+1d2fw+9/2Mx+kV1+D//WQ1/8k+CZXPCklDoz+wIze5eZ3bPLfR7fevW7/8PM/nMz+wt2GQH4JLvS/FNKL9rlqvQP2mWI/aea2T+wy0Eo3pj8drv8EvyXdrmJ+ZvM7Buufvf1ZvbXzOy7zey7zOzb7XIh/FomZnEgbhqDL8OpXbblPbuUwl4ys6+5+t2Xmdn3mtnfc/dzM/vrZvaTn8yVC1Db5Z64F+1SwnqrmX3FazjPX7LL+fmeXe69+4Kr/TziyfPVZvbuKyn4Vz78y5TSAzP7rXa5sPkhu5xnucXjD9nlH5V/zczOzexPmNn8oXN80C4XPV/mz4Db2fe3wby5uAqx/qCZ/bpnYYe5uBl3/6Vm9kdTSp/42IOFEE8Ud3+PmX1ySunXP+1rEcLsGY3wfCy4++e6+62rUO1XmJmbQqzPJO4+d/fPu5I1325mX2lm/+vTvi4hhBBvPN50Cx4z+zl2ufP8RTP7fDN75yu06Ik3Hm5mv9cuw+XfZWb/3JSjRQghxCN4U0taQgghhHhz8GaM8AghhBDiTYYWPEIIIYQ4em4ssvgf/cqffa13pfMoodHhmHk+Xb/OUlj4+ylyAq4WIZt5fp3nyKoszjTiveUsLivb4RJTrM+8CGfjVEZR7b5t4ue2X2w7z8OtnPVxrnyIa81uxXvKIa57Qr6mXYv7GeM67vUb/BxSIc6fTuJ1PvJ13OfW4jxf/+3/lMkVPyb+wFd+zvVFdRs8iynacECZm6mJ9mks7tO7eG/K4/hZeV2/1SYbrl/3cXrr2/hHgWc6IYdk4XFMnuI85tXe/YxDXF9+axXXtIlr7cZ4/9THzxP6UjWLPpnQr+pZfN6yjvtseP9DbP9KHE5V9Od8inN+5df+zYO05+9577dfP7ymO7/++dQvr1+PVTyfqo+2KZBpI5vFczi/h+eZx8/7FGNqmOLNGf5cKqdoy34Wz3zo4hnOphhbM4tzmpm1GGvDIt4/H6Ntuime9ZDFs55hfE1FHDNb3L5+XXi0WY8kFAPmjtkyXi9SnH9oOxwTz/fdX/jzDzY2v+aPf+f1A9hu4wJTFs/bbf3/t3cmSW5kWXd+nfdARJDMrCz7TXMtQ/vQTJuQ1qGtaD2aSn9VZpIRgcbb1/yDkvF+HsYMVqWBA8HuGYGgB+D+Onfc8845X1/Pm5xfQDIVp0vl5HqmEeMU6yDH7D9sj/7f51vpnwPmr8H5LJscU0l28z8+K8g1zFjnMLWNq3jvwBpsZSxUVto+rnKuJ4++wueHg4yX1sm8m4Oc37Fgje/k9f/4b//lJv35P//X//56ARw7BmP5krEuYR3cRklu4GaTtMg88j36D58fce/zWSZngwFice1s/8T7uN3XQVosGLjdmdHKNTjcE5sOnZxExZ6DnHd2uIdGOb44vLYPcj38TKzdQyNzPM0yP/77f/3P3+xLrfAoFAqFQqG4e7xb4fmlkifD//MgT5hhlCc1jyf+Fo/vQ1vwPn6CWHGFrzo+ncqzV1vJd1mEs1r7KH+LYyacT4Nfpk9HOd4YYyY8YdoFv+AcKi0rnsgHeZyNm1yP6+VJerni10WNJ89Vjl8yjk/SXkOQc9ie5Gn5cfoxUUE2y9M2fkSZVMt321Fe+1qO70b5ZbCi3wJ+qYQa/TajXYK8piNgwi/KtsIT/1XaJeEXXqj3FbsK35dYjRvkvHv8SR7lF0DAL77ay/nNQf7gUOOYSipIYXuRz0SFxDn59T+vqGS18re3wvPlt6+v4yhjKkS5xrScv77+jN+LQy+/NCuM/WeMj2bEmJXDzdFJD861zOu0yjxNz+iLINWnDb8Up21f4Rln6fPmIt8xNWjHKGNtgVfomb9ssaIlVDHrJBWO+SKfU1DF+9TIenHq5JiEistjg8a4IV5RgbEGv3KvMv4LfhWvSfp5WaTfCtrLFlZiUUVgNQWVWIv2TfjbE46pUTGdN8ybSs7HGGNclHnkIYzZvPz9usjfN4bHS3uvAb/JN6wemF8TKrT+jHGFse2yzNnLQfr8UPZV41tgW+X8J5xnXp/lGPRHnlk9l9cb5u+My+o2mUdrwfg4vX59fUXlp69kfFSPMkGqEWsu1sM5v/Hhxc1iRndMBms8Kryrle8Lq4yLzbHCKyiohm/osx5rh7vKMSPu/SXJOuhGmkR/G1rhUSgUCoVCcffQBx6FQqFQKBR3j3cprQtCiY+kD1DuxNvGg7AIWYpWCzYVtoO87rwc4x9BGYGW8lnKzGtE2RsUyFMvVMIzLulY7yOV2vrj19eWtBxoD1ukNH9ClfaMjde1VCZNxY2hZ5TcQVcFI43ksMnXO3m/xqY0e/gx3khNxgavRjZGJ5ScA/rHomTpOjnvwyINdgWNOU3Y3An6yWGTmUO52oHqGop8/vYo71+5Qdzsy88DNt2toBy3X6UPuyfsXWtkvG3Y1Fdh42qT5P0lgJYEBRMe5Vy7JHTVErkRWo635d1p9qdgQQH5RDpBxnVcQZPM0u4Wm3+vWa6ldzKwJ9CZg8PGQ2wqTBc53oMOMWjbgLm/YTPkkqRcbYwxBdQoN84/gA2fVvn7uEm5fzth3Wnl/LpRxkHGvDZnfOiA+V7JnGiSrCkOdN3V32yf8g7nz6D3LcYvtgbEFVRGj/UCY7OGGCNio7YHxTFijHSLfOYFdFUjl7+jIjLmR0rY6HrcU5T2wvUfNDko4AZClRyl7ScIXgrFFZXQPQ4UWDNhQQbleEFbdFjkLXiZcSfBuQ2mq8yv8SLXFTB2PLZ2jBAduA1iD2znqDLWELSJ2eRvSSubEzYUD9I3rsb9dAGVhg3x67qvg1yP8n0tKMMMMcaKrS3pRfo+dhCTbDLvpkesI9jOUlPggnuIxT1heAXl3cnf5gVr0B9AKzwKhUKhUCjuHvrAo1AoFAqF4u7xbq196OCHc5QSan4BdQOKZkCJ61JLCeoxg/ZBKdIFlI07Kd322MNtsSP7k4NawEgJrYFvTw9aoX1TfrbYGu7guWHxuVdQBSP8HR6w2zwe5ZgXeCs4lJk7jxLyhDJuhzIuTm8D5VCnvRrpZuikHFl+gy9Hg13vnfThwcp1XqE621Aqn0eoBGYoPhz7UK7/CBXJmKC0wyXP8GjpGmmkGPeRZxNpkEIVHT0b0M9Qchmov+YRPjzwEvKjjG0Hz4kM+vEET4wabUTrkqq/vRLEb+LXYekr0+GgRIWIHHSZpPTtJunv8yZ/3GKsrA/yukV/LPBzsREeG5jvFXw4vJPvHd70ZQbdFVfx35ixpiwnobESVCFhhkKM6iDM63mR688FY/NVjqkGGe8rGrUFDWOn21Mg//g+UdhsiPbbrrJGevqWQdkTrVxzgoqutKCJ0V4ZNOaM7w0c+/BbCfBPuoAN7EAnmHGvRJygovOY8ytUSFyPMzxz0lmu03HKghKZ0bdUR3ooihLW1BPG7YFqNHf77QMbPOsaI325eKhkMZZrKNci6Oaq4N4C1aCd5JgAhWK6wL8s4hxAh2FHgSnYgjFF+Y8t7amhVk7V/AYftRLhw1TjXB9wD8b9pLRy7+u9bC+J6IOAbSgblKXNBK+tBv2Ha6jD9+lmrfAoFAqFQqG4e+gDj0KhUCgUirvHu5TWDBOg9SSvK6huWljuU43zMxUZUCM5UA9dkDLogJ3gDSiD6oiylpXS5QCTNFNDQUVDJ2xaN8YY39KOHucHg8WIXe9dI6Xck5Nd9QtKpT5IWW/A519g029Qcq5gdFXBfp30jgk/5jnUQzlXnqQ0eQSFFmBKF0EbzrNQKB7XM0D5lCvQZJXUQW0UZcBWaOwnHQQPM+MvMN+COWHOe9WdhYqqQc5BR2OqB5hhwrhsSainw1a9oJZPmmypYdgFeiDhlBYYfB2gFuma21NaqNCbK8zHAowwpyhjucHr7TMoSbRJXBBdANqjrYViykGu62HGvIaa7lLJ+TB6ZUN8Sp3AjRhjLEr2DzB2NFAKZkSJUIB5pZEglFZjgOIFipQDyuyWcSWcp45rCuIx3kRi3AoObVNAIW1J5si5yPiqkfcQcA0rIzqg+FkRdZMXaSPPCBhQaR6/hX0PSi8j6qNIO4ZALtUYC8oSHpbGz6A+ScXxPPA5Hc57ArXY4n27Qv0EFeESERPTy/mtHtd53Y/DW8DaL/IP9Gs4433cHiqsDwEKR4PYiwJKdnWgdiso7jD2LZR1BbE921WOKVDGPj3IeU4vcowxxmTS0glU9wTivkMkxuWTfFaN7QlYN+dZ2qI5iHp4bUFnIsLHDbJej+Dtbca6FvfxJt+CVngUCoVCoVDcPfSBR6FQKBQKxd3jXUrLsawLxY5FeXi3Kx4GZSxZryhFxhPKZUFK6O0sdNURJndgQExAaZlKrs3BWCmizB72O/ArlMK2BjvMIxVbUJ2BWpqgirkg6ydApbbUaBc8S3aJBk0ov8PgySKfqtjbZy8ZY0w/SOlwYdoy5GIeZeAgyR9bAAAgAElEQVRpBV0HRU7yVJ0hc+dCUzHpH/cBCpkvqG8zYy19myrIMElzac9Repg7VhgoVMj5SMNMKcfTWC2v8g/bQO2FNooYI6S9anzmwowimA1ac/uyuQOnVaDYCTSVA707Y6pb9Fm5SHnfgaJ4uDx9fd20mLNQynhkaYF5ND/3UPtgLGcoPKzZl80TjeSctKmdYOCIZsxUaR1Ak0xQ02H8MlMtRqEEqiB0XQWasMOcoMIr+e/n9fwZvK6SCWQwdlpQow0Ua/UMtSfc2iLaKGZ5vwaF/wXXWTaqzmCoiXUwQq3YDPxj+fxLBF1jjKkxJh1VndgCUS9CR2A3xJ7ew7hNV/m+DWuQPUCxhTFfD0KlxwuMLrHujh1MC2+E9Yso36h6dYvMhRX0U1hku0TGNorHTeZmREZeRPtUyKer8V0GlOJvJ6i6kBVpoWpqsL45t1c7bU6ux2J+DZj/F+xJCFaurYdyLDfY8mBkXI8z7tOnb9PKVzxzFFLVMKwsaZ/n9i1ohUehUCgUCsXdQx94FAqFQqFQ3D3epbQqL6W2gJrj/IByOvKtKqhd1laOOSbsMAcFZmHaFwpoBVADNfKTGhwfQWMtUJpkmCaFtL+8UMOkCHTFhnPqQXutFUzJLigtIycpozzKCJICo8Z4kevvOxjpoSxvNylNtvWPKZsblMQfQfWcUX4uHlSXZb4NsqcaaaMKhnwPMG7LHiVYZLxQ+VZAYyWMEfg6mhYb76eyV2mRamgwlEtFhYK8pjEVS+gFJfFtZO4ZMuDgxTUhA4iKsCcYLCbkxC0/IH7JwnQxBKFoug1U7yrHdEH6+OV36Y/wIv10/IlqDFAdyMapYEhYFxhZ4nrbK5SINRSNOwpzr+pJoEwLVGcJ521guOap5MScH0EDbMjxqio5j5k5d/73r6+bWdQldSXHzF7ad42yDtwSM30Yg4zZFQZ+zLFqYdS6wsGvpTgUuWIGJoQNqKHshPZxoOQpFrLoAguDuhXz0df7384W4yFt8n0taIoFKqEG15CsqPRigUIQJxUxl9Mz1gVkMtrrr19fbzQmxfpSttv/5p+ibH+oJrmv1UVotWWTPm6drLML1q4rM99wvyto9xZU1HqFQg+GhM1ZBtcJdPyA+2xq5F5PlaAxxmTcXz0sVT1o3yNUmg73OIc1kVQZvWErZKSdLO4J+MyEtcNlqNGQBTdmpbQUCoVCoVAo9IFHoVAoFArF/eNdSivADIpqmYBSMU37WhzTwlmJpUuDEnXFLBUYem1JyoAJ2Supkh31LUqRM4yVqgXn2eyf586TlMIamPD5CqUzUi6TvN8e5RpeNspFpEzps5TXfEUl005q9vUlKYEFChnXvNstfxrFS/n62sAoCqXAgNyvBSq1AHUFL8fSMNALVXJA6foLK7MTqAvUygMc/DzKtxeIebpqzw3lAWosnFOB+R6VAc8okfaBFCUoxEc5vs2S90LlyEI6DOowl2XcBpyrDbfvzwRzs9VyHCEjbAMXAc6kg8HgBNrWjcjfmaH8+QhqD2ZgFczNLJRxgxc1YLEwyEMf5Xk/N9se5mgI/LEwFRyh2AqgQAOs6irSsMh2O2PoZBjyLav0zbkTpdTihd7ambi5H5OlZbDOZWQfBczBBqZ6iC4zdYZKFVRchiFhwnm3MAtNGBcW5m6HIK+ZdeRApdbozyXuafjGgtKGcaFBew9HSCWhHiowee3OcqEXZE7R/HYLuE5Q7DOonzrIZ1rkIrofkKVlmC8IyvvKfgWNs2FBrUcoK6HujRuyDDEEIWoyNe6hu2vH4thhPiZke0Uo9Irdj/FC9aIDhTRAxVuLAs3hft/i+aBAKpmRk7XBUPSIGsznBorTSeb7hLzLCPrTZDUeVCgUCoVCodAHHoVCoVAoFPePd2vtKzKWIsrjLUyDHqnMgqneNEq5sqBs3sG4q66l3Pc6oSw9Ix8DVMcj8mPmCuZ8UFll0AqXy5vyM652SuBKTnJ+fsD28VWuc4NCIqDkt3qqReQzZ5R4h13tV15mUG4ZRn2+/IAyqzHGogxcQb20QjHjUGpta+RkgUK4gIqrNxg4BimPLzAlY55SgDlWi1Lm0qB8e4VhHLLUWroFGmOGJGVqNnGE4mcBDWBIa4A2mbOURY/hl6+vXSd9mKBUsMiQiSihJ9BJ2yxtGtztZVrrWSjJCLXEr89yLY6ZZAsM36hSmkWBtFyQTQbVzU9QNLatzLuKojm0Q5iEMshX+Zwa5pqH+CZfDCql60bDPDm/B8yXBLrq+oI+gIpoRNcHZCbVH2Vcz6BoFrw2Ra7BwxPz0N7eRNIYYyLULw70oAPtkBNy4XZCGpkXK01B0T++lfYeQeN0DdZRmhxCLVQjO7AKaEdQyhXWUGOMWdBOByiw6kH6M05UyoIaBq3ua/mOB5i8WuZDYa2h4atFRlXZhK7NGNs027wVIuS6tQPlUv9FXkMBGid5PUJB1+F+NyKDrEV7lpNcF7wiTYu1/jDIfyTcr/gHNuB7w36Mh1f5vxcvY2EAvbVTOmPbSujl+yyVclizaObpwEo9Pcu1PWN45QhjR2xhWZDf+EfQCo9CoVAoFIq7hz7wKBQKhUKhuHu8S2nZiNKyoYGQ/Nl1kpJjRj1qgJmbxw580wsN8YJ8jAoKmpkGWMjY2T7L+30PkyGojw4NqJcr3bz2Jb9wBaUjp2RGmDeFLCW1+AwaK7CWj+MrUCxQaW0oV5cI0zdkBlVUiuUfQ2ktMF5cUXZ1pK5gTNVSRocyuEf8zAUmWzbT0Eyus01SaqyRgeZrGUd/hZLj749QgsxQYrX75/OwkU6S/hwaMStrkZn2CsqmXEE/QVVhH6UPE6icGSX6bZQS7FLJ6xWUwyNoz+C+rx74V5FgKliueD3K9Z5ADT1inD4dodIBRTX/RvNHlLg7KUsfB2QVMX+n0PRM+j4hP6uakYXn3/zWmuU8fkZO2Ii8ng1Glc9Z2vcD5s4Kur2j4SXYhLWT7woRKrCCrDbmVmE+rtUP+o2Ir54cKIEVFNIBSqggVFep5fwqUHphkDy0gIypHgZ+CTlcHejsyoEaglKsL9KHG9Sk9dN++4B7lc/aZQyu8vcTPBxrUMYbTPA4ZnIl57pgbV+YE7eCSsd6RCNEmtbadHu6uWmRgTXi5lJk4UxQwWGpNDXozIxsQgf30lzL1onsoCrFGmip9AX9eQDVVUDthVrGCpV4xhhzOgot9wHZWAvz7/B9vROKtcUzROS2g52Tq3y+pSEpguEq0FgWcz8hd2687LMWvwWt8CgUCoVCobh76AOPQqFQKBSKu8f7jmi7sqaUqa4oj7ZJyk40KlxH+dvuIOUyw/yoKKXOayu0VHX9mxwOM8MZpbLlVZRcx6OUqD9D7bTlPaW1oDz6gHJqsvIdDkzZBZKqK8yUxhfJ38kdSsuQAR1gFFWjhL5AvRYho6DBGI0db4mCcmbekFdGI0XQbDQ34274jEwYY6nSQ2l9Ii0J6mmQPgkw3Ip4/TPKvQa7/L3dP5/PKI8H5ClFGJQ1oDjA5JjPtfRDhqHdM2gy56XUmr9A2YPa7AFKwwVjb3TSXp+sjO1boQMVcQUlF2B++bGSwdwNQvMNEGr4s4zf+QgaB4qzJ6hgPoCKaqGCscjfGU9Scl9HKMWClJzXwz6vh6XvDaV2C8NHD/qhh3LqBarBBtTaE75jzqBJQFtnGB76LPOgh9JvgmNit32/bP5nwEinGerYMH7b6JDt94TjwcKaCmOZWVe5A2WcaFA34Hhpo0dQ2ytoopBlfWz33WkmGW6mWWVMzlDwPGHdnS23Esj4SRhvzHgqXub1YKVPRtxTZlAuPfOgQJskjJ1bYTnjM0E/lWcoeo8y7gIFxxfMKWx/GHAvXtAHPbImo6VRIdZAHF/jntMkuS/PG1TLbwz8LL57JVOG/ogjqVSYCsK1cIKZ6Scq9GDseYGR4IY1tBllLbuA8s5Q9O1uWH8ArfAoFAqFQqG4e+gDj0KhUCgUirvHu5RWRKkJog1TznDPw+7sMwyUPn0C1QOFC8RYpkLuSzWKkue6QkUSQA2BPnnZsCMdKpXwQcqAswVvZYwxKO39DleyFpWwjDLaspAmgQkUaAwHVUxGeTHAtM5A/WD/Iu1SgxnyKAOa/seotJyVcyqgtF56UQ94lOwbZLAEqAfgJWeWCTv1eymLbg/SXmcc/xEZU8cB2WNQvuUDytugMcKbLK0e9EXBs3uEAi8zE6pAzQSKbiONhww4d5W6fAJdGUGVXjYZqy2UIDbS0O77pdZ/FaGDsRhL9LX0X7UKlRaszMEMlY6HmV/f4fyfpYS8RlDD/Qe8lpdNAA2Bee1gRvm6Qun3ed+X2xMUNaDDGxhVOmQRzZYuoqCooOQqD6BDQfvYDzCbO8CEEblNqfk2ZWLc7fvSGGMi2r7F2AmgIAquISWYdkL5+ABvTvi5GY/5S+PMilQ6DQnRvA6KQDb7z6A0p7T/7Yy4LrOgT9Izvht/szPEAyPkYIKXmRNHmhzUWoEhZcTtbQJllkB9uOvtKa3rZ6Fl6krmpq2RW3bGfS1xXUY/YW6+bHKN2EVhwPiasPFGhmtE1piJGCCdjHG/yvwdnSiijDEmvCB/C/f7MkFp1cn5ff5djm+eMM9hUnsGVRmgdKUx7wBl6Yp2qdBlGzLibFKVlkKhUCgUCoU+8CgUCoVCobh/vEtpOeyiL7XU0TzqldurlOlqGNgVUFEZahpmdkzI+lmsqCg8yuMFx1+RM7K0Uo4jA7RC4bS8KbNuUIw4g7JdIyW1CWqfBnRK6iq8D7M5GEK1vbwfQbeMoMZaI+dQfoK6YpT2baCKuSUcVCgRfBqN6xoY0S1n6YcTDK6WlUaC0o6ZWSmgH4cin1l/hHKkl+EXi4y1DjlpASq9sd7TIGFkWRvXBsXIJZFaYokYqg2YzK24BosQJY/crqlQ4YYxhlwey7G33L4/D71cryvSH+6IsfmbHE8VYAWDzBChzgA1sqLdC1RNG8rSR7RJVeF8GhyPzKCqiLpxwVwxxhj4C5oIarjF/N9At25Ya6ZNLvR1lZr4xmv+iDo4xk0GXeWgxnrEorKuoPTi+8LWPwsbqUCS89siJDyYLwMyjWpQ9RH8cWNljscMhVArn4NlzawZ9HwCXQX1zxFURDbo/2VPJxTMtQhq3KP5EmhlB2rCI3vOIaPNYG2vQCu/4NoiqBxLR78FNC7otvQABfGNEK+yRSAOoKJGGCQ+IW8O9xCHa9yc3BPaUa53gxQvYmtCCdJ/jm1oQAti68AFAXZhhfLJ78d4DQ4JsVwmH+X8NqilJtDQFsvLinHXYEtK50H1QYm7gkoNUBbS+PYZWXvzr3sq7lvQCo9CoVAoFIq7hz7wKBQKhUKhuHu8W5+ts5RTExRPKUs5zqEkuMIoaRqhzOqlZIVqt5lBDfhKSpoRZeMFhmQraA8/o/SFjJkJWSr5st+Bf0L53kKB9oDHPlT2TISb1iEgEwXqhJRZZodCAJXY3sn5oQJp+hGqC6i0QtyX+2+FK+ia66+g62A+ZiZmnyCzBKzMMoHuwAiyDtTlIP9xhFGURR8aCxUNaEWDLK1UQy3j9uq113+Xkn0F0zQLumo9SclzBM0aQJW8nEEJXUSFlGGwGR6hKGO5nsaLm4zzqsj51OvtKa1jTUoL7Q6TOJdknl4vcm4PLQw8rxiboPDqWt6vMd4zMpySk3Y7g97KyLp5ppEY1TTr/rdWj+sZYZAZodTIkAgtq9BjBRlrDUzSSi/jIIKG9cjDKjBCdTC/ZAk993LNn1DSvyUaUALrB2nvuvu26sg7bAFA/l2BkV6EyenDINdWgaKaQbd3A3LuoIrJWI8jfiP3Ddb7em+QSAPMwAzDE+jzmtlozGrEa5gHRrjCeqwpjzAbfO1ByzLDEUqlLUt/DuGNY+INcH4RzqWC6qgEaZO2gkIK9PEZ2xkaqMmYtehxnwmgn+ZNVKUFXFKFe9HFy/sdtldEC9px27cJVZ3rjHv2k6wpRyvt3iC0Mp1kja5fcP8Grbjhphi5RWBBjh7GeJiFuirPoA+fv6+40wqPQqFQKBSKu4c+8CgUCoVCobh7vEtpYaO+yTB9ixFqLGRGNYinLwn0wyKlxRG0h3ffNtJKoBIWqG8czKlKL6W/U4S6oBezNV/ts7QsdvM7ODa9IjenQinX43lwZU4JPjM18q8KZT3XQFEGKqZHWX7D7vkeJeAZbXRLBNBpCWZP0YEGBP3Swzzy5W9CITxjd/8j2jShHxqYzI21UKMfWvQVHB8blKIzFD+Nhepu2z+fF5h6URX2ihKpa0DBXOW8mYcVofjZaNAG87kAJdvs0W/IrAlQIcQrcoJ+gI9k1cp5dvyPVa7rSiYNZeYzqOeSoc6gggbtXmOexkUUUacJZWwYu63IURuL1PRfoVbzEa6bxpgNfbu18jfuKq8H9OUFJfhLkmP8R5n/A2iWFxpTeqFGGtJ7lZzfAdlTHwYZB7X/Mb8R8wehI44NzC9BuR2xBluavkE646l226Qf0iaU3jZjXAeovWAcW2CEmGYZL7XHfMow8ox72jaCNowrDUzl/XmSk21p5or+nMCHNVAKW/Sng+pu6EF9MCMQ32srUMBlr/y8BZYNVDtyAQPyv7KR9ioWuXVYi131sxw/S/+ZBSaayHh8Av3r0J7MvKo2acMONPKKc2je3n5gWrpBsTjgfrxg/fWzHNPiu89F3ndYjzaYAA/o+xnPENuMbRSz9LHHXouUVaWlUCgUCoVCoQ88CoVCoVAo7h/vZ2khCz4wswNl6gwjMmsY3iIvnZGycQ/aJ9YoR1FpAjrENVKm6r1QI2sN9QaUPwF0xuUAZZUxJqHMV2Zkc8AkzhoaOUnZLdBUDrvtu1pK6AsyfYYauTwQJjX4zCtosgol+mJ/jErLwgAy1KAaXmAgtYmqZoLB0xUlSI/d8yeEn6wnXD+ovg7eaWuBYSRKotdeXvfI3rk45qzQJM+YcJDPWpO0ZbPIGHtpkOOD8mr7IAM6I/vl76BoCzN9avT/RcbI9UmOb88o68LMMPUkQW+D44P0wQT6YLlAQQlTvRlyugBVywzlXoahZos5/jxJu8czDMBA/7lJ5ukpg67CXH5BPlde3xjVtULpBHCvHcrr9YdPX18nmBCOmP+fPkHJRXoH1JAF9R4w320rrxtLCkQ+Z0AU0S0x1KAdME6pgkzIees6UB8wHqwj1FVga2ZQJS2ohYQ8tACKeIbpattizWaeGdR0ubzZPgDiv4I6b6WxK6lebGOocG0VlFkWqs6JpqOg4e0LxxjocKi0eig0fYPF6UaYX2U8PvVy7T3qC4/ICxw39E2QAdZinE5Qrq24nxyQ8Vj10pnHRtTTlmMW8mF/gFRqhTmh269XAVTnBJPECHXkkSrD4ePX1/0BZr8naesWxpkBZpHXVtaODpSse5VzOjEnDOdj/4mcO63wKBQKhUKhuHvoA49CoVAoFIq7x/sqLewkN1mojnkBNQLDtwll4wBOyzKwBSVEj13Y3kn5Kg9Svqrzf/r6uoXC6fdFSmJNAK2EqlYb9tvNG6iF7KPwFTNK/BC/mKGT84sRhls4vyrJ9a+1lP6vBceg/LpAIbBZKQPHWdor17dXDhhjTOpAXf1dSocHZLP8CpOp+Vlogw6RM9cJmTDYJe9BM64FWUcXGMOBNjjCfKuj8eAI5R+oNHMQU8B/XBCUDijPjjMMJk9QbcBYbRrl+s9FqM8ZBlekFphNk5Fj1Z0xhtFGiNAxTb/TUd0GGQZiUMetUG2saLsuUHEo11VjjsejvD+Cb1h/F5pgJYUN48wZ5efnF1H0xTMy2HAO27intJbLl6+vHUv/R+mbI9SUNc4vPEq//j6CZoOBZ9tIezWDzMHmA/KzDjKX3UGu85F0SHV7etIYY8A2G0flH9Ymj/crKGwGmLZa0Dh1J+21nWjsimys6i/yfpI5YZmnVJEKx1hm/p0DN2aMuYw4DyrkjPR7CnTlk5dXZiiBck2gfsZV+nMeZXy+guJIWLNDI68vaOyDvX1/OiqtoHB8OvD+KMc8Qmdp0SQVzAMztkJk0LnFgZrHupfQf08GtF2Qtmqx1WRD+1RpTw3Fivc72eeQoMy0WPtbGozi+mvu1ADVTYV2DY51gjthNcv60MGEsRvQ1pfv12+0wqNQKBQKheLuoQ88CoVCoVAo7h76wKNQKBQKheLu8e4ens3TRVc4xAIZZITMkG7MDTjEFtaNHjye20Bc06W4k8+psFdna2SvzieEvi0TNt5Ai+nBVRtjzOER+2S2n76+/gn7kyL4RAt3zgZBdFfsJeqO0hbhDOdJhCPO4BwXOFVWi3zOCIl+tQrPektMZ+F+Lwg9NXA/rkZcG0hXv1F2iufkmu6hcKnG/pexkn54omtnkmMGuG36QSTKUGOayu655Sly/OC80d7TUT7gFUF2X17ku9cZLsqRY0m47wYa57rBXPDyty3GfPpZzq3L+zC+W2BqaRMgY37AFgvKxjdYJozYJ2Cwv4gOpgbn7OFGHP5N+uD5N7n2l38Xjv33s+zhWdEvFtLXlHkS+z0sUCibDOsGm+Bw/UGkrwPsJ6pKzi9DWn/4yH0oCA5GCOWHT1g74L5dYYOJ73+QLh1O3qODHwL2oXTwSajPsAaBdr35JH2+jNiXASlvoMWEwWciGPIVc83TvrmH5QDuD9zjaIwxNcdekrXD4z7SYB/WBU66Geu2g0XHig2aNQI3L3AF9+Xb9yOP/YI15Pcx7e8Rt8DRyucfMKfihL0tmLMNHMFXjjtYoVSwTW9rtgmSCDCuO7iGV5Dh1wid7TuE0S7Y4xT2+5oWWLgs6OeGUQwYCzX3qY50t5fD7RXt4mEfEKWN2ijv/449QhVSEmqMj2P//XVWKzwKhUKhUCjuHvrAo1AoFAqF4u7xLqW1XOFsC8qlQEbXI+iygXyTcrEWYZB5gTQPEmDrpRzVt6LvnSykywjxq1DWiz0kbpA0n2kVbIyp6ZyM6leEtLxC3S0OIoNuoVcfJkhtQfUUnEcNCeUKSTPPgTGkGz5nXX6M9HUq6BMEgF6i9Kf/KNdwgOxwOkkplKX1mHH9QT7nfBHK7KdaKKrUyOdMF9gPoAz8EcGNMcPl9U1AoUFYZxVZL0VQ5hcZw8vv8nq6wJ0VoZQ1QlwdpocHzepbOb6FVPgKmW6Dz89PtJS9DT5B7rm0kPqC9fNwTM1J6DnSRA0oY9dJu5eDXPsEmixcZU5YK/Pr9AL33ge0IVygMZzMBbYFxhjTOToNg/aEjUH5Webpp7/I9TySxwOllRBWSqsHD3l8QlsYBzoTVhpuleBG0lu3xFhJew+LzJdDkTauQBU0jo7aGKegLpMjRQmX+QdZRxn6OTv0IQIqDdasywiXedhz9N0bV3t89Uw5MiT0dPc1i2wrKJAsz0EcvDPolAU0ucU1PILGei0Ikgb7slzgpl7t5fS3wOMHbOGAzXGE3H4DbbkYGV/JYT7+CsoQ1OtQpA0L2oFr6DbL93rPexqCks/yORPowpHzwBizjXBm/4z5dYRFBeZa44T2nEE/jZ/xvpV+9XDvXje5ftpYrKD0mhrrPkJL6fz8R9AKj0KhUCgUiruHPvAoFAqFQqG4e7xLaY1wLaUrrkWJ06M05xEemjNcSw12yMPl1MOZuWnhZgpH4I8IrstUZuHMCxwppwoKjCglTWOMKaizHqDksfjctf5ZzgM001ZJCd1ByeVAuWzYhW82uJAWluhxCAyVXyYpzXX/RAjan0FYWEaV6xkQcBfobgmlWUZ45oS+3QqogiRl9r6XdkzPaEeUIBf37XC/GUm1V1BVddhTWs0m/Xad5LjXz0LxnH+TtpxGqqsQIHmBa+2jnMeAQFuWiwMcfy1KtjP639Ol/PiGirsBalAxn+E4XcE5NhuhRiyUIB/QrxGio4wAzwPog98sKMxB+vgJwbnX61+/vp5/lvkYBmnb6hEu0MteHePQ1hVoiQUu5b/8JOvLx3+Tc607qi+h6oSw8IAwzIH9geDJiLFcgQ4KNVRqw4+htIrDOld9xjnJd5fyNzk/KGVt/PXr65XBqFiDAmiAGm1/RXBygvt1OEKhi5/FHv20IsVyeROQ6zj+oeSNZ4T5QglmjVAc5QS35IBtAlg7OoT8ZoSKnhHaPESsDxhHxnE94haD26DG1guHNcrO8l0LVGYbtkh0pALRPuUsnXCdZazUn6B8xD1kHYUK/Q0BzA9RPnOGWm/DOjtN+/vPyyaqy/EL+m+SsWmh8HtCQsGCOfW6yBqULkJhGmx5ecU96gnqQCq3HZTYDkrqNH9fcacVHoVCoVAoFHcPfeBRKBQKhUJx93iX0trtbMeO6Sc8J/lKytotlBchwLhqkK8JcJLrohxvUcvL2LXtQW9UPQPOpFSYVvnbFeXTEPa7zb2TElwPsz2aibEMzKq7xy70CQoxi4PYXrmSnfcuiuqs3WSn+gjDRI9QveuPYbTMCJqiBm2QoS6i4VxBoJzHUOmhYBkg4ZlpNvgZ6qgKJmmrtPVHqWKzqrlLgG0qluj35ecV5lovrzAxQ+k0oozcZFGSBBixrTCsOmAMdz0UggaUG5zx7AXqQnKUDGEdb28kaTGPfkG591d8Ve9p7gd1DdQVL6OMTQsOqPr06evrpycZ45cRppAIQ3Q9/raRv3UfpL9bUBKvoDONMeYJZer5jFBSmNYNWAseUMp/CFD+XeX4a5I5GLFmJSPXMzzI926ztAsphOoB82P+AUGwxpgK3zdDjXXEutuDVi8vQgkkKNnMVfo8Zqh5QHXh482GtdaB9roiJLYFfeRAMzSbjKntsg+DXbFeWFA2ZZHjZijeJqwF1wnhmFBsBqjR+ifp84I1uFuwZwDbIa4z+jPI+2u8fX8+PMkcMVA1zQjhHGeYEFo5N6pHBygcV4Zxow2fZqigcOmjkc8/mg94LVsN3AHq1E3Gyt9WmEsaY9ZXKKewLXgRhgwAABbISURBVGTMQpt1MKCNoLeYzTotMg4cVKMRlHnBuBmhDuuxjvuPCCHG9ppD+H5faoVHoVAoFArF3UMfeBQKhUKhUNw93qW04hU74WF0tf0MamFE+eqvyBXKUGahbNrQ8Q8GR44lKyghSD14x8wcoUxeQW81FjvBt33Z3DXIdzGo/12oMAAdEkmtyREeKhdTCyXQTKKWaDeh+iKolxmqrpylTa8w+hovMHy8IRJM2RyeddlXHgqkTEorSp88QrZxPqD8eYGqAOZxAUqmMMnrDcaL899ECXCFYuuxQ9m83puEbeiG8Yz8HfR77aTM6ZA/NZ6kjPqI/JrGImsGOS0Lxq1HaR2elaYBNVrO0ofr7b3NTHHCAawIqHuAavKKjKF+lfO/gIoILcrMB+mPB1DMpUgbDsi0oQiyaaRs/tO/wfBtACV5lLbN254CySsMBk/y3V9OMhZaULKHI6gYmPa17G8qYZ4l68vCLNWASiof5PgKWT8eqhNj92vKrXBBux5BX0TMx7hCEeqg0voVqpVHmAFCaebQdlsSiqKGaZ8DVb9taNMWai+oaApUk8tlT9u6kSoqmNZCOWWdUI4d1nAPii5j4T1hW0G8IP8P60UahCpZHfPs5No4L+KE3LIbwdM4EzT/Ano2Iz+KeVg0sr2COk+k2rGeLgdQwb/LWhzQZwvm/ucK1POLtFWNfRQnv2+TFX2OyLOdqvGlk/vmR2RetqDAJpi0JjxbxAeosaCmnE5CmY0wDq2OQpkfW+QxdnS4/Da0wqNQKBQKheLuoQ88CoVCoVAo7h7vZ2m9itFV03z8+np7hcIJCqz17/L85D5JCa6FCug0S33/sEgJLh+x+x9mYL6TMtgxShn8cpKyrIUJV4A6xrxRghxhtLTC3G9DfpaNUpoLhnSKfMcBr7cGxmoRahEoQcZVznVGnkqD581DwfX8IJXW0cj5jZtQLg1ywiJLh8j0yiipJmSg9SiJB5hT8nWVhE7ooZaYQBly93+LDCjzADXe454Guf4uZfT1BRSnQzkeIzxH7OgHjZUTSuIwTNsMxiQoxwU0a4+SaosMoBPMMw8wALwV+hmqtFpKv45mYlCrhU3at0K+j8E1DqDwOtB/jD/qDvL+bxfpVwPlxBEKETfImHt6JG3NczBmBrV0osIEtHcKNCeEcgiGa1MnY6RFbl+CuqgDxRKkqm/CGXNwAP1bUCoPVL7dDhbZdhGUqa0xNjPUMjAzzUHW487ChBGGfGOBsdxn0oH4W5jPJeZngeHwRuZcBYXmUu9pkDDKv2v0WwQtteC+EGCOZzLXBXnbZhnDpw15YFB7NWAfG6gXz1A1jqQGkWN1KzSgWNMRSsHfZCwvyDzbYJY47/ICpX3TgrWul/N/KFAoY9ysMA+cMIdovtvj/Q1qwHjdq5tHrI+eCm1kCi6vyGHrZS3GsmDOK2lyuZ58gUIbCi/m1tUVxyPoXyiGuw+YzH8ArfAoFAqFQqG4e+gDj0KhUCgUirvHu5TWhlylBiFQG8pi6TPKzJ0oR5ZJVBuoyhpXpAy+olR8WGE25aVcOUMRVXC2LcqkpGfodFQ1++e5M8p/LQzjTIHiAaX2EaU9ixLh1kAJAtVVghqnXsUYrEWVskAp9nyR6x9RXpyqfS7NrTBmaacaZnuJZUrUkBOUHc7J8VQ+fd5gZIXy8MGQDpPrTFAU1egr5upARGLGWUqly3WflRJnKI/QxgP+MX4AFQka4BW0Ru9JicIME2qvWEjdIZML6g8XQAkZlPTL7ZUgEbRfDUp2hkkcFXdUF7VU37EkTmM/GM9lqH24JgSsA9sife8Polx0Czozyxzvnvd0c4Hyr8I4KrhOB1PQBoq96ijXvL4gnwmGZj//RfrvMso8oCdkQCZfBeVihWVyPv+YufkK9ZJzzAODOmeQNhqirH8bKAeTMI82ZMHBhHKhKSyovhlUQe7QjlDjmIS2jr/JZ76JpCpQEkUaOjpZ/5mx6EETlwAzSNw8MpS1DdYXA/PAGZTZBnPVdcR6D0XRGN+c+A3wCIPcWITSbUAb5R55ZphrBaq0gi0FtgIFD0pugdrLwGh3hckuRMImIMuygM6mWi+3+zbpCgxywQGvuFd45DFuV7mGV6imF9zjaf65gG7vkY1VQJ8fG1CvnZzDBYa9NZXXfwCt8CgUCoVCobh76AOPQqFQKBSKu8e7lJZ9kBIRxB+mTDD7GbirHCZhMAacUa7uEDgVEyijLGXJBrkvAeZhZZKy2QtKt+Ykpb8I4zW77S+vgWQH1d7dxVn46IUA0ySUdROarYIp0xZR1oMaCZdp1gU0EegEjx3s+QeUWY0xJiQYPCFnZ4VAYrdDP0n/P3bSYCtKzocZqosAGiDK9YQeZVeYaZGSWmlO1iOr6iycw8SytDEmwPisgSqsgBL8gL5yg1xoTwM0GL01DRoDJfEGYzshu8lAOXKG8q9e8b12f963wHJmFg94mRmUIVSKoRYzsAKF2owcooGlaKiRmqO0cw1FRQ3a7sFK+zuo1bxDdtJnqKbqfZtsdDQD9VVgmgZW0awYFxlKyQ3t3lr5nPFFPufYyOtrBzob7FtZoVyEUaOD4eMtMSU5149Qnqzgw1cYQPqDbBmwC3OTZOwXZDFdoaz0MMNLuE6LrK6WBrGYBw5Gswn5Z9nulT2LlWuItfSVt6L2NTAzTcxMBK2RsB5P6LcF+xs2qNEss7dAbyVM2Zcv8rdnc3tJbHeUa3yY5drTT7iXYZ52vMYoc+qSpS/XBetJwFaAmYo7KBfRJhFjInE+QU3muda5vVPqacJcw718WuUe73Fv2bB2ZA91GYwH7cYtJaDnsf4+YJ06HOV6HO4/w4MoYDv3/fqNVngUCoVCoVDcPfSBR6FQKBQKxd3jXUrLb7LTO6DU1EClw53wPdQrltk4yL1JULI8gg5roAgKUFd47MBPDcp3UNZMFeggZJG4bl+aiyjBJpiSrV5KuQVZWg60TMH1rDA6LFm+z6I86kDdHEGljAiAanD8BJXCOf8YJUiD6zcWqhiUfl2L0iHKlxtzuFC9niPGBdQfVS/vU+Xh8L2ulX5eoTQqF2lTs6Jc3e2VPVRUsZrpBpg7ItclgIqyMFnbwCAmKLk6UKX5Jzkmg1o1uM4Gp51bUHfN7SnK3AgFkl/lu0aUrwcofAYYeyYYlFUY79MBFwBTuGqEwVj8LJ8DnsAHKW9nqCYzqMAaJo3X697Aj8aOPD8PI1ALOu2KvL0B5fEHUB2vZCtaOacJ6rJwpRoJWU2NqIkilIK++X5ez58BS/znF+nDgDbzoDKOUK+NFfgatH2hax/oMLIJE9aaDrSUh7ponmg8ByO9RsaXT/u56UFLGQsKdYQKB983g361Z8wdqMWqINeQsY4WUOZdLwrBGSZ2L1AhTYUGeLfPRvvYwlTzg3zXTNVkAwUo/FRHqOk29M0GY1oTodLCtbiWSj+ol3qosaAYbR0pQnn//EZUGslWelHmVQeszcxOg/lviFDiYfyGg3zoYGEY2GNbTC1zuUdO4cNBvuvhKH/bVN/fOqAVHoVCoVAoFHcPfeBRKBQKhUJx93if0kIJLs4wrvokJbKnBmUtUjEFBkdQYPga6hIY0lmWirFLfEFuFbM/ClkfKK5mUE9x2+/Ar47yf36Uz62CZBFRtUJvqxGKFAf6JaMsOFNdBhXZi0NuCqi7DcdcoVJoYR53SywNFClQF/XIfapo6IjS5ssqpewGTVRgpuXRFnb99ljIoJJmT8oUCi9QWsmLhCE5SuuMqaAEychfq2Ee+VjJuGKpNoPGa2Ay5x5g6Aalir/ANAyKtQa5Px5l6oaSovIDKC3QbRuq3R3UPg7qM4tzszC/TBizHTOwoFxMMI5jPleEiWiCqdjcUP1BFaMccw37MT7APC8y0wkSQl9kDNozVDowWVug4PBomAbUc4FCr2BuNkmOWUfkOaFS3pYfk6X1+gpFDunQIm1WfZC2eF1gRAdV4gn87IC1M8DwszgYc55E5eKKjPEvpDVqcC6kxRec2xvV3Yg1P1n5joTMrQaUucVv7xHbAVojfZuxtk/MGEPOX8Q2hNEwM0tQQLks5vaUVv9XMWOd/ob+AB1kYGBZ/wID3v8r425CHxTciy2UvjlyuwAMURvQ+lDDzVgTDlDqukras38jXFsxhxcnFJIlnXaQ834CM75gTm0D5p2Rz2EuXoBaF2JF04Hq+ssRWxOQn/Xg9krBb0ErPAqFQqFQKO4e+sCjUCgUCoXi7vF+lhYoF9uh7MZSGxQoBaZPFnQSq8AJke/wL9zlx9SgdwpUOhtKVtlSgYDSbY3S7RtKK8G4zkKF4hj3A6rHbPK5nqV10DiXCTv+szRMgtmch+GSR27IZIRK6mEINecfUzZvoeaIoK4y+MHEfDMYt3kr5xrrK46X/qwmOcZAaZehkLii/M4iuKNACBQb2T1SWMYYkxESU0CtGphbrgntCrpj6WBuBvVXB6rLDKBmQKd5sHUOapmMUvMJwoODu73qbsY4TaPQfpckFEDTCwWUTzCdRJtc2QseeTVQQb0kqPhAGSRQQxnSH9eAzrgigw2qoYewn5s7QRFouVzEuCzwIJhcXmEGGKjewXVyHA1sO2QRFSiNmgq0H9a77fYMiDHGmAsWSQ866NrInH25kuqRPml3+WZygs9UHFrMU2SjZVD1M/o5OSrlsKZiDiVkLL0m5BkaY8wmNFYPpRUN8WYobTkAVhjErhifHlScheJnPoNmBcXMLQ2vHio4GOZN32dB/mU8tKIUe6mkX6sraJwO7y8yxrtHabdP2P4xXuRvrw3uD+j7AXl5GQrWCverCDPegnuAh3o2D/v16mOG6gwT6RV91sPEcO5gTIy8RJNhTog5WGG9HpBTSbPBD4/yOnwSyrChcvGI8/wDaIVHoVAoFArF3UMfeBQKhUKhUNw93qW0BtBDG8rDIzKmug3KKRoSQhHUI2OKVJSHyd14oTGUlFlrGDGN9oz3QU8glyQjSyuCkjLGmO3y8vV11UvpcIPCp6CsW2op35UeBm0om6dGyqPu8u3cG7vBSNFDWQSTxBrKDCOneVNEGjqepA83GDlVMENcsU3eQ+WScQ0GlewFFJgD5ZAQJvaIauxW5I9PGbk6MC0LRT6ndm/kAzClbAeL46DsgOInQJGUIAq0yAeaQGMdKzEtrFq5hg3l8WaSD/oSpeMSsq7Gh++XWv9VVChfV7WM5QnUQv4i31v+CvUKM6kmmVPMqFlAB+ROyuMmyXUlUCwr5koPWRO8SM2E/iqzfK8xxtSN0AAGWVzrRfpvwbrjYNTpoLqhYilDyeQxDkbMgwAqbh1h2gjquWqhcIP53S0xgF6IoLcr5F6dQWMVXM95oxpLzntNQpVsoJsTNEseyieLtmsDqBLmJ20yxy2oCD/v22Wp5bNeIhSomIMb5nMENR5nGTTziq0OoFNqGIpmjjGoyCLWMmbPXaAIzNsbl70bIIDSeXoUKupllDUkoC97j7HfIHtqkbWlFRbHdDAUhWjS1Pheh4lHxag38n6FdczTlNfsKa2Me3nGPd4VGsQiw5KZXh6KqgZqLFx/1cv4arFNgRTVhk72GFtPR9KEmqWlUCgUCoVCoQ88CoVCoVAo7h/vUloRO68b7OI2M0rOiIKHh5epZym7XUCBHDopWW1fpKwdUKYqVsry8HszjGdxKIEaqAVWiKzcG3VMWGE4BUWKhRotGypzoEBD7XBBOT1fsOsdSogN6pIMOnBd5f2INp2gZMrtu93ypxFhqrjCZKxCSXyBQVkVpWQZUS7MZ/QhFB/FCbWS2Ubok/WA0vqIsmtNV0B5uUEtcq1oH2ZMjTaLyJfZcFyAKdmcZLylMzKwMCaPUKMZKAAmJ8f3yGFjlk2PzLT1AkOw6+1VWhZmYFMjfdPhulKLnDdKIkENexrGQaG2wYzxCSq+CZT0xty5M9WKMIKDUqiFid627dtkW8SUzXdQmnmZLxT1jKBWPPjJAhrDTHJMmuX8LK55ASfbgEo3GMsJ19mmvfnlrbCCQup75gRK+/VQb54nGvvJ6w40w4IxOxsYL040opPPtC8wlJ2k3a9QzmwrqIUjOsTufzt7jJ+AzKwLuz3DPBBj7wrHvQRJ5JRkLbAY26R4VhiertiesMHoklmIJlErehtYUIZUkvYYdxuo8BZOfwWU5IFKPCjdEu6nD1CxlerbppMJa/2HFnQhssa6So55WfZ5cS3o6rWV8xiOQsV5bvPArdnV8n3HDmaLUJO6nlsqYIw4yBh6HD7K91KQi3F3CN83eNUKj0KhUCgUiruHPvAoFAqFQqG4e7zLnRyRr2G9HFolKD6guinYqc1orIyS3XTFLvEkO7hXmhzSkBA8VgJlBDGWWbErPDLDKO0pEIvyZcb3HfCFNCIDo2VGOI5F8GYbqDVHhRei6s/ILFkLd9LD6GyDsV/3Y5QgdiXNBAUDSqQOOUt2lLZ3KMd+AY1To+0KaCwLE0ZakmUY14VBdtgXlMBXUJQ7BY7b92eL8ZlA8ZBOW2BwN6Fc2vZQHuHlBYqUGiZmDehQGnJSkbGMNEmDedo/kfHyr2KGSofKut0YRzl9Qhm/gYEjaei1k39YmEvSwJMZaR59nKHscAtoYRiPORrQQZVnjDEBSpAGBo7POKcW6p0B8yuuoO4WOANiPmZ8dw2qm5ROBSVIovknFHHx9rFo//hc0MevMPp7APWeAigaKLlaZIb9bkGxghqKyDerQFFMM0wlkZlVZlmbGepU9TAp/bvMv8rvHRkD5mMCXZKQp1aQ8bRim0Tkesy8rQJF2Wdmj8k1zKAlE8ZRgTrJrrLWbOaNYeINEAZpuyOMIDdQVM0rjAHP0o7MVKx7GGTSjBImqDW2f3QH0EQJauAdtS1ja3hEu4EW/vRGiRgX+fsQkI014/EBpsMZtFwHta6/QsULo9hHKGxLJdfwBCr1w1+huAQdZrEFA3GMfwit8CgUCoVCobh76AOPQqFQKBSKu8e7lNYJqp4DTYOgCmhQOktGSlMvlzOOkVrTEuVvW+zaXmmCBDOoqmb2h5xP3GAeNSMbBbSXmfalOTtI6Te8gKLwctx6lFwed+XOc9ASDiVXZJOsoHpW+KqtBtlAMDlcoCxy4Ba6+vbKAWOMmdF+DTJ00ogSL8r3DfpkW6gWkc+cUON3KzJtivQP1WuGVBqoBYd2cZ2cTwNKy9k9NTQ7mA0aKXMuoBALSuLByufOK9oeO/1bmKGFV/TnERk9uE4PY8QAyiGTDou378+fBzFes3ASfHmUc6uR1TZUMgZDkLaKzCcD9eaguliPUsYeQNUtk7xmBlvA+K02qHow/zjfjTGmQmbUCrrrl4jy/UHOaYYxIA1CMyi38AnZQhEmZlbO4wj6yJIOJ7UNU8Ch/zF084q56V/lGl47qEkn0AazrFPPMBjMGXSdhVKHlA6oCAvav5pBt3dQ1EXkil2QR0h66yLnYIwxW4XxAKXeCkrbXfG5+NsLzsla+ZwauVERNJnHPYXGtqWCaSnypIqH0pDSvxvh6fDh6+vKy7jrYeB5+iTX+BEZW8/IxctcN9CvCyh1t/3y9XUDbn4Dtemh+m1rOSYjs84EcTassVYYY0yE7CrUUItBoYzboGk912xuEaHiTvq18/L5AxSe7gBFGHIgA7aChArHh++rYbXCo1AoFAqF4u6hDzwKhUKhUCjuHraU8v2jFAqFQqFQKP4/hlZ4FAqFQqFQ3D30gUehUCgUCsXdQx94FAqFQqFQ3D30gUehUCgUCsXdQx94FAqFQqFQ3D30gUehUCgUCsXd4z8AQstNVWTZKf4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
